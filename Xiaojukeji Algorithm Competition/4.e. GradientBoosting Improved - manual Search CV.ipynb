{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from scipy.stats import norm\n",
    "\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME0 = 'GradientBoostingRegressor_final_small.pkl'\n",
    "EST_PICKLE_FILENAME1 = 'BaggingGradientBoostingRegressor_final_small.pkl'\n",
    "LOAD_EST = False\n",
    "RUN_SIMULATED_FEATURE_SELECTION = False\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\tbusy_time\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gap',\n",
       " 'day_in_week',\n",
       " 'weather_1_slots_ago',\n",
       " 'weather_2_slots_ago',\n",
       " 'weather_3_slots_ago',\n",
       " 'busy_time',\n",
       " 'tj_level1_1_slots_ago',\n",
       " 'tj_level2_1_slots_ago',\n",
       " 'tj_level3_1_slots_ago',\n",
       " 'tj_level4_1_slots_ago',\n",
       " 'tj_level1_2_slots_ago',\n",
       " 'tj_level2_2_slots_ago',\n",
       " 'tj_level3_2_slots_ago',\n",
       " 'tj_level4_2_slots_ago',\n",
       " 'tj_level1_3_slots_ago',\n",
       " 'tj_level2_3_slots_ago',\n",
       " 'tj_level3_3_slots_ago',\n",
       " 'tj_level4_3_slots_ago',\n",
       " 'temperature_1_slots_ago',\n",
       " 'pm25_1_slots_ago',\n",
       " '',\n",
       " 'temperature_2_slots_ago',\n",
       " 'pm25_2_slots_ago',\n",
       " '',\n",
       " 'temperature_3_slots_ago',\n",
       " 'pm25_3_slots_ago',\n",
       " '',\n",
       " 'gap_1_slots_ago',\n",
       " 'sum_price_1_slots_ago',\n",
       " 'gap_2_slots_ago',\n",
       " 'sum_price_2_slots_ago',\n",
       " 'gap_3_slots_ago',\n",
       " 'sum_price_3_slots_ago',\n",
       " 'f1',\n",
       " '',\n",
       " 'f11',\n",
       " 'f11_1',\n",
       " 'f11_2',\n",
       " 'f11_3',\n",
       " 'f11_4',\n",
       " 'f11_5',\n",
       " 'f11_6',\n",
       " 'f11_7',\n",
       " 'f11_8',\n",
       " 'f13_4',\n",
       " 'f13_8',\n",
       " 'f14',\n",
       " 'f14_1',\n",
       " 'f14_10',\n",
       " '',\n",
       " 'f14_2',\n",
       " 'f14_3',\n",
       " 'f14_6',\n",
       " 'f14_8',\n",
       " 'f15',\n",
       " 'f15_1',\n",
       " 'f15_2',\n",
       " 'f15_3',\n",
       " 'f15_4',\n",
       " 'f15_6',\n",
       " 'f15_7',\n",
       " 'f15_8',\n",
       " 'f16',\n",
       " 'f16_1',\n",
       " 'f16_10',\n",
       " '',\n",
       " 'f16_11',\n",
       " '',\n",
       " 'f16_12',\n",
       " '',\n",
       " 'f16_3',\n",
       " 'f16_4',\n",
       " 'f16_6',\n",
       " 'f17',\n",
       " 'f17_2',\n",
       " 'f17_3',\n",
       " 'f17_4',\n",
       " 'f17_5',\n",
       " 'f19',\n",
       " 'f19_1',\n",
       " 'f19_2',\n",
       " 'f19_3',\n",
       " 'f19_4',\n",
       " 'f1_1',\n",
       " '',\n",
       " 'f1_10',\n",
       " 'f1_11',\n",
       " 'f1_2',\n",
       " '',\n",
       " 'f1_3',\n",
       " '',\n",
       " 'f1_4',\n",
       " '',\n",
       " 'f1_5',\n",
       " '',\n",
       " 'f1_6',\n",
       " '',\n",
       " 'f1_7',\n",
       " '',\n",
       " 'f1_8',\n",
       " '',\n",
       " 'f20',\n",
       " 'f20_1',\n",
       " 'f20_2',\n",
       " 'f20_4',\n",
       " 'f20_5',\n",
       " 'f20_6',\n",
       " 'f20_7',\n",
       " 'f20_8',\n",
       " 'f20_9',\n",
       " 'f21_1',\n",
       " 'f21_2',\n",
       " 'f22',\n",
       " 'f22_1',\n",
       " 'f22_2',\n",
       " 'f22_3',\n",
       " 'f22_4',\n",
       " 'f22_5',\n",
       " 'f23',\n",
       " 'f23_1',\n",
       " 'f23_2',\n",
       " 'f23_3',\n",
       " 'f23_4',\n",
       " 'f23_5',\n",
       " 'f23_6',\n",
       " 'f24',\n",
       " 'f24_1',\n",
       " 'f24_2',\n",
       " 'f24_3',\n",
       " 'f25',\n",
       " 'f25_1',\n",
       " 'f25_3',\n",
       " 'f25_7',\n",
       " 'f25_8',\n",
       " 'f25_9',\n",
       " 'f2_1',\n",
       " '',\n",
       " 'f2_10',\n",
       " 'f2_11',\n",
       " 'f2_12',\n",
       " 'f2_13',\n",
       " 'f2_2',\n",
       " '',\n",
       " 'f2_4',\n",
       " '',\n",
       " 'f2_5',\n",
       " '',\n",
       " 'f2_6',\n",
       " '',\n",
       " 'f2_7',\n",
       " '',\n",
       " 'f2_8',\n",
       " '',\n",
       " 'f3_1',\n",
       " '',\n",
       " 'f3_2',\n",
       " '',\n",
       " 'f3_3',\n",
       " '',\n",
       " 'f4',\n",
       " '',\n",
       " 'f4_1',\n",
       " '',\n",
       " 'f4_10',\n",
       " 'f4_11',\n",
       " 'f4_13',\n",
       " 'f4_14',\n",
       " 'f4_16',\n",
       " 'f4_17',\n",
       " 'f4_18',\n",
       " 'f4_2',\n",
       " '',\n",
       " 'f4_3',\n",
       " '',\n",
       " 'f4_5',\n",
       " '',\n",
       " 'f4_6',\n",
       " '',\n",
       " 'f4_7',\n",
       " '',\n",
       " 'f4_8',\n",
       " '',\n",
       " 'f4_9',\n",
       " '',\n",
       " 'f5',\n",
       " '',\n",
       " 'f5_1',\n",
       " '',\n",
       " 'f5_3',\n",
       " '',\n",
       " 'f5_4',\n",
       " '',\n",
       " 'f6',\n",
       " '',\n",
       " 'f6_1',\n",
       " '',\n",
       " 'f6_2',\n",
       " '',\n",
       " 'f6_4',\n",
       " '',\n",
       " 'f7',\n",
       " '',\n",
       " 'f8',\n",
       " '',\n",
       " 'f8_1',\n",
       " '',\n",
       " 'f8_2',\n",
       " '',\n",
       " 'f8_3',\n",
       " '',\n",
       " 'f8_4',\n",
       " '',\n",
       " 'f8_5']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields_str = \"\"\"\n",
    "gap day_in_week weather_1_slots_ago weather_2_slots_ago weather_3_slots_ago busy_time \n",
    "tj_level1_1_slots_ago tj_level2_1_slots_ago tj_level3_1_slots_ago tj_level4_1_slots_ago \n",
    "tj_level1_2_slots_ago tj_level2_2_slots_ago tj_level3_2_slots_ago tj_level4_2_slots_ago \n",
    "tj_level1_3_slots_ago tj_level2_3_slots_ago tj_level3_3_slots_ago tj_level4_3_slots_ago \n",
    "temperature_1_slots_ago pm25_1_slots_ago  \n",
    "temperature_2_slots_ago pm25_2_slots_ago  \n",
    "temperature_3_slots_ago pm25_3_slots_ago  \n",
    "gap_1_slots_ago sum_price_1_slots_ago \n",
    "gap_2_slots_ago sum_price_2_slots_ago \n",
    "gap_3_slots_ago sum_price_3_slots_ago \n",
    "f1  f11 f11_1 f11_2 f11_3 f11_4 f11_5 f11_6 f11_7 \n",
    "f11_8 f13_4 f13_8 f14 f14_1 f14_10  f14_2 f14_3 f14_6 f14_8 f15 f15_1 \n",
    "f15_2 f15_3 f15_4 f15_6 f15_7 f15_8 f16 f16_1 f16_10  f16_11  f16_12  f16_3 \n",
    "f16_4 f16_6 f17 f17_2 f17_3 f17_4 f17_5 f19 f19_1 f19_2 f19_3 f19_4 f1_1  \n",
    "f1_10 f1_11 f1_2  f1_3  f1_4  f1_5  f1_6  f1_7  f1_8  f20 f20_1 f20_2 \n",
    "f20_4 f20_5 f20_6 f20_7 f20_8 f20_9 f21_1 f21_2 f22 f22_1 f22_2 f22_3 \n",
    "f22_4 f22_5 f23 f23_1 f23_2 f23_3 f23_4 f23_5 f23_6 f24 f24_1 f24_2 f24_3 \n",
    "f25 f25_1 f25_3 f25_7 f25_8 f25_9 f2_1  f2_10 f2_11 f2_12 f2_13 f2_2  \n",
    "f2_4  f2_5  f2_6  f2_7  f2_8  f3_1  f3_2  f3_3  f4  f4_1  f4_10 f4_11 \n",
    "f4_13 f4_14 f4_16 f4_17 f4_18 f4_2  f4_3  f4_5  f4_6  f4_7  f4_8  f4_9  \n",
    "f5  f5_1  f5_3  f5_4  f6  f6_1  f6_2  f6_4  f7  f8  f8_1  f8_2  f8_3  f8_4  \n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split(' '))\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample, IF(timeofday_slot >= 50 AND timeofday_slot <= 53, 1, 0) AS busy_time\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 102592 rows\n",
      "processed 0 rows\n",
      "processed 5000 rows\n",
      "processed 10000 rows\n",
      "processed 15000 rows\n"
     ]
    }
   ],
   "source": [
    "query = bq.Query(q_all)\n",
    "tableresult = query.results()\n",
    "\n",
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_hot = OneHotEncoder(categorical_features=[1, 2, 3, 4], sparse=False,\n",
    "                        n_values=[7, 10, 10, 10])\n",
    "one_hot.fit(Imputer().fit_transform(all_data_original))\n",
    "print \"n_values_:\"\n",
    "print one_hot.n_values_\n",
    "print \"feature_indices_:\"\n",
    "print one_hot.feature_indices_\n",
    "all_data = StandardScaler().fit_transform(one_hot.fit_transform(Imputer().fit_transform(\n",
    "      all_data_original)))\n",
    "n_features = all_data.shape[1] - 1\n",
    "print 'new number of features: {}'.format(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets.\n",
    "data_size = all_data_original.shape[0]\n",
    "training_size = data_size * 70/100\n",
    "indices = np.random.permutation(data_size)\n",
    "training_idx, test_idx = indices[:training_size], indices[training_size:]\n",
    "all_data_train, all_data_test = all_data_original[training_idx,:], all_data_original[test_idx,:]\n",
    "\n",
    "data_train = all_data_train[:,1:]\n",
    "targets_train = all_data_train[:,0]\n",
    "data_test = all_data_test[:,1:]\n",
    "targets_test = all_data_test[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find good Gradient Boosting\n",
    "steps_bagging = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('select_features', RFECV(SVR(kernel='linear'), scoring=mape_scorer, verbose=1)),\n",
    "  ('estimate', GradientBoostingRegressor())\n",
    "]\n",
    "est = Pipeline(steps_bagging)\n",
    "\n",
    "params = {\n",
    "  'estimate__learning_rate': norm.rvs(size=100),\n",
    "  'estimate__max_depth': range(1,6),\n",
    "  'estimate__min_samples_split': range(1,6),\n",
    "  'estimate__min_samples_leaf': range(1,6),\n",
    "  'estimate__subsample': norm.rvs(size=100)\n",
    "}\n",
    "\n",
    "search_params = RandomizedSearchCV(\n",
    "  estimator=est,\n",
    "  param_distributions=params,\n",
    "  cv=3,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=2,\n",
    "  verbose=3\n",
    ")\n",
    "\n",
    "search_params.fit(data_train, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find good Bagging\n",
    "\n",
    "steps_bagging = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('estimate', BaggingRegressor(\n",
    "      GradientBoostingRegressor(\n",
    "        n_estimators=190, learning_rate=0.5,\n",
    "        verbose=1\n",
    "      )))\n",
    "\n",
    "]\n",
    "\n",
    "params = {\n",
    "  'estimate__learning_rate': [0.1, 0.5, 1, 10],\n",
    "  'estimate__n_estimators': [i for i in range(110, n_features, 20)],\n",
    "#   'estimate__loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "search_params = RandomizedSearchCV(\n",
    "  estimator=est,\n",
    "  param_distributions=params,\n",
    "  cv=5,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=2,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "search_params.fit(data_train_original, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
