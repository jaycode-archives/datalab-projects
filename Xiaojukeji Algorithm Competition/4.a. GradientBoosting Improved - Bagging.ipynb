{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME1 = 'BaggingGradientBoostingRegressor_initial.pkl'\n",
    "EST_PICKLE_FILENAME2 = 'BaggingGradientBoostingRegressor_final.pkl'\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\tbusy_time\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample, IF(timeofday_slot >= 50 AND timeofday_slot <= 53, 1, 0) AS busy_time\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0\n",
    "LIMIT 1000\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1000 rows\n",
      "processed 0 rows\n"
     ]
    }
   ],
   "source": [
    "query = bq.Query(q_all)\n",
    "tableresult = query.results()\n",
    "\n",
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_values_:\n",
      "[ 7 10 10 10]\n",
      "feature_indices_:\n",
      "[ 0  7 17 27 37]\n",
      "new number of features: 196\n"
     ]
    }
   ],
   "source": [
    "one_hot = OneHotEncoder(categorical_features=[1, 2, 3, 4], sparse=False,\n",
    "                        n_values=[7, 10, 10, 10])\n",
    "one_hot.fit(Imputer().fit_transform(all_data_original))\n",
    "print \"n_values_:\"\n",
    "print one_hot.n_values_\n",
    "print \"feature_indices_:\"\n",
    "print one_hot.feature_indices_\n",
    "all_data = StandardScaler().fit_transform(one_hot.fit_transform(Imputer().fit_transform(\n",
    "      all_data_original)))\n",
    "n_features = all_data.shape[1] - 1\n",
    "print 'new number of features: {}'.format(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets.\n",
    "data_size = all_data.shape[0]\n",
    "training_size = data_size * 80/100\n",
    "indices = np.random.permutation(data_size)\n",
    "training_idx, test_idx = indices[:training_size], indices[training_size:]\n",
    "all_data_train, all_data_test = all_data_original[training_idx,:], all_data_original[test_idx,:]\n",
    "\n",
    "data_train = all_data_train[:,1:]\n",
    "targets_train = all_data_train[:,0]\n",
    "data_test = all_data_test[:,1:]\n",
    "targets_test = all_data_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fold with 1 / 196 feature ranks, score=-0.049055\n",
      "Finished fold with 2 / 196 feature ranks, score=-0.027650\n",
      "Finished fold with 3 / 196 feature ranks, score=-0.026438\n",
      "Finished fold with 4 / 196 feature ranks, score=-0.028867\n",
      "Finished fold with 5 / 196 feature ranks, score=-0.027727\n",
      "Finished fold with 6 / 196 feature ranks, score=-0.028258\n",
      "Finished fold with 7 / 196 feature ranks, score=-0.028624\n",
      "Finished fold with 8 / 196 feature ranks, score=-0.030387\n",
      "Finished fold with 9 / 196 feature ranks, score=-0.031699\n",
      "Finished fold with 10 / 196 feature ranks, score=-0.033777\n",
      "Finished fold with 11 / 196 feature ranks, score=-0.033637\n",
      "Finished fold with 12 / 196 feature ranks, score=-0.034078\n",
      "Finished fold with 13 / 196 feature ranks, score=-0.033224\n",
      "Finished fold with 14 / 196 feature ranks, score=-0.032425\n",
      "Finished fold with 15 / 196 feature ranks, score=-0.036007\n",
      "Finished fold with 16 / 196 feature ranks, score=-0.036468\n",
      "Finished fold with 17 / 196 feature ranks, score=-0.037544\n",
      "Finished fold with 18 / 196 feature ranks, score=-0.037573\n",
      "Finished fold with 19 / 196 feature ranks, score=-0.037669\n",
      "Finished fold with 20 / 196 feature ranks, score=-0.037744\n",
      "Finished fold with 21 / 196 feature ranks, score=-0.037928\n",
      "Finished fold with 22 / 196 feature ranks, score=-0.038683\n",
      "Finished fold with 23 / 196 feature ranks, score=-0.038952\n",
      "Finished fold with 24 / 196 feature ranks, score=-0.039186\n",
      "Finished fold with 25 / 196 feature ranks, score=-0.039426\n",
      "Finished fold with 26 / 196 feature ranks, score=-0.039062\n",
      "Finished fold with 27 / 196 feature ranks, score=-0.039400\n",
      "Finished fold with 28 / 196 feature ranks, score=-0.039754\n",
      "Finished fold with 29 / 196 feature ranks, score=-0.039881\n",
      "Finished fold with 30 / 196 feature ranks, score=-0.039453\n",
      "Finished fold with 31 / 196 feature ranks, score=-0.039337\n",
      "Finished fold with 32 / 196 feature ranks, score=-0.040552\n",
      "Finished fold with 33 / 196 feature ranks, score=-0.040679\n",
      "Finished fold with 34 / 196 feature ranks, score=-0.040939\n",
      "Finished fold with 35 / 196 feature ranks, score=-0.040953\n",
      "Finished fold with 36 / 196 feature ranks, score=-0.042170\n",
      "Finished fold with 37 / 196 feature ranks, score=-0.043550\n",
      "Finished fold with 38 / 196 feature ranks, score=-0.044624\n",
      "Finished fold with 39 / 196 feature ranks, score=-0.045242\n",
      "Finished fold with 40 / 196 feature ranks, score=-0.045297\n",
      "Finished fold with 41 / 196 feature ranks, score=-0.043817\n",
      "Finished fold with 42 / 196 feature ranks, score=-0.044047\n",
      "Finished fold with 43 / 196 feature ranks, score=-0.043796\n",
      "Finished fold with 44 / 196 feature ranks, score=-0.044300\n",
      "Finished fold with 45 / 196 feature ranks, score=-0.044283\n",
      "Finished fold with 46 / 196 feature ranks, score=-0.044747\n",
      "Finished fold with 47 / 196 feature ranks, score=-0.044352\n",
      "Finished fold with 48 / 196 feature ranks, score=-0.044308\n",
      "Finished fold with 49 / 196 feature ranks, score=-0.045765\n",
      "Finished fold with 50 / 196 feature ranks, score=-0.045251\n",
      "Finished fold with 51 / 196 feature ranks, score=-0.045178\n",
      "Finished fold with 52 / 196 feature ranks, score=-0.044995\n",
      "Finished fold with 53 / 196 feature ranks, score=-0.044977\n",
      "Finished fold with 54 / 196 feature ranks, score=-0.043892\n",
      "Finished fold with 55 / 196 feature ranks, score=-0.043458\n",
      "Finished fold with 56 / 196 feature ranks, score=-0.043614\n",
      "Finished fold with 57 / 196 feature ranks, score=-0.043585\n",
      "Finished fold with 58 / 196 feature ranks, score=-0.044274\n",
      "Finished fold with 59 / 196 feature ranks, score=-0.044268\n",
      "Finished fold with 60 / 196 feature ranks, score=-0.044462\n",
      "Finished fold with 61 / 196 feature ranks, score=-0.044279\n",
      "Finished fold with 62 / 196 feature ranks, score=-0.044212\n",
      "Finished fold with 63 / 196 feature ranks, score=-0.044090\n",
      "Finished fold with 64 / 196 feature ranks, score=-0.044044\n",
      "Finished fold with 65 / 196 feature ranks, score=-0.045180\n",
      "Finished fold with 66 / 196 feature ranks, score=-0.045163\n",
      "Finished fold with 67 / 196 feature ranks, score=-0.045129\n",
      "Finished fold with 68 / 196 feature ranks, score=-0.045080\n",
      "Finished fold with 69 / 196 feature ranks, score=-0.045116\n",
      "Finished fold with 70 / 196 feature ranks, score=-0.045083\n",
      "Finished fold with 71 / 196 feature ranks, score=-0.045088\n",
      "Finished fold with 72 / 196 feature ranks, score=-0.045264\n",
      "Finished fold with 73 / 196 feature ranks, score=-0.045761\n",
      "Finished fold with 74 / 196 feature ranks, score=-0.045665\n",
      "Finished fold with 75 / 196 feature ranks, score=-0.045702\n",
      "Finished fold with 76 / 196 feature ranks, score=-0.045785\n",
      "Finished fold with 77 / 196 feature ranks, score=-0.045431\n",
      "Finished fold with 78 / 196 feature ranks, score=-0.045306\n",
      "Finished fold with 79 / 196 feature ranks, score=-0.045237\n",
      "Finished fold with 80 / 196 feature ranks, score=-0.044921\n",
      "Finished fold with 81 / 196 feature ranks, score=-0.045058\n",
      "Finished fold with 82 / 196 feature ranks, score=-0.045107\n",
      "Finished fold with 83 / 196 feature ranks, score=-0.045474\n",
      "Finished fold with 84 / 196 feature ranks, score=-0.045536\n",
      "Finished fold with 85 / 196 feature ranks, score=-0.045616\n",
      "Finished fold with 86 / 196 feature ranks, score=-0.045584\n",
      "Finished fold with 87 / 196 feature ranks, score=-0.045580\n",
      "Finished fold with 88 / 196 feature ranks, score=-0.045590\n",
      "Finished fold with 89 / 196 feature ranks, score=-0.045623\n",
      "Finished fold with 90 / 196 feature ranks, score=-0.045631\n",
      "Finished fold with 91 / 196 feature ranks, score=-0.045571\n",
      "Finished fold with 92 / 196 feature ranks, score=-0.045576\n",
      "Finished fold with 93 / 196 feature ranks, score=-0.045634\n",
      "Finished fold with 94 / 196 feature ranks, score=-0.045581\n",
      "Finished fold with 95 / 196 feature ranks, score=-0.045727\n",
      "Finished fold with 96 / 196 feature ranks, score=-0.045677\n",
      "Finished fold with 97 / 196 feature ranks, score=-0.045605\n",
      "Finished fold with 98 / 196 feature ranks, score=-0.045646\n",
      "Finished fold with 99 / 196 feature ranks, score=-0.045756\n",
      "Finished fold with 100 / 196 feature ranks, score=-0.045450\n",
      "Finished fold with 101 / 196 feature ranks, score=-0.045446\n",
      "Finished fold with 102 / 196 feature ranks, score=-0.045442\n",
      "Finished fold with 103 / 196 feature ranks, score=-0.045463\n",
      "Finished fold with 104 / 196 feature ranks, score=-0.045436\n",
      "Finished fold with 105 / 196 feature ranks, score=-0.045454\n",
      "Finished fold with 106 / 196 feature ranks, score=-0.045427\n",
      "Finished fold with 107 / 196 feature ranks, score=-0.045416\n",
      "Finished fold with 108 / 196 feature ranks, score=-0.045264\n",
      "Finished fold with 109 / 196 feature ranks, score=-0.045328\n",
      "Finished fold with 110 / 196 feature ranks, score=-0.045407\n",
      "Finished fold with 111 / 196 feature ranks, score=-0.045446\n",
      "Finished fold with 112 / 196 feature ranks, score=-0.045440\n",
      "Finished fold with 113 / 196 feature ranks, score=-0.045779\n",
      "Finished fold with 114 / 196 feature ranks, score=-0.045814\n",
      "Finished fold with 115 / 196 feature ranks, score=-0.045812\n",
      "Finished fold with 116 / 196 feature ranks, score=-0.045852\n",
      "Finished fold with 117 / 196 feature ranks, score=-0.045839\n",
      "Finished fold with 118 / 196 feature ranks, score=-0.045788\n",
      "Finished fold with 119 / 196 feature ranks, score=-0.045895\n",
      "Finished fold with 120 / 196 feature ranks, score=-0.045976\n",
      "Finished fold with 121 / 196 feature ranks, score=-0.046109\n",
      "Finished fold with 122 / 196 feature ranks, score=-0.045895\n",
      "Finished fold with 123 / 196 feature ranks, score=-0.045846\n",
      "Finished fold with 124 / 196 feature ranks, score=-0.045921\n",
      "Finished fold with 125 / 196 feature ranks, score=-0.045770\n",
      "Finished fold with 126 / 196 feature ranks, score=-0.045725\n",
      "Finished fold with 127 / 196 feature ranks, score=-0.045767\n",
      "Finished fold with 128 / 196 feature ranks, score=-0.045774\n",
      "Finished fold with 129 / 196 feature ranks, score=-0.045697\n",
      "Finished fold with 130 / 196 feature ranks, score=-0.045779\n",
      "Finished fold with 131 / 196 feature ranks, score=-0.045753\n",
      "Finished fold with 132 / 196 feature ranks, score=-0.045803\n",
      "Finished fold with 133 / 196 feature ranks, score=-0.046833\n",
      "Finished fold with 134 / 196 feature ranks, score=-0.047091\n",
      "Finished fold with 135 / 196 feature ranks, score=-0.046996\n",
      "Finished fold with 136 / 196 feature ranks, score=-0.046986\n",
      "Finished fold with 137 / 196 feature ranks, score=-0.046993\n",
      "Finished fold with 138 / 196 feature ranks, score=-0.046887\n",
      "Finished fold with 139 / 196 feature ranks, score=-0.046907\n",
      "Finished fold with 140 / 196 feature ranks, score=-0.047107\n",
      "Finished fold with 141 / 196 feature ranks, score=-0.046977\n",
      "Finished fold with 142 / 196 feature ranks, score=-0.047066\n",
      "Finished fold with 143 / 196 feature ranks, score=-0.046920\n",
      "Finished fold with 144 / 196 feature ranks, score=-0.046917\n",
      "Finished fold with 145 / 196 feature ranks, score=-0.046869\n",
      "Finished fold with 146 / 196 feature ranks, score=-0.046853\n",
      "Finished fold with 147 / 196 feature ranks, score=-0.046896\n",
      "Finished fold with 148 / 196 feature ranks, score=-0.046851\n",
      "Finished fold with 149 / 196 feature ranks, score=-0.046901\n",
      "Finished fold with 150 / 196 feature ranks, score=-0.046206\n",
      "Finished fold with 151 / 196 feature ranks, score=-0.046386\n",
      "Finished fold with 152 / 196 feature ranks, score=-0.046325\n",
      "Finished fold with 153 / 196 feature ranks, score=-0.046311\n",
      "Finished fold with 154 / 196 feature ranks, score=-0.046340\n",
      "Finished fold with 155 / 196 feature ranks, score=-0.046265\n",
      "Finished fold with 156 / 196 feature ranks, score=-0.046167\n",
      "Finished fold with 157 / 196 feature ranks, score=-0.046298\n",
      "Finished fold with 158 / 196 feature ranks, score=-0.046752\n",
      "Finished fold with 159 / 196 feature ranks, score=-0.046729\n",
      "Finished fold with 160 / 196 feature ranks, score=-0.046712\n",
      "Finished fold with 161 / 196 feature ranks, score=-0.046740\n",
      "Finished fold with 162 / 196 feature ranks, score=-0.046710\n",
      "Finished fold with 163 / 196 feature ranks, score=-0.046703\n",
      "Finished fold with 164 / 196 feature ranks, score=-0.046703\n",
      "Finished fold with 165 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 166 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 167 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 168 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 169 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 170 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 171 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 172 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 173 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 174 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 175 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 176 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 177 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 178 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 179 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 180 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 181 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 182 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 183 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 184 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 185 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 186 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 187 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 188 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 189 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 190 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 191 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 192 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 193 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 194 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 195 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 196 / 196 feature ranks, score=-0.046682\n",
      "Finished fold with 1 / 196 feature ranks, score=-0.043530\n",
      "Finished fold with 2 / 196 feature ranks, score=-0.037770\n",
      "Finished fold with 3 / 196 feature ranks, score=-0.039811\n",
      "Finished fold with 4 / 196 feature ranks, score=-0.057599\n",
      "Finished fold with 5 / 196 feature ranks, score=-0.059952\n",
      "Finished fold with 6 / 196 feature ranks, score=-0.059897\n",
      "Finished fold with 7 / 196 feature ranks, score=-0.060694\n",
      "Finished fold with 8 / 196 feature ranks, score=-0.060420\n",
      "Finished fold with 9 / 196 feature ranks, score=-0.060140\n",
      "Finished fold with 10 / 196 feature ranks, score=-0.059635\n",
      "Finished fold with 11 / 196 feature ranks, score=-0.062424\n",
      "Finished fold with 12 / 196 feature ranks, score=-0.061462\n",
      "Finished fold with 13 / 196 feature ranks, score=-0.063102\n",
      "Finished fold with 14 / 196 feature ranks, score=-0.067599\n",
      "Finished fold with 15 / 196 feature ranks, score=-0.068004\n",
      "Finished fold with 16 / 196 feature ranks, score=-0.069409\n",
      "Finished fold with 17 / 196 feature ranks, score=-0.070253\n",
      "Finished fold with 18 / 196 feature ranks, score=-0.072388\n",
      "Finished fold with 19 / 196 feature ranks, score=-0.072289\n",
      "Finished fold with 20 / 196 feature ranks, score=-0.074728\n",
      "Finished fold with 21 / 196 feature ranks, score=-0.074222\n",
      "Finished fold with 22 / 196 feature ranks, score=-0.073066\n",
      "Finished fold with 23 / 196 feature ranks, score=-0.074931\n",
      "Finished fold with 24 / 196 feature ranks, score=-0.075381\n",
      "Finished fold with 25 / 196 feature ranks, score=-0.075502\n",
      "Finished fold with 26 / 196 feature ranks, score=-0.075395\n",
      "Finished fold with 27 / 196 feature ranks, score=-0.075924\n",
      "Finished fold with 28 / 196 feature ranks, score=-0.076482\n",
      "Finished fold with 29 / 196 feature ranks, score=-0.077376\n",
      "Finished fold with 30 / 196 feature ranks, score=-0.077159\n",
      "Finished fold with 31 / 196 feature ranks, score=-0.079148\n",
      "Finished fold with 32 / 196 feature ranks, score=-0.079662\n",
      "Finished fold with 33 / 196 feature ranks, score=-0.078321\n",
      "Finished fold with 34 / 196 feature ranks, score=-0.078697\n",
      "Finished fold with 35 / 196 feature ranks, score=-0.077884\n",
      "Finished fold with 36 / 196 feature ranks, score=-0.080241\n",
      "Finished fold with 37 / 196 feature ranks, score=-0.079969\n",
      "Finished fold with 38 / 196 feature ranks, score=-0.080519\n",
      "Finished fold with 39 / 196 feature ranks, score=-0.079962\n",
      "Finished fold with 40 / 196 feature ranks, score=-0.079985\n",
      "Finished fold with 41 / 196 feature ranks, score=-0.080246\n",
      "Finished fold with 42 / 196 feature ranks, score=-0.080079\n",
      "Finished fold with 43 / 196 feature ranks, score=-0.080150\n",
      "Finished fold with 44 / 196 feature ranks, score=-0.080552\n",
      "Finished fold with 45 / 196 feature ranks, score=-0.079942\n",
      "Finished fold with 46 / 196 feature ranks, score=-0.079885\n",
      "Finished fold with 47 / 196 feature ranks, score=-0.079672\n",
      "Finished fold with 48 / 196 feature ranks, score=-0.080128\n",
      "Finished fold with 49 / 196 feature ranks, score=-0.079928\n",
      "Finished fold with 50 / 196 feature ranks, score=-0.079758\n",
      "Finished fold with 51 / 196 feature ranks, score=-0.079864\n",
      "Finished fold with 52 / 196 feature ranks, score=-0.079962\n",
      "Finished fold with 53 / 196 feature ranks, score=-0.079619\n",
      "Finished fold with 54 / 196 feature ranks, score=-0.080115\n",
      "Finished fold with 55 / 196 feature ranks, score=-0.078993\n",
      "Finished fold with 56 / 196 feature ranks, score=-0.079710\n",
      "Finished fold with 57 / 196 feature ranks, score=-0.080002\n",
      "Finished fold with 58 / 196 feature ranks, score=-0.080693\n",
      "Finished fold with 59 / 196 feature ranks, score=-0.080541\n",
      "Finished fold with 60 / 196 feature ranks, score=-0.080912\n",
      "Finished fold with 61 / 196 feature ranks, score=-0.083046\n",
      "Finished fold with 62 / 196 feature ranks, score=-0.082825\n",
      "Finished fold with 63 / 196 feature ranks, score=-0.083230\n",
      "Finished fold with 64 / 196 feature ranks, score=-0.083290\n",
      "Finished fold with 65 / 196 feature ranks, score=-0.083262\n",
      "Finished fold with 66 / 196 feature ranks, score=-0.083342\n",
      "Finished fold with 67 / 196 feature ranks, score=-0.083287\n",
      "Finished fold with 68 / 196 feature ranks, score=-0.083135\n",
      "Finished fold with 69 / 196 feature ranks, score=-0.083180\n",
      "Finished fold with 70 / 196 feature ranks, score=-0.083166\n",
      "Finished fold with 71 / 196 feature ranks, score=-0.082933\n",
      "Finished fold with 72 / 196 feature ranks, score=-0.082971\n",
      "Finished fold with 73 / 196 feature ranks, score=-0.082911\n",
      "Finished fold with 74 / 196 feature ranks, score=-0.085717\n",
      "Finished fold with 75 / 196 feature ranks, score=-0.085447\n",
      "Finished fold with 76 / 196 feature ranks, score=-0.085345\n",
      "Finished fold with 77 / 196 feature ranks, score=-0.085390\n",
      "Finished fold with 78 / 196 feature ranks, score=-0.085380\n",
      "Finished fold with 79 / 196 feature ranks, score=-0.085487\n",
      "Finished fold with 80 / 196 feature ranks, score=-0.085442\n",
      "Finished fold with 81 / 196 feature ranks, score=-0.085892\n",
      "Finished fold with 82 / 196 feature ranks, score=-0.085902\n",
      "Finished fold with 83 / 196 feature ranks, score=-0.085788\n",
      "Finished fold with 84 / 196 feature ranks, score=-0.085548\n",
      "Finished fold with 85 / 196 feature ranks, score=-0.085497\n",
      "Finished fold with 86 / 196 feature ranks, score=-0.085472\n",
      "Finished fold with 87 / 196 feature ranks, score=-0.085357\n",
      "Finished fold with 88 / 196 feature ranks, score=-0.085397\n",
      "Finished fold with 89 / 196 feature ranks, score=-0.085968\n",
      "Finished fold with 90 / 196 feature ranks, score=-0.085967\n",
      "Finished fold with 91 / 196 feature ranks, score=-0.086340\n",
      "Finished fold with 92 / 196 feature ranks, score=-0.086269\n",
      "Finished fold with 93 / 196 feature ranks, score=-0.086348\n",
      "Finished fold with 94 / 196 feature ranks, score=-0.086360\n",
      "Finished fold with 95 / 196 feature ranks, score=-0.086331\n",
      "Finished fold with 96 / 196 feature ranks, score=-0.086328\n",
      "Finished fold with 97 / 196 feature ranks, score=-0.086304\n",
      "Finished fold with 98 / 196 feature ranks, score=-0.086174\n",
      "Finished fold with 99 / 196 feature ranks, score=-0.086199\n",
      "Finished fold with 100 / 196 feature ranks, score=-0.086187\n",
      "Finished fold with 101 / 196 feature ranks, score=-0.086305\n",
      "Finished fold with 102 / 196 feature ranks, score=-0.086316\n",
      "Finished fold with 103 / 196 feature ranks, score=-0.086280\n",
      "Finished fold with 104 / 196 feature ranks, score=-0.085419\n",
      "Finished fold with 105 / 196 feature ranks, score=-0.085567\n",
      "Finished fold with 106 / 196 feature ranks, score=-0.085454\n",
      "Finished fold with 107 / 196 feature ranks, score=-0.085389\n",
      "Finished fold with 108 / 196 feature ranks, score=-0.085587\n",
      "Finished fold with 109 / 196 feature ranks, score=-0.085547\n",
      "Finished fold with 110 / 196 feature ranks, score=-0.085545\n",
      "Finished fold with 111 / 196 feature ranks, score=-0.085582\n",
      "Finished fold with 112 / 196 feature ranks, score=-0.085507\n",
      "Finished fold with 113 / 196 feature ranks, score=-0.086544\n",
      "Finished fold with 114 / 196 feature ranks, score=-0.086529\n",
      "Finished fold with 115 / 196 feature ranks, score=-0.086755\n",
      "Finished fold with 116 / 196 feature ranks, score=-0.086813\n",
      "Finished fold with 117 / 196 feature ranks, score=-0.086836\n",
      "Finished fold with 118 / 196 feature ranks, score=-0.086862\n",
      "Finished fold with 119 / 196 feature ranks, score=-0.086828\n",
      "Finished fold with 120 / 196 feature ranks, score=-0.086889\n",
      "Finished fold with 121 / 196 feature ranks, score=-0.086923\n",
      "Finished fold with 122 / 196 feature ranks, score=-0.086664\n",
      "Finished fold with 123 / 196 feature ranks, score=-0.086550\n",
      "Finished fold with 124 / 196 feature ranks, score=-0.086568\n",
      "Finished fold with 125 / 196 feature ranks, score=-0.086591\n",
      "Finished fold with 126 / 196 feature ranks, score=-0.086627\n",
      "Finished fold with 127 / 196 feature ranks, score=-0.086556\n",
      "Finished fold with 128 / 196 feature ranks, score=-0.086771\n",
      "Finished fold with 129 / 196 feature ranks, score=-0.085988\n",
      "Finished fold with 130 / 196 feature ranks, score=-0.085949\n",
      "Finished fold with 131 / 196 feature ranks, score=-0.086004\n",
      "Finished fold with 132 / 196 feature ranks, score=-0.089538\n",
      "Finished fold with 133 / 196 feature ranks, score=-0.089563\n",
      "Finished fold with 134 / 196 feature ranks, score=-0.089513\n",
      "Finished fold with 135 / 196 feature ranks, score=-0.089496\n",
      "Finished fold with 136 / 196 feature ranks, score=-0.089502\n",
      "Finished fold with 137 / 196 feature ranks, score=-0.089514\n",
      "Finished fold with 138 / 196 feature ranks, score=-0.089536\n",
      "Finished fold with 139 / 196 feature ranks, score=-0.089500\n",
      "Finished fold with 140 / 196 feature ranks, score=-0.089508\n",
      "Finished fold with 141 / 196 feature ranks, score=-0.089592\n",
      "Finished fold with 142 / 196 feature ranks, score=-0.089589\n",
      "Finished fold with 143 / 196 feature ranks, score=-0.089613\n",
      "Finished fold with 144 / 196 feature ranks, score=-0.089585\n",
      "Finished fold with 145 / 196 feature ranks, score=-0.089565\n",
      "Finished fold with 146 / 196 feature ranks, score=-0.089548\n",
      "Finished fold with 147 / 196 feature ranks, score=-0.089751\n",
      "Finished fold with 148 / 196 feature ranks, score=-0.089599\n",
      "Finished fold with 149 / 196 feature ranks, score=-0.089638\n",
      "Finished fold with 150 / 196 feature ranks, score=-0.089513\n",
      "Finished fold with 151 / 196 feature ranks, score=-0.089706\n",
      "Finished fold with 152 / 196 feature ranks, score=-0.089720\n",
      "Finished fold with 153 / 196 feature ranks, score=-0.089709\n",
      "Finished fold with 154 / 196 feature ranks, score=-0.089695\n",
      "Finished fold with 155 / 196 feature ranks, score=-0.089785\n",
      "Finished fold with 156 / 196 feature ranks, score=-0.089801\n",
      "Finished fold with 157 / 196 feature ranks, score=-0.089719\n",
      "Finished fold with 158 / 196 feature ranks, score=-0.089671\n",
      "Finished fold with 159 / 196 feature ranks, score=-0.089728\n",
      "Finished fold with 160 / 196 feature ranks, score=-0.089730\n",
      "Finished fold with 161 / 196 feature ranks, score=-0.089702\n",
      "Finished fold with 162 / 196 feature ranks, score=-0.089726\n",
      "Finished fold with 163 / 196 feature ranks, score=-0.089698\n",
      "Finished fold with 164 / 196 feature ranks, score=-0.089747\n",
      "Finished fold with 165 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 166 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 167 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 168 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 169 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 170 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 171 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 172 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 173 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 174 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 175 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 176 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 177 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 178 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 179 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 180 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 181 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 182 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 183 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 184 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 185 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 186 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 187 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 188 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 189 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 190 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 191 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 192 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 193 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 194 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 195 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 196 / 196 feature ranks, score=-0.089760\n",
      "Finished fold with 1 / 196 feature ranks, score=-0.047746\n",
      "Finished fold with 2 / 196 feature ranks, score=-0.073732\n",
      "Finished fold with 3 / 196 feature ranks, score=-0.079165\n",
      "Finished fold with 4 / 196 feature ranks, score=-0.081620\n",
      "Finished fold with 5 / 196 feature ranks, score=-0.086130\n",
      "Finished fold with 6 / 196 feature ranks, score=-0.090074\n",
      "Finished fold with 7 / 196 feature ranks, score=-0.089310\n",
      "Finished fold with 8 / 196 feature ranks, score=-0.088683\n",
      "Finished fold with 9 / 196 feature ranks, score=-0.092349\n",
      "Finished fold with 10 / 196 feature ranks, score=-0.091470\n",
      "Finished fold with 11 / 196 feature ranks, score=-0.093695\n",
      "Finished fold with 12 / 196 feature ranks, score=-0.096875\n",
      "Finished fold with 13 / 196 feature ranks, score=-0.100481\n",
      "Finished fold with 14 / 196 feature ranks, score=-0.100850\n",
      "Finished fold with 15 / 196 feature ranks, score=-0.102026\n",
      "Finished fold with 16 / 196 feature ranks, score=-0.102738\n",
      "Finished fold with 17 / 196 feature ranks, score=-0.104817\n",
      "Finished fold with 18 / 196 feature ranks, score=-0.103865\n",
      "Finished fold with 19 / 196 feature ranks, score=-0.104748\n",
      "Finished fold with 20 / 196 feature ranks, score=-0.104329\n",
      "Finished fold with 21 / 196 feature ranks, score=-0.104141\n",
      "Finished fold with 22 / 196 feature ranks, score=-0.103064\n",
      "Finished fold with 23 / 196 feature ranks, score=-0.103053\n",
      "Finished fold with 24 / 196 feature ranks, score=-0.102860\n",
      "Finished fold with 25 / 196 feature ranks, score=-0.103333\n",
      "Finished fold with 26 / 196 feature ranks, score=-0.103612\n",
      "Finished fold with 27 / 196 feature ranks, score=-0.103377\n",
      "Finished fold with 28 / 196 feature ranks, score=-0.103451\n",
      "Finished fold with 29 / 196 feature ranks, score=-0.108476\n",
      "Finished fold with 30 / 196 feature ranks, score=-0.108368\n",
      "Finished fold with 31 / 196 feature ranks, score=-0.109305\n",
      "Finished fold with 32 / 196 feature ranks, score=-0.109612\n",
      "Finished fold with 33 / 196 feature ranks, score=-0.111401\n",
      "Finished fold with 34 / 196 feature ranks, score=-0.111165\n",
      "Finished fold with 35 / 196 feature ranks, score=-0.111100\n",
      "Finished fold with 36 / 196 feature ranks, score=-0.109767\n",
      "Finished fold with 37 / 196 feature ranks, score=-0.110633\n",
      "Finished fold with 38 / 196 feature ranks, score=-0.114093\n",
      "Finished fold with 39 / 196 feature ranks, score=-0.113677\n",
      "Finished fold with 40 / 196 feature ranks, score=-0.113336\n",
      "Finished fold with 41 / 196 feature ranks, score=-0.112544\n",
      "Finished fold with 42 / 196 feature ranks, score=-0.112867\n",
      "Finished fold with 43 / 196 feature ranks, score=-0.117020\n",
      "Finished fold with 44 / 196 feature ranks, score=-0.117863\n",
      "Finished fold with 45 / 196 feature ranks, score=-0.117845\n",
      "Finished fold with 46 / 196 feature ranks, score=-0.119455\n",
      "Finished fold with 47 / 196 feature ranks, score=-0.119017\n",
      "Finished fold with 48 / 196 feature ranks, score=-0.118314\n",
      "Finished fold with 49 / 196 feature ranks, score=-0.116856\n",
      "Finished fold with 50 / 196 feature ranks, score=-0.117896\n",
      "Finished fold with 51 / 196 feature ranks, score=-0.119695\n",
      "Finished fold with 52 / 196 feature ranks, score=-0.119651\n",
      "Finished fold with 53 / 196 feature ranks, score=-0.119735\n",
      "Finished fold with 54 / 196 feature ranks, score=-0.123194\n",
      "Finished fold with 55 / 196 feature ranks, score=-0.122879\n",
      "Finished fold with 56 / 196 feature ranks, score=-0.122084\n",
      "Finished fold with 57 / 196 feature ranks, score=-0.122346\n",
      "Finished fold with 58 / 196 feature ranks, score=-0.122362\n",
      "Finished fold with 59 / 196 feature ranks, score=-0.121844\n",
      "Finished fold with 60 / 196 feature ranks, score=-0.122389\n",
      "Finished fold with 61 / 196 feature ranks, score=-0.122041\n",
      "Finished fold with 62 / 196 feature ranks, score=-0.122226\n",
      "Finished fold with 63 / 196 feature ranks, score=-0.125214\n",
      "Finished fold with 64 / 196 feature ranks, score=-0.125699\n",
      "Finished fold with 65 / 196 feature ranks, score=-0.125624\n",
      "Finished fold with 66 / 196 feature ranks, score=-0.125795\n",
      "Finished fold with 67 / 196 feature ranks, score=-0.125524\n",
      "Finished fold with 68 / 196 feature ranks, score=-0.125484\n",
      "Finished fold with 69 / 196 feature ranks, score=-0.125778\n",
      "Finished fold with 70 / 196 feature ranks, score=-0.125554\n",
      "Finished fold with 71 / 196 feature ranks, score=-0.125591\n",
      "Finished fold with 72 / 196 feature ranks, score=-0.125615\n",
      "Finished fold with 73 / 196 feature ranks, score=-0.125436\n",
      "Finished fold with 74 / 196 feature ranks, score=-0.125141\n",
      "Finished fold with 75 / 196 feature ranks, score=-0.125142\n",
      "Finished fold with 76 / 196 feature ranks, score=-0.125015\n",
      "Finished fold with 77 / 196 feature ranks, score=-0.124813\n",
      "Finished fold with 78 / 196 feature ranks, score=-0.124810\n",
      "Finished fold with 79 / 196 feature ranks, score=-0.124782\n",
      "Finished fold with 80 / 196 feature ranks, score=-0.124664\n",
      "Finished fold with 81 / 196 feature ranks, score=-0.124760\n",
      "Finished fold with 82 / 196 feature ranks, score=-0.124653\n",
      "Finished fold with 83 / 196 feature ranks, score=-0.125598\n",
      "Finished fold with 84 / 196 feature ranks, score=-0.125760\n",
      "Finished fold with 85 / 196 feature ranks, score=-0.125845\n",
      "Finished fold with 86 / 196 feature ranks, score=-0.125818\n",
      "Finished fold with 87 / 196 feature ranks, score=-0.125797\n",
      "Finished fold with 88 / 196 feature ranks, score=-0.125815\n",
      "Finished fold with 89 / 196 feature ranks, score=-0.126063\n",
      "Finished fold with 90 / 196 feature ranks, score=-0.126002\n",
      "Finished fold with 91 / 196 feature ranks, score=-0.125885\n",
      "Finished fold with 92 / 196 feature ranks, score=-0.125926\n",
      "Finished fold with 93 / 196 feature ranks, score=-0.125801\n",
      "Finished fold with 94 / 196 feature ranks, score=-0.125792\n",
      "Finished fold with 95 / 196 feature ranks, score=-0.125858\n",
      "Finished fold with 96 / 196 feature ranks, score=-0.125870\n",
      "Finished fold with 97 / 196 feature ranks, score=-0.125876\n",
      "Finished fold with 98 / 196 feature ranks, score=-0.125874\n",
      "Finished fold with 99 / 196 feature ranks, score=-0.124347\n",
      "Finished fold with 100 / 196 feature ranks, score=-0.124345\n",
      "Finished fold with 101 / 196 feature ranks, score=-0.124259\n",
      "Finished fold with 102 / 196 feature ranks, score=-0.124348\n",
      "Finished fold with 103 / 196 feature ranks, score=-0.124366\n",
      "Finished fold with 104 / 196 feature ranks, score=-0.124308\n",
      "Finished fold with 105 / 196 feature ranks, score=-0.124309\n",
      "Finished fold with 106 / 196 feature ranks, score=-0.124160\n",
      "Finished fold with 107 / 196 feature ranks, score=-0.124426\n",
      "Finished fold with 108 / 196 feature ranks, score=-0.124286\n",
      "Finished fold with 109 / 196 feature ranks, score=-0.124191\n",
      "Finished fold with 110 / 196 feature ranks, score=-0.124379\n",
      "Finished fold with 111 / 196 feature ranks, score=-0.125142\n",
      "Finished fold with 112 / 196 feature ranks, score=-0.125209\n",
      "Finished fold with 113 / 196 feature ranks, score=-0.125019\n",
      "Finished fold with 114 / 196 feature ranks, score=-0.124956\n",
      "Finished fold with 115 / 196 feature ranks, score=-0.124983\n",
      "Finished fold with 116 / 196 feature ranks, score=-0.124992\n",
      "Finished fold with 117 / 196 feature ranks, score=-0.125056\n",
      "Finished fold with 118 / 196 feature ranks, score=-0.124503\n",
      "Finished fold with 119 / 196 feature ranks, score=-0.124443\n",
      "Finished fold with 120 / 196 feature ranks, score=-0.124510\n",
      "Finished fold with 121 / 196 feature ranks, score=-0.124544\n",
      "Finished fold with 122 / 196 feature ranks, score=-0.124478\n",
      "Finished fold with 123 / 196 feature ranks, score=-0.124115\n",
      "Finished fold with 124 / 196 feature ranks, score=-0.124274\n",
      "Finished fold with 125 / 196 feature ranks, score=-0.124262\n",
      "Finished fold with 126 / 196 feature ranks, score=-0.124263\n",
      "Finished fold with 127 / 196 feature ranks, score=-0.125133\n",
      "Finished fold with 128 / 196 feature ranks, score=-0.125188\n",
      "Finished fold with 129 / 196 feature ranks, score=-0.125180\n",
      "Finished fold with 130 / 196 feature ranks, score=-0.125220\n",
      "Finished fold with 131 / 196 feature ranks, score=-0.125234\n",
      "Finished fold with 132 / 196 feature ranks, score=-0.125263\n",
      "Finished fold with 133 / 196 feature ranks, score=-0.125107\n",
      "Finished fold with 134 / 196 feature ranks, score=-0.125144\n",
      "Finished fold with 135 / 196 feature ranks, score=-0.125091\n",
      "Finished fold with 136 / 196 feature ranks, score=-0.125050\n",
      "Finished fold with 137 / 196 feature ranks, score=-0.124800\n",
      "Finished fold with 138 / 196 feature ranks, score=-0.124785\n",
      "Finished fold with 139 / 196 feature ranks, score=-0.124794\n",
      "Finished fold with 140 / 196 feature ranks, score=-0.124768\n",
      "Finished fold with 141 / 196 feature ranks, score=-0.124767\n",
      "Finished fold with 142 / 196 feature ranks, score=-0.124802\n",
      "Finished fold with 143 / 196 feature ranks, score=-0.124788\n",
      "Finished fold with 144 / 196 feature ranks, score=-0.124767\n",
      "Finished fold with 145 / 196 feature ranks, score=-0.124788\n",
      "Finished fold with 146 / 196 feature ranks, score=-0.125423\n",
      "Finished fold with 147 / 196 feature ranks, score=-0.125432\n",
      "Finished fold with 148 / 196 feature ranks, score=-0.125471\n",
      "Finished fold with 149 / 196 feature ranks, score=-0.125465\n",
      "Finished fold with 150 / 196 feature ranks, score=-0.125508\n",
      "Finished fold with 151 / 196 feature ranks, score=-0.125536\n",
      "Finished fold with 152 / 196 feature ranks, score=-0.125492\n",
      "Finished fold with 153 / 196 feature ranks, score=-0.125463\n",
      "Finished fold with 154 / 196 feature ranks, score=-0.125560\n",
      "Finished fold with 155 / 196 feature ranks, score=-0.125579\n",
      "Finished fold with 156 / 196 feature ranks, score=-0.125559\n",
      "Finished fold with 157 / 196 feature ranks, score=-0.125944\n",
      "Finished fold with 158 / 196 feature ranks, score=-0.126006\n",
      "Finished fold with 159 / 196 feature ranks, score=-0.125976\n",
      "Finished fold with 160 / 196 feature ranks, score=-0.125921\n",
      "Finished fold with 161 / 196 feature ranks, score=-0.125969\n",
      "Finished fold with 162 / 196 feature ranks, score=-0.125950\n",
      "Finished fold with 163 / 196 feature ranks, score=-0.125944\n",
      "Finished fold with 164 / 196 feature ranks, score=-0.126006\n",
      "Finished fold with 165 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 166 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 167 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 168 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 169 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 170 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 171 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 172 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 173 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 174 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 175 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 176 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 177 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 178 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 179 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 180 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 181 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 182 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 183 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 184 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 185 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 186 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 187 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 188 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 189 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 190 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 191 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 192 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 193 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 194 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 195 / 196 feature ranks, score=-0.126008\n",
      "Finished fold with 196 / 196 feature ranks, score=-0.126008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFECV(cv=None,\n",
       "   estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
       "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       "   estimator_params=None,\n",
       "   scoring=make_scorer(mape, greater_is_better=False), step=1, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv = RFECV(SVR(kernel='linear'), scoring=mape_scorer, verbose=1)\n",
    "rfecv.fit(all_data[training_idx,1:], targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[195 194 193 192 191 190 189 188  72 176  50 175 174 173 170 169 178 177\n",
      "  43 181  24 168 167 165 179 180 172 171  35 182  23 166 183 185 186 187\n",
      "   1 184   4  33  20  29  52  44 113  10   5  19   9  13  77  21  49  91\n",
      " 146  22   1  14   6  15   2   7 160 144 155 114 148 161 125 154 116  17\n",
      "  60  99 153 140  37 136 142  85 137  71 126  83 150  26   3 158  95 124\n",
      "  55 162 110  38 118 138  76  25  74 147  87 127 156 115  63 128 133  79\n",
      "  69  45 151  54 141  59  89 157  16 122  61 132 120 134  30  42 149  82\n",
      "  57 103  64  12  94  58  80 164  27  98 130 104  39 145 117 163  96 129\n",
      "  32 123  66  31  18  67 105  41  86  70  62  34  53  78 135 106 109  46\n",
      " 108 112  90  51  92   8  81  73  28  47  88  97 107  84  40  36 139  48\n",
      " 159  75  93  56 143  68 111 100 119 101  11 102  65 121 152 131]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]\n"
     ]
    }
   ],
   "source": [
    "print rfecv.n_features_\n",
    "print rfecv.ranking_\n",
    "print rfecv.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Initial Bagging Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(EST_PICKLE_FILENAME1):\n",
    "  bagging_est = pickle.load(open(EST_PICKLE_FILENAME1, 'r'))\n",
    "else:\n",
    "  steps = [\n",
    "    ('impute', Imputer()),\n",
    "    # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "    # should start at index 0.\n",
    "    ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                             n_values=[7, 10, 10, 10])),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('estimate', GradientBoostingRegressor(n_estimators=n_features, learning_rate=0.5))\n",
    "\n",
    "  ]\n",
    "\n",
    "  est = Pipeline(steps)\n",
    "\n",
    "  steps_bagging = [\n",
    "    ('impute', Imputer()),\n",
    "    # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "    # should start at index 0.\n",
    "    ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                             n_values=[7, 10, 10, 10])),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('estimate', BaggingRegressor(\n",
    "        GradientBoostingRegressor(\n",
    "          n_estimators=n_features, learning_rate=0.5,\n",
    "          verbose=1\n",
    "        )))\n",
    "\n",
    "  ]\n",
    "\n",
    "  bagging_est = Pipeline(steps_bagging)\n",
    "  bagging_est.fit(data_train, targets_train)\n",
    "  pickle.dump(bagging_est, open(EST_PICKLE_FILENAME1, \"w\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predictions = bagging_est.predict(data_test)\n",
    "print(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps_bagging = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('estimate', BaggingRegressor(\n",
    "      GradientBoostingRegressor(\n",
    "        n_estimators=n_features, learning_rate=0.5,\n",
    "        verbose=1\n",
    "      )))\n",
    "\n",
    "]\n",
    "\n",
    "params = {\n",
    "  'estimate__learning_rate': [0.1, 0.5, 1, 10],\n",
    "  'estimate__n_estimators': [i for i in range(110, n_features, 20)],\n",
    "#   'estimate__loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "search_params = RandomizedSearchCV(\n",
    "  estimator=est,\n",
    "  param_distributions=params,\n",
    "  cv=5,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=2,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "search_params.fit(data_train_original, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
