{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME0 = 'GradientBoostingRegressor_initial.pkl'\n",
    "EST_PICKLE_FILENAME1 = 'BaggingGradientBoostingRegressor_initial.pkl'\n",
    "EST_PICKLE_FILENAME2 = 'BaggingGradientBoostingRegressor_final.pkl'\n",
    "LOAD_EST = False\n",
    "RUN_SIMULATED_FEATURE_SELECTION = False\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\tbusy_time\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample, IF(timeofday_slot >= 50 AND timeofday_slot <= 53, 1, 0) AS busy_time\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0\n",
    "LIMIT 5000\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5000 rows\n",
      "processed 0 rows\n"
     ]
    }
   ],
   "source": [
    "query = bq.Query(q_all)\n",
    "tableresult = query.results()\n",
    "\n",
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_values_:\n",
      "[ 7 10 10 10]\n",
      "feature_indices_:\n",
      "[ 0  7 17 27 37]\n",
      "new number of features: 196\n"
     ]
    }
   ],
   "source": [
    "one_hot = OneHotEncoder(categorical_features=[1, 2, 3, 4], sparse=False,\n",
    "                        n_values=[7, 10, 10, 10])\n",
    "one_hot.fit(Imputer().fit_transform(all_data_original))\n",
    "print \"n_values_:\"\n",
    "print one_hot.n_values_\n",
    "print \"feature_indices_:\"\n",
    "print one_hot.feature_indices_\n",
    "all_data = StandardScaler().fit_transform(one_hot.fit_transform(Imputer().fit_transform(\n",
    "      all_data_original)))\n",
    "n_features = all_data.shape[1] - 1\n",
    "print 'new number of features: {}'.format(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if RUN_SIMULATED_FEATURE_SELECTION:\n",
    "  rfecv = RFECV(SVR(kernel='linear'), scoring=mape_scorer, verbose=1)\n",
    "  rfecv.fit(all_data[training_idx,1:], targets_train)\n",
    "\n",
    "  print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "  # Plot number of features VS. cross-validation scores\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Number of features selected\")\n",
    "  plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "  plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "  plt.show()\n",
    "  rfecv.transform(all_data[training_idx,1:]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets.\n",
    "data_size = all_data.shape[0]\n",
    "training_size = data_size * 80/100\n",
    "indices = np.random.permutation(data_size)\n",
    "training_idx, test_idx = indices[:training_size], indices[training_size:]\n",
    "all_data_train, all_data_test = all_data_original[training_idx,:], all_data_original[test_idx,:]\n",
    "\n",
    "data_train = all_data_train[:,1:]\n",
    "targets_train = all_data_train[:,0]\n",
    "data_test = all_data_test[:,1:]\n",
    "targets_test = all_data_test[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Initial Bagging Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1         548.3838           16.04s\n",
      "         2         212.6178           11.63s\n",
      "         3         115.6721           10.12s\n",
      "         4          84.3745            9.50s\n",
      "         5          65.8737            8.99s\n",
      "         6          56.6408            8.64s\n",
      "         7          53.0160            8.72s\n",
      "         8          50.1151            8.49s\n",
      "         9          47.0500            8.31s\n",
      "        10          43.5573            8.14s\n",
      "        20          26.4335            7.32s\n",
      "        30          17.9776            6.71s\n",
      "        40          12.4601            6.28s\n",
      "        50          10.0435            5.81s\n",
      "        60           7.9723            5.39s\n",
      "        70           6.6196            4.99s\n",
      "        80           5.6752            4.58s\n",
      "        90           5.0225            4.18s\n",
      "       100           4.2537            3.76s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         745.2248           19.38s\n",
      "         2         291.7087           15.40s\n",
      "         3         146.1584           12.79s\n",
      "         4         101.0813           11.40s\n",
      "         5          80.2794           10.47s\n",
      "         6          71.0017            9.82s\n",
      "         7          66.5908            9.37s\n",
      "         8          64.0519            9.10s\n",
      "         9          57.3915            8.86s\n",
      "        10          54.9613            8.65s\n",
      "        20          32.0245            7.63s\n",
      "        30          20.4567            7.07s\n",
      "        40          15.7418            6.52s\n",
      "        50          11.3504            6.05s\n",
      "        60           9.2591            5.57s\n",
      "        70           7.2737            5.12s\n",
      "        80           6.2463            4.71s\n",
      "        90           5.1318            4.30s\n",
      "       100           4.5012            3.87s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         756.7729           15.55s\n",
      "         2         284.5745           11.43s\n",
      "         3         140.0984           10.03s\n",
      "         4          90.9126            9.38s\n",
      "         5          76.2337            8.93s\n",
      "         6          66.9281            8.60s\n",
      "         7          61.1596            8.35s\n",
      "         8          57.9895            8.27s\n",
      "         9          55.0989            8.13s\n",
      "        10          51.9341            7.98s\n",
      "        20          28.7880            7.15s\n",
      "        30          20.5861            6.66s\n",
      "        40          14.2736            6.20s\n",
      "        50          10.9941            5.79s\n",
      "        60           8.9892            5.39s\n",
      "        70           7.3477            5.00s\n",
      "        80           6.2346            4.57s\n",
      "        90           5.3061            4.15s\n",
      "       100           4.5957            3.75s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         816.8651           15.97s\n",
      "         2         298.0889           11.61s\n",
      "         3         158.0172           10.12s\n",
      "         4         111.6292            9.53s\n",
      "         5          92.5218            9.14s\n",
      "         6          75.3513            8.80s\n",
      "         7          70.9349            8.61s\n",
      "         8          68.8842            8.32s\n",
      "         9          64.6249            8.30s\n",
      "        10          59.3984            8.14s\n",
      "        20          29.9261            7.28s\n",
      "        30          21.0865            6.86s\n",
      "        40          16.1175            6.60s\n",
      "        50          12.6306            6.24s\n",
      "        60           9.7197            5.90s\n",
      "        70           7.5684            5.36s\n",
      "        80           6.7264            4.87s\n",
      "        90           5.7846            4.42s\n",
      "       100           5.0443            3.96s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         454.9004           15.90s\n",
      "         2         199.0780           11.86s\n",
      "         3         117.0042           10.46s\n",
      "         4          87.2286            9.75s\n",
      "         5          77.6311            9.23s\n",
      "         6          68.1498            8.89s\n",
      "         7          64.5051            8.53s\n",
      "         8          57.9342            8.40s\n",
      "         9          54.1893            8.28s\n",
      "        10          52.1049            8.24s\n",
      "        20          33.8304            7.29s\n",
      "        30          20.6442            6.77s\n",
      "        40          15.1091            6.30s\n",
      "        50          11.3432            5.87s\n",
      "        60           8.4404            5.47s\n",
      "        70           7.0352            5.09s\n",
      "        80           6.2178            4.67s\n",
      "        90           5.1093            4.25s\n",
      "       100           4.4534            3.83s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         768.3051           16.34s\n",
      "         2         273.5583           11.93s\n",
      "         3         135.3614           10.47s\n",
      "         4          88.5329            9.74s\n",
      "         5          74.3674            9.20s\n",
      "         6          69.2502            8.79s\n",
      "         7          62.1289            8.62s\n",
      "         8          54.7377            8.44s\n",
      "         9          50.6533            8.34s\n",
      "        10          47.5582            8.33s\n",
      "        20          29.5759            7.64s\n",
      "        30          19.6226            7.06s\n",
      "        40          14.1954            6.56s\n",
      "        50          10.7344            6.08s\n",
      "        60           8.6919            5.62s\n",
      "        70           7.2123            5.18s\n",
      "        80           6.0395            4.73s\n",
      "        90           5.0670            4.30s\n",
      "       100           4.3859            3.86s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         690.2169           15.85s\n",
      "         2         274.5126           11.60s\n",
      "         3         142.1414           10.14s\n",
      "         4          98.3674            9.41s\n",
      "         5          75.5913            8.96s\n",
      "         6          67.8997            8.58s\n",
      "         7          64.2557            8.29s\n",
      "         8          59.4191            8.13s\n",
      "         9          55.4092            7.93s\n",
      "        10          53.3605            7.80s\n",
      "        20          30.1517            7.11s\n",
      "        30          20.0562            6.64s\n",
      "        40          14.7775            6.21s\n",
      "        50          11.2542            5.80s\n",
      "        60           9.1738            5.37s\n",
      "        70           7.7204            4.96s\n",
      "        80           6.4398            4.55s\n",
      "        90           5.5006            4.14s\n",
      "       100           4.7345            3.72s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         722.4940           15.90s\n",
      "         2         295.4001           11.50s\n",
      "         3         154.8440           10.00s\n",
      "         4         107.2086            9.25s\n",
      "         5          86.1025            8.85s\n",
      "         6          78.4609            8.51s\n",
      "         7          72.7790            8.28s\n",
      "         8          70.7721            8.03s\n",
      "         9          63.5667            7.87s\n",
      "        10          60.0977            7.73s\n",
      "        20          37.2327            6.99s\n",
      "        30          22.1219            6.52s\n",
      "        40          16.4456            6.10s\n",
      "        50          12.4490            5.69s\n",
      "        60           9.1682            5.31s\n",
      "        70           7.5255            4.92s\n",
      "        80           6.2318            4.53s\n",
      "        90           5.3500            4.13s\n",
      "       100           4.7173            3.71s\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(EST_PICKLE_FILENAME1) and LOAD_EST:\n",
    "  bagging_est = pickle.load(open(EST_PICKLE_FILENAME1, 'r'))\n",
    "  est = pickle.load(open(EST_PICKLE_FILENAME0, 'r'))\n",
    "else:\n",
    "  steps = [\n",
    "    ('impute', Imputer()),\n",
    "    # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "    # should start at index 0.\n",
    "    ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                             n_values=[7, 10, 10, 10])),\n",
    "    ('scale', StandardScaler()),\n",
    "#     ('select_features', RFECV(SVR(kernel='linear'), scoring=mape_scorer, verbose=1)),\n",
    "    ('estimate', GradientBoostingRegressor(n_estimators=190, learning_rate=0.5))\n",
    "\n",
    "  ]\n",
    "\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data_train, targets_train)\n",
    "#   pickle.dump(bagging_est, open(EST_PICKLE_FILENAME0, \"w\") )\n",
    "\n",
    "  steps_bagging = [\n",
    "    ('impute', Imputer()),\n",
    "    # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "    # should start at index 0.\n",
    "    ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                             n_values=[7, 10, 10, 10])),\n",
    "    ('scale', StandardScaler()),\n",
    "#     ('select_features', RFECV(SVR(kernel='linear'), scoring=mape_scorer, verbose=1)),\n",
    "    ('estimate', BaggingRegressor(\n",
    "        GradientBoostingRegressor(\n",
    "          n_estimators=190, learning_rate=0.5,\n",
    "          verbose=1\n",
    "        )))\n",
    "  ]\n",
    "\n",
    "  bagging_est = Pipeline(steps_bagging)\n",
    "  bagging_est.fit(data_train, targets_train)\n",
    "#   pickle.dump(bagging_est, open(EST_PICKLE_FILENAME1, \"w\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predictions = est.predict(data_test)\n",
    "print 'gradient boosting score: {}'.format(mape(targets_test, test_predictions))\n",
    "\n",
    "test_predictions = bagging_est.predict(data_test)\n",
    "print 'gradient boosting with bagging score: {}'.format(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps_bagging = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('estimate', BaggingRegressor(\n",
    "      GradientBoostingRegressor(\n",
    "        n_estimators=n_features, learning_rate=0.5,\n",
    "        verbose=1\n",
    "      )))\n",
    "\n",
    "]\n",
    "\n",
    "params = {\n",
    "  'estimate__learning_rate': [0.1, 0.5, 1, 10],\n",
    "  'estimate__n_estimators': [i for i in range(110, n_features, 20)],\n",
    "#   'estimate__loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "search_params = RandomizedSearchCV(\n",
    "  estimator=est,\n",
    "  param_distributions=params,\n",
    "  cv=5,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=2,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "search_params.fit(data_train_original, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
