{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME1 = 'BaggingGradientBoostingRegressor_initial.pkl'\n",
    "EST_PICKLE_FILENAME2 = 'BaggingGradientBoostingRegressor_final.pkl'\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\tbusy_time\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample, IF(timeofday_slot >= 50 AND timeofday_slot <= 53, 1, 0) AS busy_time\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 102592 rows\n",
      "processed 0 rows\n",
      "processed 5000 rows\n",
      "processed 10000 rows\n",
      "processed 15000 rows\n",
      "processed 20000 rows\n",
      "processed 25000 rows\n",
      "processed 30000 rows\n",
      "processed 35000 rows\n",
      "processed 40000 rows\n",
      "processed 45000 rows\n",
      "processed 50000 rows\n",
      "processed 55000 rows\n",
      "processed 60000 rows\n",
      "processed 65000 rows\n",
      "processed 70000 rows\n",
      "processed 75000 rows\n",
      "processed 80000 rows\n",
      "processed 85000 rows\n",
      "processed 90000 rows\n",
      "processed 95000 rows\n",
      "processed 100000 rows\n"
     ]
    }
   ],
   "source": [
    "query = bq.Query(q_all)\n",
    "tableresult = query.results()\n",
    "\n",
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_values_:\n",
      "[ 7 10 10 10]\n",
      "feature_indices_:\n",
      "[ 0  7 17 27 37]\n",
      "new number of features: 196\n"
     ]
    }
   ],
   "source": [
    "one_hot = OneHotEncoder(categorical_features=[1, 2, 3, 4], sparse=False,\n",
    "                        n_values=[7, 10, 10, 10])\n",
    "one_hot.fit(Imputer().fit_transform(all_data_original))\n",
    "print \"n_values_:\"\n",
    "print one_hot.n_values_\n",
    "print \"feature_indices_:\"\n",
    "print one_hot.feature_indices_\n",
    "all_data = one_hot.fit_transform(Imputer().fit_transform(all_data_original))\n",
    "n_features = all_data.shape[1] - 1\n",
    "print 'new number of features: {}'.format(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets.\n",
    "data_size = all_data.shape[0]\n",
    "training_size = data_size * 80/100\n",
    "indices = np.random.permutation(data_size)\n",
    "training_idx, test_idx = indices[:training_size], indices[training_size:]\n",
    "all_data_train, all_data_test = all_data_original[training_idx,:], all_data_original[test_idx,:]\n",
    "\n",
    "data_train = all_data_train[:,1:]\n",
    "targets_train = all_data_train[:,0]\n",
    "data_test = all_data_test[:,1:]\n",
    "targets_test = all_data_test[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Testing Algorithm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.is_file()\n",
    "steps = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('estimate', GradientBoostingRegressor(n_estimators=n_features, learning_rate=0.5))\n",
    "\n",
    "]\n",
    "\n",
    "est = Pipeline(steps)\n",
    "\n",
    "steps_bagging = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('estimate', BaggingRegressor(\n",
    "      GradientBoostingRegressor(\n",
    "        n_estimators=n_features, learning_rate=0.5,\n",
    "        verbose=1\n",
    "      )))\n",
    "\n",
    "]\n",
    "\n",
    "bagging_est = Pipeline(steps_bagging)\n",
    "bagging_est.fit(data_train, targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1341.5769           14.70m\n",
      "         2         672.8178           11.46m\n",
      "         3         486.7712           10.29m\n",
      "         4         426.1532            9.67m\n",
      "         5         394.1713            9.36m\n",
      "         6         374.1199            9.28m\n",
      "         7         364.4321            8.98m\n",
      "         8         356.9992            8.89m\n",
      "         9         340.4438            8.70m\n",
      "        10         331.6061            8.63m\n",
      "        20         270.0078            7.96m\n",
      "        30         234.5842            7.61m\n",
      "        40         207.8794            7.02m\n",
      "        50         183.1355            6.50m\n",
      "        60         167.9636            5.71m\n",
      "        70         158.5358            5.04m\n",
      "        80         147.5498            4.46m\n",
      "        90         135.4950            3.96m\n",
      "       100         127.4815            3.49m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1518.4832           10.51m\n",
      "         2         722.0450            7.94m\n",
      "         3         493.5493            7.04m\n",
      "         4         422.0309            6.66m\n",
      "         5         384.2887            6.34m\n",
      "         6         365.9387            6.11m\n",
      "         7         351.8895            5.94m\n",
      "         8         337.8139            5.84m\n",
      "         9         325.5187            5.70m\n",
      "        10         319.9580            5.60m\n",
      "        20         271.2655            5.03m\n",
      "        30         236.9103            4.68m\n",
      "        40         213.0857            4.37m\n",
      "        50         188.1036            4.07m\n",
      "        60         173.0913            3.79m\n",
      "        70         159.8373            3.52m\n",
      "        80         148.9218            3.23m\n",
      "        90         139.2556            2.95m\n",
      "       100         130.8034            2.67m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1371.1258           13.40m\n",
      "         2         681.3040           10.83m\n",
      "         3         485.8149            9.36m\n",
      "         4         419.1125            8.88m\n",
      "         5         392.7545            8.46m\n",
      "         6         374.4081            8.12m\n",
      "         7         366.2587            7.85m\n",
      "         8         357.3034            7.52m\n",
      "         9         350.7221            7.59m\n",
      "        10         338.8122            7.34m\n",
      "        20         287.7066            6.43m\n",
      "        30         249.9087            5.78m\n",
      "        40         217.0226            5.38m\n",
      "        50         193.7882            5.00m\n",
      "        60         175.6596            4.61m\n",
      "        70         164.6633            4.28m\n",
      "        80         154.3352            3.92m\n",
      "        90         143.0478            3.55m\n",
      "       100         137.0873            3.20m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1396.9109           12.99m\n",
      "         2         682.1921            9.27m\n",
      "         3         479.9279            8.33m\n",
      "         4         413.7660            8.07m\n",
      "         5         384.7397            7.49m\n",
      "         6         361.5347            7.08m\n",
      "         7         349.8554            7.23m\n",
      "         8         345.7395            6.92m\n",
      "         9         334.7315            6.85m\n",
      "        10         327.3081            6.84m\n",
      "        20         258.5431            6.16m\n",
      "        30         231.2480            5.59m\n",
      "        40         205.0113            5.22m\n",
      "        50         185.3496            5.00m\n",
      "        60         170.4929            4.95m\n",
      "        70         158.1743            4.83m\n",
      "        80         150.2883            4.66m\n",
      "        90         144.1101            4.37m\n",
      "       100         135.9585            4.03m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1671.3124           17.32m\n",
      "         2         794.9834           13.28m\n",
      "         3         545.0894           11.53m\n",
      "         4         468.1762           10.80m\n",
      "         5         431.7544           10.27m\n",
      "         6         407.2783            9.94m\n",
      "         7         390.9957            9.71m\n",
      "         8         371.3201            9.50m\n",
      "         9         363.2931            9.22m\n",
      "        10         354.2480            9.04m\n",
      "        20         294.5966            8.06m\n",
      "        30         247.7729            7.47m\n",
      "        40         222.5638            7.09m\n",
      "        50         198.2067            6.45m\n",
      "        60         181.1919            5.65m\n",
      "        70         167.0960            5.02m\n",
      "        80         156.5613            4.47m\n",
      "        90         147.6434            3.98m\n",
      "       100         137.3642            3.55m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1335.5039           12.12m\n",
      "         2         678.2863            9.03m\n",
      "         3         494.4016            8.05m\n",
      "         4         432.5953            7.47m\n",
      "         5         404.0946            7.06m\n",
      "         6         378.2821            6.77m\n",
      "         7         365.7584            6.72m\n",
      "         8         352.2175            6.55m\n",
      "         9         341.5453            6.43m\n",
      "        10         331.6381            6.30m\n",
      "        20         274.7994            5.60m\n",
      "        30         241.8451            5.21m\n",
      "        40         222.5012            4.83m\n",
      "        50         197.4690            4.54m\n",
      "        60         183.4577            4.19m\n",
      "        70         171.1824            3.84m\n",
      "        80         157.2356            3.53m\n",
      "        90         149.3085            3.22m\n",
      "       100         140.5858            2.91m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1364.2374           11.87m\n",
      "         2         687.4578            8.74m\n",
      "         3         496.9720            7.85m\n",
      "         4         421.0136            7.52m\n",
      "         5         383.0479            7.22m\n",
      "         6         365.6808            6.94m\n",
      "         7         345.7769            6.75m\n",
      "         8         339.1404            6.59m\n",
      "         9         330.3858            6.57m\n",
      "        10         320.7955            6.45m\n",
      "        20         259.1084            5.86m\n",
      "        30         226.5084            5.42m\n",
      "        40         201.9859            5.00m\n",
      "        50         176.0841            4.61m\n",
      "        60         160.9979            4.25m\n",
      "        70         150.3279            3.92m\n",
      "        80         141.4311            3.58m\n",
      "        90         135.5418            3.26m\n",
      "       100         126.7847            2.95m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1266.4536           11.27m\n",
      "         2         658.3211            8.47m\n",
      "         3         477.8557            7.52m\n",
      "         4         414.2515            7.02m\n",
      "         5         388.1341            6.81m\n",
      "         6         370.8282            6.56m\n",
      "         7         353.5740            6.38m\n",
      "         8         344.8323            6.23m\n",
      "         9         336.0869            6.07m\n",
      "        10         328.0489            5.95m\n",
      "        20         272.4116            5.35m\n",
      "        30         240.0749            4.97m\n",
      "        40         209.1064            4.65m\n",
      "        50         188.6133            4.33m\n",
      "        60         171.3237            4.00m\n",
      "        70         162.4787            3.67m\n",
      "        80         152.6390            3.37m\n",
      "        90         143.0782            3.08m\n",
      "       100         134.6611            2.80m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1453.3947           12.50m\n",
      "         2         702.1964            9.98m\n",
      "         3         495.0317            8.88m\n",
      "         4         424.6887            8.21m\n",
      "         5         392.4512            8.08m\n",
      "         6         371.2523            7.70m\n",
      "         7         366.1791            7.56m\n",
      "         8         349.4508            7.41m\n",
      "         9         344.2865            7.21m\n",
      "        10         332.4526            7.12m\n",
      "        20         271.5910            6.21m\n",
      "        30         236.1126            5.80m\n",
      "        40         208.8483            5.37m\n",
      "        50         188.3693            4.98m\n",
      "        60         173.4103            4.57m\n",
      "        70         160.7537            4.14m\n",
      "        80         148.2023            3.77m\n",
      "        90         138.9863            3.41m\n",
      "       100         129.7010            3.07m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1650.7749           10.82m\n",
      "         2         772.2223            8.09m\n",
      "         3         530.3388            7.26m\n",
      "         4         456.4882            6.91m\n",
      "         5         417.2416            6.59m\n",
      "         6         390.8760            6.33m\n",
      "         7         371.7564            6.17m\n",
      "         8         365.8141            6.00m\n",
      "         9         356.5420            5.87m\n",
      "        10         344.5792            5.78m\n",
      "        20         278.1346            5.20m\n",
      "        30         236.0032            4.82m\n",
      "        40         213.4316            4.52m\n",
      "        50         189.8995            4.25m\n",
      "        60         173.6669            4.00m\n",
      "        70         161.9432            3.70m\n",
      "        80         153.7255            3.41m\n",
      "        90         145.7485            3.15m\n",
      "       100         137.0379            2.89m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
       "       handle_unknown='error', n_values=[7, 10, 10, 10], sparse=False)), ('scale', StandardScaler(copy=True, with_mean=T...ax_samples=1.0, n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817805589442\n"
     ]
    }
   ],
   "source": [
    "test_predictions = bagging_est.predict(data_test)\n",
    "print(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(bagging_est, open(EST_PICKLE_FILENAME1, \"w\") )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
