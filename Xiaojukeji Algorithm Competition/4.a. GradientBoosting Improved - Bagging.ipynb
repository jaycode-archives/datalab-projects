{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME0 = 'GradientBoostingRegressor_initial.pkl'\n",
    "EST_PICKLE_FILENAME1 = 'BaggingGradientBoostingRegressor_initial.pkl'\n",
    "EST_PICKLE_FILENAME2 = 'BaggingGradientBoostingRegressor_final.pkl'\n",
    "LOAD_EST = False\n",
    "RUN_SIMULATED_FEATURE_SELECTION = False\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\tbusy_time\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample, IF(timeofday_slot >= 50 AND timeofday_slot <= 53, 1, 0) AS busy_time\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0\n",
    "LIMIT 5000\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5000 rows\n",
      "processed 0 rows\n"
     ]
    }
   ],
   "source": [
    "query = bq.Query(q_all)\n",
    "tableresult = query.results()\n",
    "\n",
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_values_:\n",
      "[ 7 10 10 10]\n",
      "feature_indices_:\n",
      "[ 0  7 17 27 37]\n",
      "new number of features: 196\n"
     ]
    }
   ],
   "source": [
    "one_hot = OneHotEncoder(categorical_features=[1, 2, 3, 4], sparse=False,\n",
    "                        n_values=[7, 10, 10, 10])\n",
    "one_hot.fit(Imputer().fit_transform(all_data_original))\n",
    "print \"n_values_:\"\n",
    "print one_hot.n_values_\n",
    "print \"feature_indices_:\"\n",
    "print one_hot.feature_indices_\n",
    "all_data = StandardScaler().fit_transform(one_hot.fit_transform(Imputer().fit_transform(\n",
    "      all_data_original)))\n",
    "n_features = all_data.shape[1] - 1\n",
    "print 'new number of features: {}'.format(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if RUN_SIMULATED_FEATURE_SELECTION:\n",
    "  rfecv = RFECV(SVR(kernel='linear'), scoring=mape_scorer, verbose=1)\n",
    "  rfecv.fit(all_data[training_idx,1:], targets_train)\n",
    "\n",
    "  print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "  # Plot number of features VS. cross-validation scores\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Number of features selected\")\n",
    "  plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "  plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "  plt.show()\n",
    "  rfecv.transform(all_data[training_idx,1:]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets.\n",
    "data_size = all_data.shape[0]\n",
    "training_size = data_size * 80/100\n",
    "indices = np.random.permutation(data_size)\n",
    "training_idx, test_idx = indices[:training_size], indices[training_size:]\n",
    "all_data_train, all_data_test = all_data_original[training_idx,:], all_data_original[test_idx,:]\n",
    "\n",
    "data_train = all_data_train[:,1:]\n",
    "targets_train = all_data_train[:,0]\n",
    "data_test = all_data_test[:,1:]\n",
    "targets_test = all_data_test[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Initial Bagging Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fold with 1 / 196 feature ranks, score=-1.055186\n",
      "Finished fold with 2 / 196 feature ranks, score=-1.019186\n",
      "Finished fold with 3 / 196 feature ranks, score=-1.013523\n",
      "Finished fold with 4 / 196 feature ranks, score=-0.994215\n",
      "Finished fold with 5 / 196 feature ranks, score=-0.986181\n",
      "Finished fold with 6 / 196 feature ranks, score=-0.969988\n",
      "Finished fold with 7 / 196 feature ranks, score=-0.980457\n",
      "Finished fold with 8 / 196 feature ranks, score=-0.979808\n",
      "Finished fold with 9 / 196 feature ranks, score=-0.979818\n",
      "Finished fold with 10 / 196 feature ranks, score=-0.982698\n",
      "Finished fold with 11 / 196 feature ranks, score=-0.983559\n",
      "Finished fold with 12 / 196 feature ranks, score=-0.984306\n",
      "Finished fold with 13 / 196 feature ranks, score=-0.987338\n",
      "Finished fold with 14 / 196 feature ranks, score=-0.986814\n",
      "Finished fold with 15 / 196 feature ranks, score=-0.979035\n",
      "Finished fold with 16 / 196 feature ranks, score=-0.978511\n",
      "Finished fold with 17 / 196 feature ranks, score=-0.975568\n",
      "Finished fold with 18 / 196 feature ranks, score=-0.976920\n",
      "Finished fold with 19 / 196 feature ranks, score=-0.975429\n",
      "Finished fold with 20 / 196 feature ranks, score=-0.976436\n",
      "Finished fold with 21 / 196 feature ranks, score=-0.979541\n",
      "Finished fold with 22 / 196 feature ranks, score=-0.978462\n",
      "Finished fold with 23 / 196 feature ranks, score=-0.976898\n",
      "Finished fold with 24 / 196 feature ranks, score=-0.976773\n",
      "Finished fold with 25 / 196 feature ranks, score=-0.978559\n",
      "Finished fold with 26 / 196 feature ranks, score=-0.979694\n",
      "Finished fold with 27 / 196 feature ranks, score=-0.979682\n",
      "Finished fold with 28 / 196 feature ranks, score=-0.975938\n",
      "Finished fold with 29 / 196 feature ranks, score=-0.971298\n",
      "Finished fold with 30 / 196 feature ranks, score=-0.971005\n",
      "Finished fold with 31 / 196 feature ranks, score=-0.969956\n",
      "Finished fold with 32 / 196 feature ranks, score=-0.969456\n",
      "Finished fold with 33 / 196 feature ranks, score=-0.963425\n",
      "Finished fold with 34 / 196 feature ranks, score=-0.965307\n",
      "Finished fold with 35 / 196 feature ranks, score=-0.963667\n",
      "Finished fold with 36 / 196 feature ranks, score=-0.960763\n",
      "Finished fold with 37 / 196 feature ranks, score=-0.961228\n",
      "Finished fold with 38 / 196 feature ranks, score=-0.962412\n",
      "Finished fold with 39 / 196 feature ranks, score=-0.962331\n",
      "Finished fold with 40 / 196 feature ranks, score=-0.968971\n",
      "Finished fold with 41 / 196 feature ranks, score=-0.967860\n",
      "Finished fold with 42 / 196 feature ranks, score=-0.969557\n",
      "Finished fold with 43 / 196 feature ranks, score=-0.971191\n",
      "Finished fold with 44 / 196 feature ranks, score=-0.971135\n",
      "Finished fold with 45 / 196 feature ranks, score=-0.972221\n",
      "Finished fold with 46 / 196 feature ranks, score=-0.970463\n",
      "Finished fold with 47 / 196 feature ranks, score=-0.967112\n",
      "Finished fold with 48 / 196 feature ranks, score=-0.967079\n",
      "Finished fold with 49 / 196 feature ranks, score=-0.966203\n",
      "Finished fold with 50 / 196 feature ranks, score=-0.967019\n",
      "Finished fold with 51 / 196 feature ranks, score=-0.966519\n",
      "Finished fold with 52 / 196 feature ranks, score=-0.969012\n",
      "Finished fold with 53 / 196 feature ranks, score=-0.970289\n",
      "Finished fold with 54 / 196 feature ranks, score=-0.968632\n",
      "Finished fold with 55 / 196 feature ranks, score=-0.967914\n",
      "Finished fold with 56 / 196 feature ranks, score=-0.967966\n",
      "Finished fold with 57 / 196 feature ranks, score=-0.968321\n",
      "Finished fold with 58 / 196 feature ranks, score=-0.956122\n",
      "Finished fold with 59 / 196 feature ranks, score=-0.958548\n",
      "Finished fold with 60 / 196 feature ranks, score=-0.959021\n",
      "Finished fold with 61 / 196 feature ranks, score=-0.958828\n",
      "Finished fold with 62 / 196 feature ranks, score=-0.958246\n",
      "Finished fold with 63 / 196 feature ranks, score=-0.953047\n",
      "Finished fold with 64 / 196 feature ranks, score=-0.952758\n",
      "Finished fold with 65 / 196 feature ranks, score=-0.954722\n",
      "Finished fold with 66 / 196 feature ranks, score=-0.954911\n",
      "Finished fold with 67 / 196 feature ranks, score=-0.954457\n",
      "Finished fold with 68 / 196 feature ranks, score=-0.955900\n",
      "Finished fold with 69 / 196 feature ranks, score=-0.955578\n",
      "Finished fold with 70 / 196 feature ranks, score=-0.947186\n",
      "Finished fold with 71 / 196 feature ranks, score=-0.947202\n",
      "Finished fold with 72 / 196 feature ranks, score=-0.946044\n",
      "Finished fold with 73 / 196 feature ranks, score=-0.945557\n",
      "Finished fold with 74 / 196 feature ranks, score=-0.945200\n",
      "Finished fold with 75 / 196 feature ranks, score=-0.945005\n",
      "Finished fold with 76 / 196 feature ranks, score=-0.945040\n",
      "Finished fold with 77 / 196 feature ranks, score=-0.944516\n",
      "Finished fold with 78 / 196 feature ranks, score=-0.944469\n",
      "Finished fold with 79 / 196 feature ranks, score=-0.948539\n",
      "Finished fold with 80 / 196 feature ranks, score=-0.948550\n",
      "Finished fold with 81 / 196 feature ranks, score=-0.947162\n",
      "Finished fold with 82 / 196 feature ranks, score=-0.946465\n",
      "Finished fold with 83 / 196 feature ranks, score=-0.946917\n",
      "Finished fold with 84 / 196 feature ranks, score=-0.946873\n",
      "Finished fold with 85 / 196 feature ranks, score=-0.946864\n",
      "Finished fold with 86 / 196 feature ranks, score=-0.946090\n",
      "Finished fold with 87 / 196 feature ranks, score=-0.946077\n",
      "Finished fold with 88 / 196 feature ranks, score=-0.945661\n",
      "Finished fold with 89 / 196 feature ranks, score=-0.945526\n",
      "Finished fold with 90 / 196 feature ranks, score=-0.945923\n",
      "Finished fold with 91 / 196 feature ranks, score=-0.945774\n",
      "Finished fold with 92 / 196 feature ranks, score=-0.946705\n",
      "Finished fold with 93 / 196 feature ranks, score=-0.946587\n",
      "Finished fold with 94 / 196 feature ranks, score=-0.946952\n",
      "Finished fold with 95 / 196 feature ranks, score=-0.946788\n",
      "Finished fold with 96 / 196 feature ranks, score=-0.947034\n",
      "Finished fold with 97 / 196 feature ranks, score=-0.947385\n",
      "Finished fold with 98 / 196 feature ranks, score=-0.948949\n",
      "Finished fold with 99 / 196 feature ranks, score=-0.949209\n",
      "Finished fold with 100 / 196 feature ranks, score=-0.948849\n",
      "Finished fold with 101 / 196 feature ranks, score=-0.948774\n",
      "Finished fold with 102 / 196 feature ranks, score=-0.948599\n",
      "Finished fold with 103 / 196 feature ranks, score=-0.948499\n",
      "Finished fold with 104 / 196 feature ranks, score=-0.948221\n",
      "Finished fold with 105 / 196 feature ranks, score=-0.948101\n",
      "Finished fold with 106 / 196 feature ranks, score=-0.948133\n",
      "Finished fold with 107 / 196 feature ranks, score=-0.947858\n",
      "Finished fold with 108 / 196 feature ranks, score=-0.947838\n",
      "Finished fold with 109 / 196 feature ranks, score=-0.947708\n",
      "Finished fold with 110 / 196 feature ranks, score=-0.945159\n",
      "Finished fold with 111 / 196 feature ranks, score=-0.945203\n",
      "Finished fold with 112 / 196 feature ranks, score=-0.944479\n",
      "Finished fold with 113 / 196 feature ranks, score=-0.957947\n",
      "Finished fold with 114 / 196 feature ranks, score=-0.957411\n",
      "Finished fold with 115 / 196 feature ranks, score=-0.956118\n",
      "Finished fold with 116 / 196 feature ranks, score=-0.955981\n",
      "Finished fold with 117 / 196 feature ranks, score=-0.956138\n",
      "Finished fold with 118 / 196 feature ranks, score=-0.956183\n",
      "Finished fold with 119 / 196 feature ranks, score=-0.956305\n",
      "Finished fold with 120 / 196 feature ranks, score=-0.956431\n",
      "Finished fold with 121 / 196 feature ranks, score=-0.956427\n",
      "Finished fold with 122 / 196 feature ranks, score=-0.956271\n",
      "Finished fold with 123 / 196 feature ranks, score=-0.956537\n",
      "Finished fold with 124 / 196 feature ranks, score=-0.956519\n",
      "Finished fold with 125 / 196 feature ranks, score=-0.959877\n",
      "Finished fold with 126 / 196 feature ranks, score=-0.960062\n",
      "Finished fold with 127 / 196 feature ranks, score=-0.959929\n",
      "Finished fold with 128 / 196 feature ranks, score=-0.960044\n",
      "Finished fold with 129 / 196 feature ranks, score=-0.960250\n",
      "Finished fold with 130 / 196 feature ranks, score=-0.960373\n",
      "Finished fold with 131 / 196 feature ranks, score=-0.960424\n",
      "Finished fold with 132 / 196 feature ranks, score=-0.960401\n",
      "Finished fold with 133 / 196 feature ranks, score=-0.960402\n",
      "Finished fold with 134 / 196 feature ranks, score=-0.959355\n",
      "Finished fold with 135 / 196 feature ranks, score=-0.959346\n",
      "Finished fold with 136 / 196 feature ranks, score=-0.959363\n",
      "Finished fold with 137 / 196 feature ranks, score=-0.959564\n",
      "Finished fold with 138 / 196 feature ranks, score=-0.959531\n",
      "Finished fold with 139 / 196 feature ranks, score=-0.959549\n",
      "Finished fold with 140 / 196 feature ranks, score=-0.959609\n",
      "Finished fold with 141 / 196 feature ranks, score=-0.959271\n",
      "Finished fold with 142 / 196 feature ranks, score=-0.959325\n",
      "Finished fold with 143 / 196 feature ranks, score=-0.960321\n",
      "Finished fold with 144 / 196 feature ranks, score=-0.962182\n",
      "Finished fold with 145 / 196 feature ranks, score=-0.962136\n",
      "Finished fold with 146 / 196 feature ranks, score=-0.964071\n",
      "Finished fold with 147 / 196 feature ranks, score=-0.964016\n",
      "Finished fold with 148 / 196 feature ranks, score=-0.964259\n",
      "Finished fold with 149 / 196 feature ranks, score=-0.964175\n",
      "Finished fold with 150 / 196 feature ranks, score=-0.964406\n",
      "Finished fold with 151 / 196 feature ranks, score=-0.964771\n",
      "Finished fold with 152 / 196 feature ranks, score=-0.964874\n",
      "Finished fold with 153 / 196 feature ranks, score=-0.965185\n",
      "Finished fold with 154 / 196 feature ranks, score=-0.965247\n",
      "Finished fold with 155 / 196 feature ranks, score=-0.965237\n",
      "Finished fold with 156 / 196 feature ranks, score=-0.965147\n",
      "Finished fold with 157 / 196 feature ranks, score=-0.963621\n",
      "Finished fold with 158 / 196 feature ranks, score=-0.963602\n",
      "Finished fold with 159 / 196 feature ranks, score=-0.963571\n",
      "Finished fold with 160 / 196 feature ranks, score=-0.963599\n",
      "Finished fold with 161 / 196 feature ranks, score=-0.963396\n",
      "Finished fold with 162 / 196 feature ranks, score=-0.963380\n",
      "Finished fold with 163 / 196 feature ranks, score=-0.963373\n",
      "Finished fold with 164 / 196 feature ranks, score=-0.962795\n",
      "Finished fold with 165 / 196 feature ranks, score=-0.962757\n",
      "Finished fold with 166 / 196 feature ranks, score=-0.962380\n",
      "Finished fold with 167 / 196 feature ranks, score=-0.962379\n",
      "Finished fold with 168 / 196 feature ranks, score=-0.962449\n",
      "Finished fold with 169 / 196 feature ranks, score=-0.963010\n",
      "Finished fold with 170 / 196 feature ranks, score=-0.963774\n",
      "Finished fold with 171 / 196 feature ranks, score=-0.963881\n",
      "Finished fold with 172 / 196 feature ranks, score=-0.962805\n",
      "Finished fold with 173 / 196 feature ranks, score=-0.962938\n",
      "Finished fold with 174 / 196 feature ranks, score=-0.962854\n",
      "Finished fold with 175 / 196 feature ranks, score=-0.962639\n",
      "Finished fold with 176 / 196 feature ranks, score=-0.962507\n",
      "Finished fold with 177 / 196 feature ranks, score=-0.962443\n",
      "Finished fold with 178 / 196 feature ranks, score=-0.962482\n",
      "Finished fold with 179 / 196 feature ranks, score=-0.962408\n",
      "Finished fold with 180 / 196 feature ranks, score=-0.962103\n",
      "Finished fold with 181 / 196 feature ranks, score=-0.962324\n",
      "Finished fold with 182 / 196 feature ranks, score=-0.962561\n",
      "Finished fold with 183 / 196 feature ranks, score=-0.962463\n",
      "Finished fold with 184 / 196 feature ranks, score=-0.962372\n",
      "Finished fold with 185 / 196 feature ranks, score=-0.962428\n",
      "Finished fold with 186 / 196 feature ranks, score=-0.962390\n",
      "Finished fold with 187 / 196 feature ranks, score=-0.962476\n",
      "Finished fold with 188 / 196 feature ranks, score=-0.962446\n",
      "Finished fold with 189 / 196 feature ranks, score=-0.962446\n",
      "Finished fold with 190 / 196 feature ranks, score=-0.962446\n",
      "Finished fold with 191 / 196 feature ranks, score=-0.962446\n",
      "Finished fold with 192 / 196 feature ranks, score=-0.962446\n",
      "Finished fold with 193 / 196 feature ranks, score=-0.962446\n",
      "Finished fold with 194 / 196 feature ranks, score=-0.962446\n",
      "Finished fold with 195 / 196 feature ranks, score=-0.962446\n",
      "Finished fold with 196 / 196 feature ranks, score=-0.962446\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(EST_PICKLE_FILENAME1) and LOAD_EST:\n",
    "  bagging_est = pickle.load(open(EST_PICKLE_FILENAME1, 'r'))\n",
    "  est = pickle.load(open(EST_PICKLE_FILENAME0, 'r'))\n",
    "else:\n",
    "  steps = [\n",
    "    ('impute', Imputer()),\n",
    "    # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "    # should start at index 0.\n",
    "    ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                             n_values=[7, 10, 10, 10])),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('select_features', RFECV(SVR(kernel='linear'), scoring=mape_scorer, verbose=1)),\n",
    "    ('estimate', GradientBoostingRegressor(n_estimators=190, learning_rate=0.5))\n",
    "\n",
    "  ]\n",
    "\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data_train, targets_train)\n",
    "#   pickle.dump(bagging_est, open(EST_PICKLE_FILENAME0, \"w\") )\n",
    "\n",
    "  steps_bagging = [\n",
    "    ('impute', Imputer()),\n",
    "    # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "    # should start at index 0.\n",
    "    ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                             n_values=[7, 10, 10, 10])),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('select_features', RFECV(SVR(kernel='linear'), scoring=mape_scorer, verbose=1)),\n",
    "    ('estimate', BaggingRegressor(\n",
    "        GradientBoostingRegressor(\n",
    "          n_estimators=190, learning_rate=0.5,\n",
    "          verbose=1\n",
    "        )))\n",
    "  ]\n",
    "\n",
    "  bagging_est = Pipeline(steps_bagging)\n",
    "  bagging_est.fit(data_train, targets_train)\n",
    "#   pickle.dump(bagging_est, open(EST_PICKLE_FILENAME1, \"w\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = est.predict(data_test)\n",
    "print 'gradient boosting score: {}'.format(mape(targets_test, test_predictions))\n",
    "\n",
    "test_predictions = bagging_est.predict(data_test)\n",
    "print 'gradient boosting with bagging score: {}'.format(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient boosting score: 0.761279405029\n",
      "gradient boosting with bagging score: 0.730381766421\n"
     ]
    }
   ],
   "source": [
    "# Without RFECV\n",
    "test_predictions = est.predict(data_test)\n",
    "print 'gradient boosting score: {}'.format(mape(targets_test, test_predictions))\n",
    "\n",
    "test_predictions = bagging_est.predict(data_test)\n",
    "print 'gradient boosting with bagging score: {}'.format(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without RFECV, the scores were:\n",
    "\n",
    "gradient boosting score: 0.761279405029\n",
    "\n",
    "gradient boosting with bagging score: 0.730381766421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps_bagging = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('estimate', BaggingRegressor(\n",
    "      GradientBoostingRegressor(\n",
    "        n_estimators=n_features, learning_rate=0.5,\n",
    "        verbose=1\n",
    "      )))\n",
    "\n",
    "]\n",
    "\n",
    "params = {\n",
    "  'estimate__learning_rate': [0.1, 0.5, 1, 10],\n",
    "  'estimate__n_estimators': [i for i in range(110, n_features, 20)],\n",
    "#   'estimate__loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "search_params = RandomizedSearchCV(\n",
    "  estimator=est,\n",
    "  param_distributions=params,\n",
    "  cv=5,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=2,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "search_params.fit(data_train_original, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
