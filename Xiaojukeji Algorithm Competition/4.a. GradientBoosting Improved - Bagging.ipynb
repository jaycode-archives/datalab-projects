{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME1 = 'BaggingGradientBoostingRegressor_initial.pkl'\n",
    "EST_PICKLE_FILENAME2 = 'BaggingGradientBoostingRegressor_final.pkl'\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\tbusy_time\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample, IF(timeofday_slot >= 50 AND timeofday_slot <= 53, 1, 0) AS busy_time\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0\n",
    "LIMIT 5000\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5000 rows\n",
      "processed 0 rows\n"
     ]
    }
   ],
   "source": [
    "query = bq.Query(q_all)\n",
    "tableresult = query.results()\n",
    "\n",
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_values_:\n",
      "[ 7 10 10 10]\n",
      "feature_indices_:\n",
      "[ 0  7 17 27 37]\n",
      "new number of features: 196\n"
     ]
    }
   ],
   "source": [
    "one_hot = OneHotEncoder(categorical_features=[1, 2, 3, 4], sparse=False,\n",
    "                        n_values=[7, 10, 10, 10])\n",
    "one_hot.fit(Imputer().fit_transform(all_data_original))\n",
    "print \"n_values_:\"\n",
    "print one_hot.n_values_\n",
    "print \"feature_indices_:\"\n",
    "print one_hot.feature_indices_\n",
    "all_data = StandardScaler().fit_transform(one_hot.fit_transform(Imputer().fit_transform(\n",
    "      all_data_original)))\n",
    "n_features = all_data.shape[1] - 1\n",
    "print 'new number of features: {}'.format(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets.\n",
    "data_size = all_data.shape[0]\n",
    "training_size = data_size * 80/100\n",
    "indices = np.random.permutation(data_size)\n",
    "training_idx, test_idx = indices[:training_size], indices[training_size:]\n",
    "all_data_train, all_data_test = all_data_original[training_idx,:], all_data_original[test_idx,:]\n",
    "\n",
    "data_train = all_data_train[:,1:]\n",
    "targets_train = all_data_train[:,0]\n",
    "data_test = all_data_test[:,1:]\n",
    "targets_test = all_data_test[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fold with 1 / 196 feature ranks, score=-0.046896\n",
      "Finished fold with 2 / 196 feature ranks, score=-0.023659\n",
      "Finished fold with 3 / 196 feature ranks, score=-0.017603\n",
      "Finished fold with 4 / 196 feature ranks, score=-0.006706\n",
      "Finished fold with 5 / 196 feature ranks, score=-0.012832\n",
      "Finished fold with 6 / 196 feature ranks, score=-0.016082\n",
      "Finished fold with 7 / 196 feature ranks, score=-0.017269\n",
      "Finished fold with 8 / 196 feature ranks, score=-0.018012\n",
      "Finished fold with 9 / 196 feature ranks, score=-0.016229\n",
      "Finished fold with 10 / 196 feature ranks, score=-0.027767\n",
      "Finished fold with 11 / 196 feature ranks, score=-0.027933\n",
      "Finished fold with 12 / 196 feature ranks, score=-0.026494\n",
      "Finished fold with 13 / 196 feature ranks, score=-0.026253\n",
      "Finished fold with 14 / 196 feature ranks, score=-0.026390\n",
      "Finished fold with 15 / 196 feature ranks, score=-0.026775\n",
      "Finished fold with 16 / 196 feature ranks, score=-0.026718\n",
      "Finished fold with 17 / 196 feature ranks, score=-0.019817\n",
      "Finished fold with 18 / 196 feature ranks, score=-0.020107\n",
      "Finished fold with 19 / 196 feature ranks, score=-0.015555\n",
      "Finished fold with 20 / 196 feature ranks, score=-0.009879\n",
      "Finished fold with 21 / 196 feature ranks, score=-0.011731\n",
      "Finished fold with 22 / 196 feature ranks, score=-0.010210\n",
      "Finished fold with 23 / 196 feature ranks, score=-0.010710\n",
      "Finished fold with 24 / 196 feature ranks, score=-0.010778\n",
      "Finished fold with 25 / 196 feature ranks, score=-0.010782\n",
      "Finished fold with 26 / 196 feature ranks, score=-0.012683\n",
      "Finished fold with 27 / 196 feature ranks, score=-0.014691\n",
      "Finished fold with 28 / 196 feature ranks, score=-0.015457\n",
      "Finished fold with 29 / 196 feature ranks, score=-0.014548\n",
      "Finished fold with 30 / 196 feature ranks, score=-0.015473\n",
      "Finished fold with 31 / 196 feature ranks, score=-0.015599\n",
      "Finished fold with 32 / 196 feature ranks, score=-0.015387\n",
      "Finished fold with 33 / 196 feature ranks, score=-0.016410\n",
      "Finished fold with 34 / 196 feature ranks, score=-0.016518\n",
      "Finished fold with 35 / 196 feature ranks, score=-0.016143\n",
      "Finished fold with 36 / 196 feature ranks, score=-0.015535\n",
      "Finished fold with 37 / 196 feature ranks, score=-0.015655\n",
      "Finished fold with 38 / 196 feature ranks, score=-0.015583\n",
      "Finished fold with 39 / 196 feature ranks, score=-0.015635\n",
      "Finished fold with 40 / 196 feature ranks, score=-0.015599\n",
      "Finished fold with 41 / 196 feature ranks, score=-0.015393\n",
      "Finished fold with 42 / 196 feature ranks, score=-0.015397\n",
      "Finished fold with 43 / 196 feature ranks, score=-0.015525\n",
      "Finished fold with 44 / 196 feature ranks, score=-0.015828\n",
      "Finished fold with 45 / 196 feature ranks, score=-0.015923\n",
      "Finished fold with 46 / 196 feature ranks, score=-0.015795\n",
      "Finished fold with 47 / 196 feature ranks, score=-0.016504\n",
      "Finished fold with 48 / 196 feature ranks, score=-0.016469\n",
      "Finished fold with 49 / 196 feature ranks, score=-0.016432\n",
      "Finished fold with 50 / 196 feature ranks, score=-0.016241\n",
      "Finished fold with 51 / 196 feature ranks, score=-0.016512\n",
      "Finished fold with 52 / 196 feature ranks, score=-0.016605\n",
      "Finished fold with 53 / 196 feature ranks, score=-0.016671\n",
      "Finished fold with 54 / 196 feature ranks, score=-0.016659\n",
      "Finished fold with 55 / 196 feature ranks, score=-0.016834\n",
      "Finished fold with 56 / 196 feature ranks, score=-0.016964\n",
      "Finished fold with 57 / 196 feature ranks, score=-0.016901\n",
      "Finished fold with 58 / 196 feature ranks, score=-0.016543\n",
      "Finished fold with 59 / 196 feature ranks, score=-0.016420\n",
      "Finished fold with 60 / 196 feature ranks, score=-0.016453\n",
      "Finished fold with 61 / 196 feature ranks, score=-0.017154\n",
      "Finished fold with 62 / 196 feature ranks, score=-0.017292\n",
      "Finished fold with 63 / 196 feature ranks, score=-0.016911\n",
      "Finished fold with 64 / 196 feature ranks, score=-0.017253\n",
      "Finished fold with 65 / 196 feature ranks, score=-0.017422\n",
      "Finished fold with 66 / 196 feature ranks, score=-0.016879\n",
      "Finished fold with 67 / 196 feature ranks, score=-0.017189\n",
      "Finished fold with 68 / 196 feature ranks, score=-0.017004\n",
      "Finished fold with 69 / 196 feature ranks, score=-0.017184\n",
      "Finished fold with 70 / 196 feature ranks, score=-0.017076\n",
      "Finished fold with 71 / 196 feature ranks, score=-0.016977\n",
      "Finished fold with 72 / 196 feature ranks, score=-0.016782\n",
      "Finished fold with 73 / 196 feature ranks, score=-0.016500\n",
      "Finished fold with 74 / 196 feature ranks, score=-0.016722\n",
      "Finished fold with 75 / 196 feature ranks, score=-0.016776\n",
      "Finished fold with 76 / 196 feature ranks, score=-0.016667\n",
      "Finished fold with 77 / 196 feature ranks, score=-0.016878\n",
      "Finished fold with 78 / 196 feature ranks, score=-0.016956\n",
      "Finished fold with 79 / 196 feature ranks, score=-0.016750\n",
      "Finished fold with 80 / 196 feature ranks, score=-0.016509\n",
      "Finished fold with 81 / 196 feature ranks, score=-0.016549\n",
      "Finished fold with 82 / 196 feature ranks, score=-0.016768\n",
      "Finished fold with 83 / 196 feature ranks, score=-0.016873\n",
      "Finished fold with 84 / 196 feature ranks, score=-0.016951\n",
      "Finished fold with 85 / 196 feature ranks, score=-0.016849\n",
      "Finished fold with 86 / 196 feature ranks, score=-0.016814\n",
      "Finished fold with 87 / 196 feature ranks, score=-0.016800\n",
      "Finished fold with 88 / 196 feature ranks, score=-0.016815\n",
      "Finished fold with 89 / 196 feature ranks, score=-0.016788\n",
      "Finished fold with 90 / 196 feature ranks, score=-0.016908\n",
      "Finished fold with 91 / 196 feature ranks, score=-0.016896\n",
      "Finished fold with 92 / 196 feature ranks, score=-0.016939\n",
      "Finished fold with 93 / 196 feature ranks, score=-0.016574\n",
      "Finished fold with 94 / 196 feature ranks, score=-0.016618\n",
      "Finished fold with 95 / 196 feature ranks, score=-0.016884\n",
      "Finished fold with 96 / 196 feature ranks, score=-0.016848\n",
      "Finished fold with 97 / 196 feature ranks, score=-0.016890\n",
      "Finished fold with 98 / 196 feature ranks, score=-0.016802\n",
      "Finished fold with 99 / 196 feature ranks, score=-0.016787\n",
      "Finished fold with 100 / 196 feature ranks, score=-0.016852\n",
      "Finished fold with 101 / 196 feature ranks, score=-0.016790\n",
      "Finished fold with 102 / 196 feature ranks, score=-0.016840\n",
      "Finished fold with 103 / 196 feature ranks, score=-0.016800\n",
      "Finished fold with 104 / 196 feature ranks, score=-0.016781\n",
      "Finished fold with 105 / 196 feature ranks, score=-0.016795\n",
      "Finished fold with 106 / 196 feature ranks, score=-0.016771\n",
      "Finished fold with 107 / 196 feature ranks, score=-0.016718\n",
      "Finished fold with 108 / 196 feature ranks, score=-0.016743\n",
      "Finished fold with 109 / 196 feature ranks, score=-0.016758\n",
      "Finished fold with 110 / 196 feature ranks, score=-0.016775\n",
      "Finished fold with 111 / 196 feature ranks, score=-0.016674\n",
      "Finished fold with 112 / 196 feature ranks, score=-0.016828\n",
      "Finished fold with 113 / 196 feature ranks, score=-0.016658\n",
      "Finished fold with 114 / 196 feature ranks, score=-0.016719\n",
      "Finished fold with 115 / 196 feature ranks, score=-0.016715\n",
      "Finished fold with 116 / 196 feature ranks, score=-0.016696\n",
      "Finished fold with 117 / 196 feature ranks, score=-0.016822\n",
      "Finished fold with 118 / 196 feature ranks, score=-0.016670\n",
      "Finished fold with 119 / 196 feature ranks, score=-0.016583\n",
      "Finished fold with 120 / 196 feature ranks, score=-0.016613\n",
      "Finished fold with 121 / 196 feature ranks, score=-0.016486\n",
      "Finished fold with 122 / 196 feature ranks, score=-0.016571\n",
      "Finished fold with 123 / 196 feature ranks, score=-0.016751\n",
      "Finished fold with 124 / 196 feature ranks, score=-0.016611\n",
      "Finished fold with 125 / 196 feature ranks, score=-0.016641\n",
      "Finished fold with 126 / 196 feature ranks, score=-0.016662\n",
      "Finished fold with 127 / 196 feature ranks, score=-0.016604\n",
      "Finished fold with 128 / 196 feature ranks, score=-0.016632\n",
      "Finished fold with 129 / 196 feature ranks, score=-0.016586\n",
      "Finished fold with 130 / 196 feature ranks, score=-0.016700\n",
      "Finished fold with 131 / 196 feature ranks, score=-0.016699\n",
      "Finished fold with 132 / 196 feature ranks, score=-0.016556\n",
      "Finished fold with 133 / 196 feature ranks, score=-0.016634\n",
      "Finished fold with 134 / 196 feature ranks, score=-0.016601\n",
      "Finished fold with 135 / 196 feature ranks, score=-0.016687\n",
      "Finished fold with 136 / 196 feature ranks, score=-0.016653\n",
      "Finished fold with 137 / 196 feature ranks, score=-0.016570\n",
      "Finished fold with 138 / 196 feature ranks, score=-0.016764\n",
      "Finished fold with 139 / 196 feature ranks, score=-0.016675\n",
      "Finished fold with 140 / 196 feature ranks, score=-0.016681\n",
      "Finished fold with 141 / 196 feature ranks, score=-0.016663\n",
      "Finished fold with 142 / 196 feature ranks, score=-0.016718\n",
      "Finished fold with 143 / 196 feature ranks, score=-0.016697\n",
      "Finished fold with 144 / 196 feature ranks, score=-0.016743\n",
      "Finished fold with 145 / 196 feature ranks, score=-0.016731\n",
      "Finished fold with 146 / 196 feature ranks, score=-0.016723\n",
      "Finished fold with 147 / 196 feature ranks, score=-0.016754\n",
      "Finished fold with 148 / 196 feature ranks, score=-0.016673\n",
      "Finished fold with 149 / 196 feature ranks, score=-0.016717\n",
      "Finished fold with 150 / 196 feature ranks, score=-0.016684\n",
      "Finished fold with 151 / 196 feature ranks, score=-0.016709\n",
      "Finished fold with 152 / 196 feature ranks, score=-0.016746\n",
      "Finished fold with 153 / 196 feature ranks, score=-0.016723\n",
      "Finished fold with 154 / 196 feature ranks, score=-0.016689\n",
      "Finished fold with 155 / 196 feature ranks, score=-0.016743\n",
      "Finished fold with 156 / 196 feature ranks, score=-0.016604\n",
      "Finished fold with 157 / 196 feature ranks, score=-0.016707\n",
      "Finished fold with 158 / 196 feature ranks, score=-0.016706\n",
      "Finished fold with 159 / 196 feature ranks, score=-0.016731\n",
      "Finished fold with 160 / 196 feature ranks, score=-0.016679\n",
      "Finished fold with 161 / 196 feature ranks, score=-0.016826\n",
      "Finished fold with 162 / 196 feature ranks, score=-0.016719\n",
      "Finished fold with 163 / 196 feature ranks, score=-0.016666\n",
      "Finished fold with 164 / 196 feature ranks, score=-0.016809\n",
      "Finished fold with 165 / 196 feature ranks, score=-0.016666\n",
      "Finished fold with 166 / 196 feature ranks, score=-0.016616\n",
      "Finished fold with 167 / 196 feature ranks, score=-0.016646\n",
      "Finished fold with 168 / 196 feature ranks, score=-0.016653\n",
      "Finished fold with 169 / 196 feature ranks, score=-0.016651\n",
      "Finished fold with 170 / 196 feature ranks, score=-0.016627\n",
      "Finished fold with 171 / 196 feature ranks, score=-0.016598\n",
      "Finished fold with 172 / 196 feature ranks, score=-0.016680\n",
      "Finished fold with 173 / 196 feature ranks, score=-0.016746\n",
      "Finished fold with 174 / 196 feature ranks, score=-0.016661\n",
      "Finished fold with 175 / 196 feature ranks, score=-0.016764\n",
      "Finished fold with 176 / 196 feature ranks, score=-0.016624\n",
      "Finished fold with 177 / 196 feature ranks, score=-0.016677\n",
      "Finished fold with 178 / 196 feature ranks, score=-0.016681\n",
      "Finished fold with 179 / 196 feature ranks, score=-0.016629\n",
      "Finished fold with 180 / 196 feature ranks, score=-0.016776\n",
      "Finished fold with 181 / 196 feature ranks, score=-0.016608\n",
      "Finished fold with 182 / 196 feature ranks, score=-0.016716\n",
      "Finished fold with 183 / 196 feature ranks, score=-0.016750\n",
      "Finished fold with 184 / 196 feature ranks, score=-0.016742\n",
      "Finished fold with 185 / 196 feature ranks, score=-0.016582\n",
      "Finished fold with 186 / 196 feature ranks, score=-0.016718\n",
      "Finished fold with 187 / 196 feature ranks, score=-0.016744\n",
      "Finished fold with 188 / 196 feature ranks, score=-0.016683\n",
      "Finished fold with 189 / 196 feature ranks, score=-0.016683\n",
      "Finished fold with 190 / 196 feature ranks, score=-0.016683\n",
      "Finished fold with 191 / 196 feature ranks, score=-0.016683\n",
      "Finished fold with 192 / 196 feature ranks, score=-0.016683\n",
      "Finished fold with 193 / 196 feature ranks, score=-0.016683\n",
      "Finished fold with 194 / 196 feature ranks, score=-0.016683\n",
      "Finished fold with 195 / 196 feature ranks, score=-0.016683\n",
      "Finished fold with 196 / 196 feature ranks, score=-0.016683\n",
      "Finished fold with 1 / 196 feature ranks, score=-0.048885\n",
      "Finished fold with 2 / 196 feature ranks, score=-0.024439\n",
      "Finished fold with 3 / 196 feature ranks, score=-0.013539\n",
      "Finished fold with 4 / 196 feature ranks, score=-0.010705\n",
      "Finished fold with 5 / 196 feature ranks, score=-0.011294\n",
      "Finished fold with 6 / 196 feature ranks, score=-0.016980\n",
      "Finished fold with 7 / 196 feature ranks, score=-0.014917\n",
      "Finished fold with 8 / 196 feature ranks, score=-0.019394\n",
      "Finished fold with 9 / 196 feature ranks, score=-0.016548\n",
      "Finished fold with 10 / 196 feature ranks, score=-0.016306\n",
      "Finished fold with 11 / 196 feature ranks, score=-0.009673\n",
      "Finished fold with 12 / 196 feature ranks, score=-0.009597\n",
      "Finished fold with 13 / 196 feature ranks, score=-0.008900\n",
      "Finished fold with 14 / 196 feature ranks, score=-0.009326\n",
      "Finished fold with 15 / 196 feature ranks, score=-0.010253\n",
      "Finished fold with 16 / 196 feature ranks, score=-0.010295\n",
      "Finished fold with 17 / 196 feature ranks, score=-0.010005\n",
      "Finished fold with 18 / 196 feature ranks, score=-0.010003\n",
      "Finished fold with 19 / 196 feature ranks, score=-0.009986\n",
      "Finished fold with 20 / 196 feature ranks, score=-0.010079\n",
      "Finished fold with 21 / 196 feature ranks, score=-0.010714\n",
      "Finished fold with 22 / 196 feature ranks, score=-0.010685\n",
      "Finished fold with 23 / 196 feature ranks, score=-0.010606\n",
      "Finished fold with 24 / 196 feature ranks, score=-0.011685\n",
      "Finished fold with 25 / 196 feature ranks, score=-0.011780\n",
      "Finished fold with 26 / 196 feature ranks, score=-0.011502\n",
      "Finished fold with 27 / 196 feature ranks, score=-0.011987\n",
      "Finished fold with 28 / 196 feature ranks, score=-0.011957\n",
      "Finished fold with 29 / 196 feature ranks, score=-0.011938\n",
      "Finished fold with 30 / 196 feature ranks, score=-0.011996\n",
      "Finished fold with 31 / 196 feature ranks, score=-0.012009\n",
      "Finished fold with 32 / 196 feature ranks, score=-0.011974\n",
      "Finished fold with 33 / 196 feature ranks, score=-0.012505\n",
      "Finished fold with 34 / 196 feature ranks, score=-0.012277\n",
      "Finished fold with 35 / 196 feature ranks, score=-0.012853\n",
      "Finished fold with 36 / 196 feature ranks, score=-0.012870\n",
      "Finished fold with 37 / 196 feature ranks, score=-0.013046\n",
      "Finished fold with 38 / 196 feature ranks, score=-0.013158\n",
      "Finished fold with 39 / 196 feature ranks, score=-0.013106\n",
      "Finished fold with 40 / 196 feature ranks, score=-0.013341\n",
      "Finished fold with 41 / 196 feature ranks, score=-0.013510\n",
      "Finished fold with 42 / 196 feature ranks, score=-0.013717\n",
      "Finished fold with 43 / 196 feature ranks, score=-0.013601\n",
      "Finished fold with 44 / 196 feature ranks, score=-0.013600\n",
      "Finished fold with 45 / 196 feature ranks, score=-0.013610\n",
      "Finished fold with 46 / 196 feature ranks, score=-0.013625\n",
      "Finished fold with 47 / 196 feature ranks, score=-0.013620\n",
      "Finished fold with 48 / 196 feature ranks, score=-0.013912\n",
      "Finished fold with 49 / 196 feature ranks, score=-0.013930\n",
      "Finished fold with 50 / 196 feature ranks, score=-0.014025\n",
      "Finished fold with 51 / 196 feature ranks, score=-0.013965\n",
      "Finished fold with 52 / 196 feature ranks, score=-0.013941\n",
      "Finished fold with 53 / 196 feature ranks, score=-0.013908\n",
      "Finished fold with 54 / 196 feature ranks, score=-0.013730\n",
      "Finished fold with 55 / 196 feature ranks, score=-0.013708\n",
      "Finished fold with 56 / 196 feature ranks, score=-0.013627\n",
      "Finished fold with 57 / 196 feature ranks, score=-0.013655\n",
      "Finished fold with 58 / 196 feature ranks, score=-0.013648\n",
      "Finished fold with 59 / 196 feature ranks, score=-0.013622\n",
      "Finished fold with 60 / 196 feature ranks, score=-0.013522\n",
      "Finished fold with 61 / 196 feature ranks, score=-0.013621\n",
      "Finished fold with 62 / 196 feature ranks, score=-0.013610\n",
      "Finished fold with 63 / 196 feature ranks, score=-0.013653\n",
      "Finished fold with 64 / 196 feature ranks, score=-0.013635\n",
      "Finished fold with 65 / 196 feature ranks, score=-0.013773\n",
      "Finished fold with 66 / 196 feature ranks, score=-0.013768\n",
      "Finished fold with 67 / 196 feature ranks, score=-0.013790\n",
      "Finished fold with 68 / 196 feature ranks, score=-0.013777\n",
      "Finished fold with 69 / 196 feature ranks, score=-0.013743\n",
      "Finished fold with 70 / 196 feature ranks, score=-0.013739\n",
      "Finished fold with 71 / 196 feature ranks, score=-0.013798\n",
      "Finished fold with 72 / 196 feature ranks, score=-0.013728\n",
      "Finished fold with 73 / 196 feature ranks, score=-0.013713\n",
      "Finished fold with 74 / 196 feature ranks, score=-0.013748\n",
      "Finished fold with 75 / 196 feature ranks, score=-0.013711\n",
      "Finished fold with 76 / 196 feature ranks, score=-0.013727\n",
      "Finished fold with 77 / 196 feature ranks, score=-0.013726\n",
      "Finished fold with 78 / 196 feature ranks, score=-0.013776\n",
      "Finished fold with 79 / 196 feature ranks, score=-0.013739\n",
      "Finished fold with 80 / 196 feature ranks, score=-0.013741\n",
      "Finished fold with 81 / 196 feature ranks, score=-0.013748\n",
      "Finished fold with 82 / 196 feature ranks, score=-0.013808\n",
      "Finished fold with 83 / 196 feature ranks, score=-0.013767\n",
      "Finished fold with 84 / 196 feature ranks, score=-0.013794\n",
      "Finished fold with 85 / 196 feature ranks, score=-0.013795\n",
      "Finished fold with 86 / 196 feature ranks, score=-0.013799\n",
      "Finished fold with 87 / 196 feature ranks, score=-0.013783\n",
      "Finished fold with 88 / 196 feature ranks, score=-0.013785\n",
      "Finished fold with 89 / 196 feature ranks, score=-0.013755\n",
      "Finished fold with 90 / 196 feature ranks, score=-0.013900\n",
      "Finished fold with 91 / 196 feature ranks, score=-0.013841\n",
      "Finished fold with 92 / 196 feature ranks, score=-0.014012\n",
      "Finished fold with 93 / 196 feature ranks, score=-0.014026\n",
      "Finished fold with 94 / 196 feature ranks, score=-0.013990\n",
      "Finished fold with 95 / 196 feature ranks, score=-0.013951\n",
      "Finished fold with 96 / 196 feature ranks, score=-0.014103\n",
      "Finished fold with 97 / 196 feature ranks, score=-0.014105\n",
      "Finished fold with 98 / 196 feature ranks, score=-0.014186\n",
      "Finished fold with 99 / 196 feature ranks, score=-0.014174\n",
      "Finished fold with 100 / 196 feature ranks, score=-0.014085\n",
      "Finished fold with 101 / 196 feature ranks, score=-0.014110\n",
      "Finished fold with 102 / 196 feature ranks, score=-0.014050\n",
      "Finished fold with 103 / 196 feature ranks, score=-0.014078\n",
      "Finished fold with 104 / 196 feature ranks, score=-0.014091\n",
      "Finished fold with 105 / 196 feature ranks, score=-0.014112\n",
      "Finished fold with 106 / 196 feature ranks, score=-0.014095\n",
      "Finished fold with 107 / 196 feature ranks, score=-0.014122\n",
      "Finished fold with 108 / 196 feature ranks, score=-0.014149\n",
      "Finished fold with 109 / 196 feature ranks, score=-0.014048\n",
      "Finished fold with 110 / 196 feature ranks, score=-0.014176\n",
      "Finished fold with 111 / 196 feature ranks, score=-0.014117\n",
      "Finished fold with 112 / 196 feature ranks, score=-0.014141\n",
      "Finished fold with 113 / 196 feature ranks, score=-0.014041\n",
      "Finished fold with 114 / 196 feature ranks, score=-0.014214\n",
      "Finished fold with 115 / 196 feature ranks, score=-0.014228\n",
      "Finished fold with 116 / 196 feature ranks, score=-0.014162\n",
      "Finished fold with 117 / 196 feature ranks, score=-0.014242\n",
      "Finished fold with 118 / 196 feature ranks, score=-0.014239\n",
      "Finished fold with 119 / 196 feature ranks, score=-0.014168\n",
      "Finished fold with 120 / 196 feature ranks, score=-0.014185\n",
      "Finished fold with 121 / 196 feature ranks, score=-0.014270\n",
      "Finished fold with 122 / 196 feature ranks, score=-0.014215\n",
      "Finished fold with 123 / 196 feature ranks, score=-0.014238\n",
      "Finished fold with 124 / 196 feature ranks, score=-0.014250\n",
      "Finished fold with 125 / 196 feature ranks, score=-0.014211\n",
      "Finished fold with 126 / 196 feature ranks, score=-0.014198\n",
      "Finished fold with 127 / 196 feature ranks, score=-0.014282\n",
      "Finished fold with 128 / 196 feature ranks, score=-0.014194\n",
      "Finished fold with 129 / 196 feature ranks, score=-0.014328\n",
      "Finished fold with 130 / 196 feature ranks, score=-0.014156\n",
      "Finished fold with 131 / 196 feature ranks, score=-0.014316\n",
      "Finished fold with 132 / 196 feature ranks, score=-0.014253\n",
      "Finished fold with 133 / 196 feature ranks, score=-0.014304\n",
      "Finished fold with 134 / 196 feature ranks, score=-0.014192\n",
      "Finished fold with 135 / 196 feature ranks, score=-0.014327\n",
      "Finished fold with 136 / 196 feature ranks, score=-0.014245\n",
      "Finished fold with 137 / 196 feature ranks, score=-0.014325\n",
      "Finished fold with 138 / 196 feature ranks, score=-0.014273\n",
      "Finished fold with 139 / 196 feature ranks, score=-0.014282\n",
      "Finished fold with 140 / 196 feature ranks, score=-0.014315\n",
      "Finished fold with 141 / 196 feature ranks, score=-0.014173\n",
      "Finished fold with 142 / 196 feature ranks, score=-0.014257\n",
      "Finished fold with 143 / 196 feature ranks, score=-0.014262\n",
      "Finished fold with 144 / 196 feature ranks, score=-0.014287\n",
      "Finished fold with 145 / 196 feature ranks, score=-0.014249\n",
      "Finished fold with 146 / 196 feature ranks, score=-0.014311\n",
      "Finished fold with 147 / 196 feature ranks, score=-0.014257\n",
      "Finished fold with 148 / 196 feature ranks, score=-0.014278\n",
      "Finished fold with 149 / 196 feature ranks, score=-0.014187\n",
      "Finished fold with 150 / 196 feature ranks, score=-0.014306\n",
      "Finished fold with 151 / 196 feature ranks, score=-0.014280\n",
      "Finished fold with 152 / 196 feature ranks, score=-0.014257\n",
      "Finished fold with 153 / 196 feature ranks, score=-0.014289\n",
      "Finished fold with 154 / 196 feature ranks, score=-0.014278\n",
      "Finished fold with 155 / 196 feature ranks, score=-0.014324\n",
      "Finished fold with 156 / 196 feature ranks, score=-0.014183\n",
      "Finished fold with 157 / 196 feature ranks, score=-0.014249\n",
      "Finished fold with 158 / 196 feature ranks, score=-0.014275\n",
      "Finished fold with 159 / 196 feature ranks, score=-0.014224\n",
      "Finished fold with 160 / 196 feature ranks, score=-0.014150\n",
      "Finished fold with 161 / 196 feature ranks, score=-0.014214\n",
      "Finished fold with 162 / 196 feature ranks, score=-0.014294\n",
      "Finished fold with 163 / 196 feature ranks, score=-0.014324\n",
      "Finished fold with 164 / 196 feature ranks, score=-0.014277\n",
      "Finished fold with 165 / 196 feature ranks, score=-0.014313\n",
      "Finished fold with 166 / 196 feature ranks, score=-0.014322\n",
      "Finished fold with 167 / 196 feature ranks, score=-0.014201\n",
      "Finished fold with 168 / 196 feature ranks, score=-0.014287\n",
      "Finished fold with 169 / 196 feature ranks, score=-0.014213\n",
      "Finished fold with 170 / 196 feature ranks, score=-0.014308\n",
      "Finished fold with 171 / 196 feature ranks, score=-0.014289\n",
      "Finished fold with 172 / 196 feature ranks, score=-0.014280\n",
      "Finished fold with 173 / 196 feature ranks, score=-0.014293\n",
      "Finished fold with 174 / 196 feature ranks, score=-0.014274\n",
      "Finished fold with 175 / 196 feature ranks, score=-0.014317\n",
      "Finished fold with 176 / 196 feature ranks, score=-0.014289\n",
      "Finished fold with 177 / 196 feature ranks, score=-0.014266\n",
      "Finished fold with 178 / 196 feature ranks, score=-0.014276\n",
      "Finished fold with 179 / 196 feature ranks, score=-0.014283\n",
      "Finished fold with 180 / 196 feature ranks, score=-0.014261\n",
      "Finished fold with 181 / 196 feature ranks, score=-0.014256\n",
      "Finished fold with 182 / 196 feature ranks, score=-0.014414\n",
      "Finished fold with 183 / 196 feature ranks, score=-0.014297\n",
      "Finished fold with 184 / 196 feature ranks, score=-0.014269\n",
      "Finished fold with 185 / 196 feature ranks, score=-0.014186\n",
      "Finished fold with 186 / 196 feature ranks, score=-0.014208\n",
      "Finished fold with 187 / 196 feature ranks, score=-0.014337\n",
      "Finished fold with 188 / 196 feature ranks, score=-0.014388\n",
      "Finished fold with 189 / 196 feature ranks, score=-0.014388\n",
      "Finished fold with 190 / 196 feature ranks, score=-0.014388\n",
      "Finished fold with 191 / 196 feature ranks, score=-0.014388\n",
      "Finished fold with 192 / 196 feature ranks, score=-0.014388\n",
      "Finished fold with 193 / 196 feature ranks, score=-0.014388\n",
      "Finished fold with 194 / 196 feature ranks, score=-0.014388\n",
      "Finished fold with 195 / 196 feature ranks, score=-0.014388\n",
      "Finished fold with 196 / 196 feature ranks, score=-0.014388\n",
      "Finished fold with 1 / 196 feature ranks, score=-0.048826\n",
      "Finished fold with 2 / 196 feature ranks, score=-0.014298\n",
      "Finished fold with 3 / 196 feature ranks, score=-0.010264\n",
      "Finished fold with 4 / 196 feature ranks, score=-0.010406\n",
      "Finished fold with 5 / 196 feature ranks, score=-0.019582\n",
      "Finished fold with 6 / 196 feature ranks, score=-0.002936\n",
      "Finished fold with 7 / 196 feature ranks, score=-0.002812\n",
      "Finished fold with 8 / 196 feature ranks, score=-0.018948\n",
      "Finished fold with 9 / 196 feature ranks, score=-0.019269\n",
      "Finished fold with 10 / 196 feature ranks, score=-0.014589\n",
      "Finished fold with 11 / 196 feature ranks, score=-0.025165\n",
      "Finished fold with 12 / 196 feature ranks, score=-0.017228\n",
      "Finished fold with 13 / 196 feature ranks, score=-0.015540\n",
      "Finished fold with 14 / 196 feature ranks, score=-0.016008\n",
      "Finished fold with 15 / 196 feature ranks, score=-0.015797\n",
      "Finished fold with 16 / 196 feature ranks, score=-0.017282\n",
      "Finished fold with 17 / 196 feature ranks, score=-0.017752\n",
      "Finished fold with 18 / 196 feature ranks, score=-0.015338\n",
      "Finished fold with 19 / 196 feature ranks, score=-0.013593\n",
      "Finished fold with 20 / 196 feature ranks, score=-0.013610\n",
      "Finished fold with 21 / 196 feature ranks, score=-0.012836\n",
      "Finished fold with 22 / 196 feature ranks, score=-0.012304\n",
      "Finished fold with 23 / 196 feature ranks, score=-0.012727\n",
      "Finished fold with 24 / 196 feature ranks, score=-0.011842\n",
      "Finished fold with 25 / 196 feature ranks, score=-0.011653\n",
      "Finished fold with 26 / 196 feature ranks, score=-0.012039\n",
      "Finished fold with 27 / 196 feature ranks, score=-0.012454\n",
      "Finished fold with 28 / 196 feature ranks, score=-0.012421\n",
      "Finished fold with 29 / 196 feature ranks, score=-0.012171\n",
      "Finished fold with 30 / 196 feature ranks, score=-0.012108\n",
      "Finished fold with 31 / 196 feature ranks, score=-0.012077\n",
      "Finished fold with 32 / 196 feature ranks, score=-0.012067\n",
      "Finished fold with 33 / 196 feature ranks, score=-0.012380\n",
      "Finished fold with 34 / 196 feature ranks, score=-0.012723\n",
      "Finished fold with 35 / 196 feature ranks, score=-0.012743\n",
      "Finished fold with 36 / 196 feature ranks, score=-0.013545\n",
      "Finished fold with 37 / 196 feature ranks, score=-0.012720\n",
      "Finished fold with 38 / 196 feature ranks, score=-0.012782\n",
      "Finished fold with 39 / 196 feature ranks, score=-0.012792\n",
      "Finished fold with 40 / 196 feature ranks, score=-0.012819\n",
      "Finished fold with 41 / 196 feature ranks, score=-0.013046\n",
      "Finished fold with 42 / 196 feature ranks, score=-0.013029\n",
      "Finished fold with 43 / 196 feature ranks, score=-0.012997\n",
      "Finished fold with 44 / 196 feature ranks, score=-0.013039\n",
      "Finished fold with 45 / 196 feature ranks, score=-0.013045\n",
      "Finished fold with 46 / 196 feature ranks, score=-0.012926\n",
      "Finished fold with 47 / 196 feature ranks, score=-0.012904\n",
      "Finished fold with 48 / 196 feature ranks, score=-0.012851\n",
      "Finished fold with 49 / 196 feature ranks, score=-0.012871\n",
      "Finished fold with 50 / 196 feature ranks, score=-0.012845\n",
      "Finished fold with 51 / 196 feature ranks, score=-0.012847\n",
      "Finished fold with 52 / 196 feature ranks, score=-0.012960\n",
      "Finished fold with 53 / 196 feature ranks, score=-0.012966\n",
      "Finished fold with 54 / 196 feature ranks, score=-0.012961\n",
      "Finished fold with 55 / 196 feature ranks, score=-0.013005\n",
      "Finished fold with 56 / 196 feature ranks, score=-0.012981\n",
      "Finished fold with 57 / 196 feature ranks, score=-0.012974\n",
      "Finished fold with 58 / 196 feature ranks, score=-0.012697\n",
      "Finished fold with 59 / 196 feature ranks, score=-0.012707\n",
      "Finished fold with 60 / 196 feature ranks, score=-0.012602\n",
      "Finished fold with 61 / 196 feature ranks, score=-0.012612\n",
      "Finished fold with 62 / 196 feature ranks, score=-0.012608\n",
      "Finished fold with 63 / 196 feature ranks, score=-0.012598\n",
      "Finished fold with 64 / 196 feature ranks, score=-0.012567\n",
      "Finished fold with 65 / 196 feature ranks, score=-0.012506\n",
      "Finished fold with 66 / 196 feature ranks, score=-0.012521\n",
      "Finished fold with 67 / 196 feature ranks, score=-0.012510\n",
      "Finished fold with 68 / 196 feature ranks, score=-0.012448\n",
      "Finished fold with 69 / 196 feature ranks, score=-0.012511\n",
      "Finished fold with 70 / 196 feature ranks, score=-0.012564\n",
      "Finished fold with 71 / 196 feature ranks, score=-0.012495\n",
      "Finished fold with 72 / 196 feature ranks, score=-0.012621\n",
      "Finished fold with 73 / 196 feature ranks, score=-0.012525\n",
      "Finished fold with 74 / 196 feature ranks, score=-0.012518\n",
      "Finished fold with 75 / 196 feature ranks, score=-0.012558\n",
      "Finished fold with 76 / 196 feature ranks, score=-0.012528\n",
      "Finished fold with 77 / 196 feature ranks, score=-0.012702\n",
      "Finished fold with 78 / 196 feature ranks, score=-0.012691\n",
      "Finished fold with 79 / 196 feature ranks, score=-0.012806\n",
      "Finished fold with 80 / 196 feature ranks, score=-0.012820\n",
      "Finished fold with 81 / 196 feature ranks, score=-0.012897\n",
      "Finished fold with 82 / 196 feature ranks, score=-0.012857\n",
      "Finished fold with 83 / 196 feature ranks, score=-0.012940\n",
      "Finished fold with 84 / 196 feature ranks, score=-0.012954\n",
      "Finished fold with 85 / 196 feature ranks, score=-0.012970\n",
      "Finished fold with 86 / 196 feature ranks, score=-0.013092\n",
      "Finished fold with 87 / 196 feature ranks, score=-0.013090\n",
      "Finished fold with 88 / 196 feature ranks, score=-0.013060\n",
      "Finished fold with 89 / 196 feature ranks, score=-0.013054\n",
      "Finished fold with 90 / 196 feature ranks, score=-0.013071\n",
      "Finished fold with 91 / 196 feature ranks, score=-0.013016\n",
      "Finished fold with 92 / 196 feature ranks, score=-0.013045\n",
      "Finished fold with 93 / 196 feature ranks, score=-0.013070\n",
      "Finished fold with 94 / 196 feature ranks, score=-0.013072\n",
      "Finished fold with 95 / 196 feature ranks, score=-0.013064\n",
      "Finished fold with 96 / 196 feature ranks, score=-0.013091\n",
      "Finished fold with 97 / 196 feature ranks, score=-0.012966\n",
      "Finished fold with 98 / 196 feature ranks, score=-0.013047\n",
      "Finished fold with 99 / 196 feature ranks, score=-0.013062\n",
      "Finished fold with 100 / 196 feature ranks, score=-0.013200\n",
      "Finished fold with 101 / 196 feature ranks, score=-0.013110\n",
      "Finished fold with 102 / 196 feature ranks, score=-0.013169\n",
      "Finished fold with 103 / 196 feature ranks, score=-0.013105\n",
      "Finished fold with 104 / 196 feature ranks, score=-0.013121\n",
      "Finished fold with 105 / 196 feature ranks, score=-0.013130\n",
      "Finished fold with 106 / 196 feature ranks, score=-0.013136\n",
      "Finished fold with 107 / 196 feature ranks, score=-0.013125\n",
      "Finished fold with 108 / 196 feature ranks, score=-0.013065\n",
      "Finished fold with 109 / 196 feature ranks, score=-0.013064\n",
      "Finished fold with 110 / 196 feature ranks, score=-0.013009\n",
      "Finished fold with 111 / 196 feature ranks, score=-0.013182\n",
      "Finished fold with 112 / 196 feature ranks, score=-0.013221\n",
      "Finished fold with 113 / 196 feature ranks, score=-0.013188\n",
      "Finished fold with 114 / 196 feature ranks, score=-0.013130\n",
      "Finished fold with 115 / 196 feature ranks, score=-0.013201\n",
      "Finished fold with 116 / 196 feature ranks, score=-0.013214\n",
      "Finished fold with 117 / 196 feature ranks, score=-0.013232\n",
      "Finished fold with 118 / 196 feature ranks, score=-0.013196\n",
      "Finished fold with 119 / 196 feature ranks, score=-0.013193\n",
      "Finished fold with 120 / 196 feature ranks, score=-0.013275\n",
      "Finished fold with 121 / 196 feature ranks, score=-0.013243\n",
      "Finished fold with 122 / 196 feature ranks, score=-0.013299\n",
      "Finished fold with 123 / 196 feature ranks, score=-0.013255\n",
      "Finished fold with 124 / 196 feature ranks, score=-0.013150\n",
      "Finished fold with 125 / 196 feature ranks, score=-0.013239\n",
      "Finished fold with 126 / 196 feature ranks, score=-0.013228\n",
      "Finished fold with 127 / 196 feature ranks, score=-0.013207\n",
      "Finished fold with 128 / 196 feature ranks, score=-0.013231\n",
      "Finished fold with 129 / 196 feature ranks, score=-0.013199\n",
      "Finished fold with 130 / 196 feature ranks, score=-0.013190\n",
      "Finished fold with 131 / 196 feature ranks, score=-0.013216\n",
      "Finished fold with 132 / 196 feature ranks, score=-0.013218\n",
      "Finished fold with 133 / 196 feature ranks, score=-0.013179\n",
      "Finished fold with 134 / 196 feature ranks, score=-0.013053\n",
      "Finished fold with 135 / 196 feature ranks, score=-0.012979\n",
      "Finished fold with 136 / 196 feature ranks, score=-0.013051\n",
      "Finished fold with 137 / 196 feature ranks, score=-0.013023\n",
      "Finished fold with 138 / 196 feature ranks, score=-0.012985\n",
      "Finished fold with 139 / 196 feature ranks, score=-0.012911\n",
      "Finished fold with 140 / 196 feature ranks, score=-0.012963\n",
      "Finished fold with 141 / 196 feature ranks, score=-0.012996\n",
      "Finished fold with 142 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 143 / 196 feature ranks, score=-0.012910\n",
      "Finished fold with 144 / 196 feature ranks, score=-0.012939\n",
      "Finished fold with 145 / 196 feature ranks, score=-0.012902\n",
      "Finished fold with 146 / 196 feature ranks, score=-0.012928\n",
      "Finished fold with 147 / 196 feature ranks, score=-0.013016\n",
      "Finished fold with 148 / 196 feature ranks, score=-0.012941\n",
      "Finished fold with 149 / 196 feature ranks, score=-0.012934\n",
      "Finished fold with 150 / 196 feature ranks, score=-0.013000\n",
      "Finished fold with 151 / 196 feature ranks, score=-0.012995\n",
      "Finished fold with 152 / 196 feature ranks, score=-0.012934\n",
      "Finished fold with 153 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 154 / 196 feature ranks, score=-0.012934\n",
      "Finished fold with 155 / 196 feature ranks, score=-0.012937\n",
      "Finished fold with 156 / 196 feature ranks, score=-0.012873\n",
      "Finished fold with 157 / 196 feature ranks, score=-0.012958\n",
      "Finished fold with 158 / 196 feature ranks, score=-0.012964\n",
      "Finished fold with 159 / 196 feature ranks, score=-0.012912\n",
      "Finished fold with 160 / 196 feature ranks, score=-0.012940\n",
      "Finished fold with 161 / 196 feature ranks, score=-0.012899\n",
      "Finished fold with 162 / 196 feature ranks, score=-0.012996\n",
      "Finished fold with 163 / 196 feature ranks, score=-0.012973\n",
      "Finished fold with 164 / 196 feature ranks, score=-0.012865\n",
      "Finished fold with 165 / 196 feature ranks, score=-0.012959\n",
      "Finished fold with 166 / 196 feature ranks, score=-0.012972\n",
      "Finished fold with 167 / 196 feature ranks, score=-0.012951\n",
      "Finished fold with 168 / 196 feature ranks, score=-0.012963\n",
      "Finished fold with 169 / 196 feature ranks, score=-0.012912\n",
      "Finished fold with 170 / 196 feature ranks, score=-0.012899\n",
      "Finished fold with 171 / 196 feature ranks, score=-0.012926\n",
      "Finished fold with 172 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 173 / 196 feature ranks, score=-0.012948\n",
      "Finished fold with 174 / 196 feature ranks, score=-0.012972\n",
      "Finished fold with 175 / 196 feature ranks, score=-0.012950\n",
      "Finished fold with 176 / 196 feature ranks, score=-0.012925\n",
      "Finished fold with 177 / 196 feature ranks, score=-0.012892\n",
      "Finished fold with 178 / 196 feature ranks, score=-0.012961\n",
      "Finished fold with 179 / 196 feature ranks, score=-0.012904\n",
      "Finished fold with 180 / 196 feature ranks, score=-0.013022\n",
      "Finished fold with 181 / 196 feature ranks, score=-0.012974\n",
      "Finished fold with 182 / 196 feature ranks, score=-0.012985\n",
      "Finished fold with 183 / 196 feature ranks, score=-0.012938\n",
      "Finished fold with 184 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 185 / 196 feature ranks, score=-0.012942\n",
      "Finished fold with 186 / 196 feature ranks, score=-0.012983\n",
      "Finished fold with 187 / 196 feature ranks, score=-0.012901\n",
      "Finished fold with 188 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 189 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 190 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 191 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 192 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 193 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 194 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 195 / 196 feature ranks, score=-0.012947\n",
      "Finished fold with 196 / 196 feature ranks, score=-0.012947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFECV(cv=None,\n",
       "   estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
       "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       "   estimator_params=None,\n",
       "   scoring=make_scorer(mape, greater_is_better=False), step=1, verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv = RFECV(SVR(kernel='linear'), scoring=mape_scorer, verbose=1)\n",
    "rfecv.fit(all_data[training_idx,1:], targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features : 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAFhCAYAAAD+2OlvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhhjTK+/AOdd4xMEUbNvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa0ad99a3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04820252, -0.02079843, -0.01380174, -0.00927224, -0.0145694 ,\n",
       "       -0.01199945, -0.01166627, -0.01878477, -0.01734849, -0.0195538 ,\n",
       "       -0.02092372, -0.01777325, -0.01689787, -0.01724139, -0.01760842,\n",
       "       -0.01809834, -0.01585822, -0.01514929, -0.01304456, -0.01118938,\n",
       "       -0.01176045, -0.01106637, -0.01134785, -0.011435  , -0.01140495,\n",
       "       -0.01207491, -0.01304405, -0.01327839, -0.0128857 , -0.01319245,\n",
       "       -0.01322818, -0.01314254, -0.01376516, -0.01383943, -0.01391283,\n",
       "       -0.01398327, -0.01380692, -0.01384078, -0.01384422, -0.01391986,\n",
       "       -0.01398264, -0.01404736, -0.01404069, -0.01415581, -0.01419274,\n",
       "       -0.01411536, -0.01434265, -0.0144108 , -0.01441102, -0.01437007,\n",
       "       -0.01444125, -0.01450191, -0.01451479, -0.01444968, -0.01451545,\n",
       "       -0.01452402, -0.01450979, -0.01429599, -0.0142498 , -0.01419244,\n",
       "       -0.01446256, -0.01450367, -0.01438751, -0.01448494, -0.01456733,\n",
       "       -0.0143895 , -0.01449669, -0.01440952, -0.0144796 , -0.01445984,\n",
       "       -0.01442339, -0.01437697, -0.01424619, -0.01432923, -0.01434817,\n",
       "       -0.0143077 , -0.01443539, -0.01447434, -0.01443164, -0.01435675,\n",
       "       -0.01439797, -0.01447781, -0.01452691, -0.01456635, -0.01453783,\n",
       "       -0.01456822, -0.01455773, -0.01455328, -0.01453233, -0.01462644,\n",
       "       -0.01458413, -0.01466503, -0.01455676, -0.0145601 , -0.014633  ,\n",
       "       -0.01468088, -0.01465386, -0.01467856, -0.0146745 , -0.01471259,\n",
       "       -0.01467004, -0.01468612, -0.014661  , -0.01466461, -0.01467887,\n",
       "       -0.01466741, -0.01465496, -0.0146523 , -0.0146234 , -0.01465331,\n",
       "       -0.01465765, -0.01473013, -0.01462942, -0.01468755, -0.01471464,\n",
       "       -0.01469042, -0.01476513, -0.01470173, -0.01464789, -0.0146909 ,\n",
       "       -0.01466671, -0.01469511, -0.01474827, -0.01467014, -0.01469699,\n",
       "       -0.01469607, -0.01469748, -0.0146857 , -0.01470457, -0.01468199,\n",
       "       -0.01474372, -0.01467588, -0.01470548, -0.01461519, -0.01466457,\n",
       "       -0.01464977, -0.01463956, -0.01467409, -0.01462228, -0.01465315,\n",
       "       -0.01461081, -0.01464084, -0.01462306, -0.01465621, -0.01462753,\n",
       "       -0.01465391, -0.01467575, -0.01463066, -0.01461256, -0.01466344,\n",
       "       -0.01466139, -0.01464536, -0.01465297, -0.01463386, -0.01466816,\n",
       "       -0.01455311, -0.01463779, -0.01464851, -0.01462204, -0.01458971,\n",
       "       -0.01464646, -0.01466975, -0.01465438, -0.01465034, -0.01464614,\n",
       "       -0.0146365 , -0.01459907, -0.01463428, -0.01459201, -0.01461165,\n",
       "       -0.01460428, -0.01463576, -0.01466233, -0.01463564, -0.01467689,\n",
       "       -0.01461265, -0.01461162, -0.01463968, -0.0146054 , -0.01468602,\n",
       "       -0.01461279, -0.014705  , -0.01466145, -0.01465268, -0.01456989,\n",
       "       -0.01463653, -0.01466084, -0.01467272, -0.01467272, -0.01467272,\n",
       "       -0.01467272, -0.01467272, -0.01467272, -0.01467272, -0.01467272,\n",
       "       -0.01467272])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 25 183  26 190  24  30 187  80  20  34  35 113  39 186 116  49 192 121\n",
      "  48  43  53  82 184 188 104  19 189 103  36  54  38  79 185 191  81  77\n",
      "   1 193  23   1  29   2  33   1  32  28   4  18   3  16  17  21   6  22\n",
      "  14   5   1   8   9   7  11  10  59 157 132 161 152 118 140 153 147  68\n",
      "  94  42  90  58  63 178 163 136 133  71 102 177 165 146 111 180  88 137\n",
      " 144  69  86 101  78  12  72 131 162 170 176 168 125 148 129 135  52  89\n",
      "  76 175 150 100 105 151 164 107 179  55 117 149  47  51 156  92 167 138\n",
      "  65 181 139  40  95 155 114  87 173 182  98  83 158  57 123  27  46  15\n",
      "  31 160  91 169 130 109 166  67  62 127  44  64  84  37 119  75 141 128\n",
      " 143  97  66  96 126  13  45  41  70  73 174 122  60  56 134 115 154  85\n",
      " 106 112  50 110  93  74 120 124 145 171 142 108  61 172  99 159]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False  True False False False  True False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]\n"
     ]
    }
   ],
   "source": [
    "print rfecv.ranking_\n",
    "print rfecv.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Initial Bagging Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(EST_PICKLE_FILENAME1):\n",
    "  bagging_est = pickle.load(open(EST_PICKLE_FILENAME1, 'r'))\n",
    "else:\n",
    "  steps = [\n",
    "    ('impute', Imputer()),\n",
    "    # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "    # should start at index 0.\n",
    "    ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                             n_values=[7, 10, 10, 10])),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('select_features', RFECV(SVR(kernel='linear'), scoring=mape_scorer, verbose=1))\n",
    "    ('estimate', GradientBoostingRegressor(n_estimators=190, learning_rate=0.5))\n",
    "\n",
    "  ]\n",
    "\n",
    "  est = Pipeline(steps)\n",
    "\n",
    "  steps_bagging = [\n",
    "    ('impute', Imputer()),\n",
    "    # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "    # should start at index 0.\n",
    "    ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                             n_values=[7, 10, 10, 10])),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('estimate', BaggingRegressor(\n",
    "        GradientBoostingRegressor(\n",
    "          n_estimators=n_features, learning_rate=0.5,\n",
    "          verbose=1\n",
    "        )))\n",
    "\n",
    "  ]\n",
    "\n",
    "  bagging_est = Pipeline(steps_bagging)\n",
    "  bagging_est.fit(data_train, targets_train)\n",
    "  pickle.dump(bagging_est, open(EST_PICKLE_FILENAME1, \"w\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predictions = bagging_est.predict(data_test)\n",
    "print(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps_bagging = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('estimate', BaggingRegressor(\n",
    "      GradientBoostingRegressor(\n",
    "        n_estimators=n_features, learning_rate=0.5,\n",
    "        verbose=1\n",
    "      )))\n",
    "\n",
    "]\n",
    "\n",
    "params = {\n",
    "  'estimate__learning_rate': [0.1, 0.5, 1, 10],\n",
    "  'estimate__n_estimators': [i for i in range(110, n_features, 20)],\n",
    "#   'estimate__loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "search_params = RandomizedSearchCV(\n",
    "  estimator=est,\n",
    "  param_distributions=params,\n",
    "  cv=5,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=2,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "search_params.fit(data_train_original, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
