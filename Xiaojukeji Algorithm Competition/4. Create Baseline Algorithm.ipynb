{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME = 'baseline_final_estimator.pkl'\n",
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\tbusy_time\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.sum(np.absolute((y-predictions)/y)) / (y.shape[0]*predictions.shape[0])\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.583333333333\n",
      "1018.41666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "predictions = np.array([1.0, 2.0, 3.0, 4.0]).astype('float32')\n",
    "y = np.array([1.0, 2.0, 3.0, 4.0]).astype('float32')\n",
    "\n",
    "# Should return 0.0\n",
    "print mape(y, predictions)\n",
    "\n",
    "# Should return higher score\n",
    "predictions = np.array([1.0, 2.0, 2.0, 3.0]).astype('float32')\n",
    "print(mape(y, predictions))\n",
    "\n",
    "# Should return highest score\n",
    "predictions = np.array([1000.0, 22.0, 11.0, 31.0]).astype('float32')\n",
    "print(mape(y, predictions))\n",
    "\n",
    "# est = LogisticRegression()\n",
    "# X = np.random.rand(10,4)\n",
    "# y = X.sum(axis=1)\n",
    "# est.fit(X, y)\n",
    "# predictions = est.predict(X)\n",
    "# print(mape(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample, IF(timeofday_slot >= 50 AND timeofday_slot <= 53, 1, 0) AS busy_time\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0\n",
    "LIMIT 5000\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5000 rows\n",
      "processed 0 rows\n"
     ]
    }
   ],
   "source": [
    "query = bq.Query(q_all)\n",
    "tableresult = query.results()\n",
    "\n",
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([  25,   25,   32, ..., 4993, 4993, 4993]), array([24, 25,  6, ..., 21, 22, 23]))\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64))\n",
      "np.max= nan\n",
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([], dtype=int64), array([], dtype=int64))\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64))\n",
      "np.max= 461563.0\n"
     ]
    }
   ],
   "source": [
    "# This chunk does further wrangling to dataset to produce training and test sets.\n",
    "\n",
    "# Useful code to check NaN and Inf values. This is needed since these values would\n",
    "# cause \"Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "# errors when left unchecked.\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data))\n",
    "print \"np.max=\", np.max(abs(all_data))\n",
    "\n",
    "# Impute all NaN with numbers (not sure what to replace inf yet)\n",
    "all_data[np.isnan(all_data)] = 0\n",
    "# all_data[np.isinf(all_data)] = 0\n",
    "\n",
    "# See that NaN and Inf values replaced\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data))\n",
    "print \"np.max=\", np.max(abs(all_data))\n",
    "\n",
    "# Split the data into train and test sets.\n",
    "data_size = all_data.shape[0]\n",
    "training_size = data_size * 80/100\n",
    "indices = np.random.permutation(data_size)\n",
    "training_idx, test_idx = indices[:training_size], indices[training_size:]\n",
    "all_data_train, all_data_test = all_data[training_idx,:], all_data[test_idx,:]\n",
    "\n",
    "data_train = all_data_train[:,1:]\n",
    "targets_train = all_data_train[:,0]\n",
    "data_test = all_data_test[:,1:]\n",
    "targets_test = all_data_test[:,0]\n",
    "data_train_original = np.copy(data_train)\n",
    "data_test_original = np.copy(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# one_hot = OneHotEncoder(categorical_features=[0, 1, 14, 17, 20], n_values='auto')\n",
    "one_hot = OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False)\n",
    "one_hot.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 10, 10, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.n_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7, 17, 27, 37])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.feature_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new number of features: 192\n"
     ]
    }
   ],
   "source": [
    "data_train = one_hot.transform(data_train_original).todense()\n",
    "data_test = one_hot.transform(data_test_original).todense()\n",
    "n_features = data_train.shape[1]\n",
    "print 'new number of features: {}'.format(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Testing Algorithm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423.136508905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "class CustomRegressor(BaseEstimator, RegressorMixin):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, X, y):\n",
    "#     self.classes_, indices = np.unique([\"foo\", \"bar\", \"foo\"],\n",
    "#                                     return_inverse=True)\n",
    "#     self.majority_ = np.argmax(np.bincount(indices))\n",
    "    return self\n",
    "\n",
    "  def predict(self, X):\n",
    "    # 56: gap_1_slots_ago\n",
    "    # 58: gap_2_slots_ago\n",
    "    # 60: gap_3_slots_ago\n",
    "#     X = X.tocsr()\n",
    "#     v1 = coo_matrix(np.asmatrix(np.ones(X.shape[0])).T)\n",
    "    v1 = np.asmatrix(np.ones(X.shape[0]))\n",
    "    v2 = np.asmatrix((X[:, 23]*0.65+X[:, 25]*0.25+X[:, 27]*0.15)/2)\n",
    "    predictions = np.asarray(np.concatenate((v1, v2), axis=0).max(axis=0))\n",
    "    \n",
    "    return predictions\n",
    "  \n",
    "custom_est = CustomRegressor()\n",
    "custom_est.fit(data_train_original, data_test_original)\n",
    "custom_predictions = custom_est.predict(data_test_original)\n",
    "print(mape(targets_test, custom_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] estimate__max_features=110 ......................................\n",
      "[CV] estimate__max_features=110 ......................................\n",
      "[CV] estimate__max_features=110 ......................................\n",
      "[CV] estimate__max_features=110 ......................................\n",
      "[CV] estimate__max_features=110 ......................................\n",
      "[CV] estimate__max_features=111 ......................................\n",
      "[CV] estimate__max_features=111 ......................................\n",
      "[CV] estimate__max_features=111 ......................................\n",
      "[CV] estimate__max_features=111 ......................................\n"
     ]
    },
    {
     "ename": "JoblibTypeError",
     "evalue": "JoblibTypeError\n___________________________________________________________________________\nMultiprocessing exception:\n    ...........................................................................\n/usr/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    157     pkg_name = mod_name.rpartition('.')[0]\n    158     main_globals = sys.modules[\"__main__\"].__dict__\n    159     if alter_argv:\n    160         sys.argv[0] = fname\n    161     return _run_code(code, main_globals, None,\n--> 162                      \"__main__\", fname, loader, pkg_name)\n        fname = '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = 'ipykernel'\n    163 \n    164 def run_module(mod_name, init_globals=None,\n    165                run_name=None, alter_sys=False):\n    166     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x7fe0039a1330, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}, init_globals=None, mod_name='__main__', mod_fname='/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='ipykernel')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x7fe0039a1330, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    591         \n    592         If a global instance already exists, this reinitializes and starts it\n    593         \"\"\"\n    594         app = cls.instance(**kwargs)\n    595         app.initialize(argv)\n--> 596         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    597 \n    598 #-----------------------------------------------------------------------------\n    599 # utility functions, for convenience\n    600 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    437         \n    438         if self.poller is not None:\n    439             self.poller.start()\n    440         self.kernel.start()\n    441         try:\n--> 442             ioloop.IOLoop.instance().start()\n    443         except KeyboardInterrupt:\n    444             pass\n    445 \n    446 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    157             PollIOLoop.configure(ZMQIOLoop)\n    158         return PollIOLoop.current(*args, **kwargs)\n    159     \n    160     def start(self):\n    161         try:\n--> 162             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    163         except ZMQError as e:\n    164             if e.errno == ETERM:\n    165                 # quietly return on ETERM\n    166                 pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    878                 self._events.update(event_pairs)\n    879                 while self._events:\n    880                     fd, events = self._events.popitem()\n    881                     try:\n    882                         fd_obj, handler_func = self._handlers[fd]\n--> 883                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    884                     except (OSError, IOError) as e:\n    885                         if errno_from_exception(e) == errno.EPIPE:\n    886                             # Happens when the client closes the connection\n    887                             pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-06-16T19:07:27.115341', u'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', u'msg_type': u'execute_request', u'session': u'3AD45FA1ABDD44C181DDC60AF66E1EAA', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', 'msg_type': u'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['3AD45FA1ABDD44C181DDC60AF66E1EAA']\n        msg = {'buffers': [], 'content': {u'allow_stdin': True, u'code': u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-06-16T19:07:27.115341', u'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', u'msg_type': u'execute_request', u'session': u'3AD45FA1ABDD44C181DDC60AF66E1EAA', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', 'msg_type': u'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['3AD45FA1ABDD44C181DDC60AF66E1EAA'], parent={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-06-16T19:07:27.115341', u'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', u'msg_type': u'execute_request', u'session': u'3AD45FA1ABDD44C181DDC60AF66E1EAA', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', 'msg_type': u'execute_request', 'parent_header': {}})\n    386         if not silent:\n    387             self.execution_count += 1\n    388             self._publish_execute_input(code, parent, self.execution_count)\n    389 \n    390         reply_content = self.do_execute(code, silent, store_history,\n--> 391                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    392 \n    393         # Flush output before sending the reply.\n    394         sys.stdout.flush()\n    395         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    194 \n    195         reply_content = {}\n    196         # FIXME: the shell calls the exception handler itself.\n    197         shell._reply_content = None\n    198         try:\n--> 199             shell.run_cell(code, store_history=store_history, silent=silent)\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_'\n        store_history = True\n        silent = False\n    200         except:\n    201             status = u'error'\n    202             # FIXME: this code right now isn't being used yet by default,\n    203             # because the run_cell() call above directly fires off exception\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_', store_history=True, silent=False, shell_futures=True)\n   2718                 self.displayhook.exec_result = result\n   2719 \n   2720                 # Execute the user code\n   2721                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2722                 self.run_ast_nodes(code_ast.body, cell_name,\n-> 2723                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2724 \n   2725                 # Reset this so later displayed values do not modify the\n   2726                 # ExecutionResult\n   2727                 self.displayhook.exec_result = None\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Print object>, <_ast.Print object>, <_ast.Print object>, <_ast.Expr object>], cell_name='<ipython-input-18-a95ff53b18b8>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   2820 \n   2821         try:\n   2822             for i, node in enumerate(to_run_exec):\n   2823                 mod = ast.Module([node])\n   2824                 code = compiler(mod, cell_name, \"exec\")\n-> 2825                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fdfcc9020b0, file \"<ipython-input-18-a95ff53b18b8>\", line 49>\n        result = <IPython.core.interactiveshell.ExecutionResult object>\n   2826                     return True\n   2827 \n   2828             for i, node in enumerate(to_run_interactive):\n   2829                 mod = ast.Interactive([node])\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fdfcc9020b0, file \"<ipython-input-18-a95ff53b18b8>\", line 49>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   2880         outflag = 1  # happens in more places, so it's easier as default\n   2881         try:\n   2882             try:\n   2883                 self.hooks.pre_run_code_hook()\n   2884                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2885                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fdfcc9020b0, file \"<ipython-input-18-a95ff53b18b8>\", line 49>\n        self.user_global_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'CustomRegressor': <class '__main__.CustomRegressor'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'EST_PICKLE_FILENAME': 'baseline_final_estimator.pkl', 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'Imputer': <class 'sklearn.preprocessing.imputation.Imputer'>, 'In': ['', u'import pdb\\nimport numpy as np\\nimport gcp.big... features are added.\\nn_features = len(features)', u'# def mape(y, predictions):\\n#   num_timeslots...rer = make_scorer(mape, greater_is_better=False)', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u'def mape(y, predictions):\\n#   num_timeslots =...rer = make_scorer(mape, greater_is_better=False)', u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u'get_ipython().run_cell_magic(u\\'sql\\', u\\'--mo...\\n\\\\n# The above query randomizes its outputs.\")', u\"query = bq.Query(q_all)\\ntableresult = query.r...rcounter)\\nall_data_original = np.copy(all_data)\", u'# This chunk does further wrangling to dataset..._train)\\ndata_test_original = np.copy(data_test)', u\"from sklearn.preprocessing import OneHotEncode...2, 3], n_values='auto')\\none_hot.fit(data_train)\", u'one_hot.n_values_', u'one_hot.feature_indices_', u\"data_train = one_hot.transform(data_train_orig... 'new number of features: {}'.format(n_features)\", u'from sklearn.base import BaseEstimator, Regres...)\\nprint(mape(targets_test, custom_predictions))', u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_'], 'K': <module 'keras.backend' from '/usr/local/lib/python2.7/dist-packages/keras/backend/__init__.pyc'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, ...}\n        self.user_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'CustomRegressor': <class '__main__.CustomRegressor'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'EST_PICKLE_FILENAME': 'baseline_final_estimator.pkl', 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'Imputer': <class 'sklearn.preprocessing.imputation.Imputer'>, 'In': ['', u'import pdb\\nimport numpy as np\\nimport gcp.big... features are added.\\nn_features = len(features)', u'# def mape(y, predictions):\\n#   num_timeslots...rer = make_scorer(mape, greater_is_better=False)', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u'def mape(y, predictions):\\n#   num_timeslots =...rer = make_scorer(mape, greater_is_better=False)', u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u'get_ipython().run_cell_magic(u\\'sql\\', u\\'--mo...\\n\\\\n# The above query randomizes its outputs.\")', u\"query = bq.Query(q_all)\\ntableresult = query.r...rcounter)\\nall_data_original = np.copy(all_data)\", u'# This chunk does further wrangling to dataset..._train)\\ndata_test_original = np.copy(data_test)', u\"from sklearn.preprocessing import OneHotEncode...2, 3], n_values='auto')\\none_hot.fit(data_train)\", u'one_hot.n_values_', u'one_hot.feature_indices_', u\"data_train = one_hot.transform(data_train_orig... 'new number of features: {}'.format(n_features)\", u'from sklearn.base import BaseEstimator, Regres...)\\nprint(mape(targets_test, custom_predictions))', u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_'], 'K': <module 'keras.backend' from '/usr/local/lib/python2.7/dist-packages/keras/backend/__init__.pyc'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, ...}\n   2886             finally:\n   2887                 # Reset our crash handler in place\n   2888                 sys.excepthook = old_excepthook\n   2889         except SystemExit as e:\n\n...........................................................................\n/content/TeguhWPurwanto@gmail.com/Xiaojukeji Algorithm Competition/<ipython-input-18-a95ff53b18b8> in <module>()\n     44   scoring=mape_scorer,\n     45   n_jobs=5,\n     46   verbose=3\n     47 )\n     48 \n---> 49 search_params.fit(data_train_original, targets_train)\n     50 print(search_params.grid_scores_)\n     51 print(search_params.best_params_)\n     52 print(search_params.best_score_)\n     53 search_params.best_estimator_\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...scorer(mape, greater_is_better=False), verbose=3), X=array([[  3.00000000e+00,   4.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]]), y=array([  8.,   1.,   1., ...,  12.,   1.,   2.]))\n    727         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    728             Target relative to X for classification or regression;\n    729             None for unsupervised learning.\n    730 \n    731         \"\"\"\n--> 732         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...corer(mape, greater_is_better=False), verbose=3)>\n        X = array([[  3.00000000e+00,   4.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]])\n        y = array([  8.,   1.,   1., ...,  12.,   1.,   2.])\n        self.param_grid = {'estimate__max_features': [110, 111, 112, 113, 114, 115, 116, 117, 118, 119]}\n    733 \n    734 \n    735 class RandomizedSearchCV(BaseSearchCV):\n    736     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...scorer(mape, greater_is_better=False), verbose=3), X=array([[  3.00000000e+00,   4.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]]), y=array([  8.,   1.,   1., ...,  12.,   1.,   2.]), parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    500         )(\n    501             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    502                                     train, test, self.verbose, parameters,\n    503                                     self.fit_params, return_parameters=True,\n    504                                     error_score=self.error_score)\n--> 505                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    506                 for train, test in cv)\n    507 \n    508         # Out is a list of triplet: score, estimator, n_test_samples\n    509         n_fits = len(out)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=5), iterable=<itertools.islice object>)\n    661             if pre_dispatch == \"all\" or n_jobs == 1:\n    662                 # The iterable was consumed all at once by the above for loop.\n    663                 # No need to wait for async callbacks to trigger to\n    664                 # consumption.\n    665                 self._iterating = False\n--> 666             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=5)>\n    667             # Make sure that we get a last message telling us we are done\n    668             elapsed_time = time.time() - self._start_time\n    669             self._print('Done %3i out of %3i | elapsed: %s finished',\n    670                         (len(self._output),\n\n    ---------------------------------------------------------------------------\n    Sub-process traceback:\n    ---------------------------------------------------------------------------\n    TypeError                                          Thu Jun 16 19:07:27 2016\nPID: 7252                                     Python 2.7.9: /usr/bin/python\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.pyc in _fit_and_score(estimator=Pipeline(steps=[('one_hot', OneHotEncoder(catego...random_state=None,\n           splitter='best'))]), X=array([[  3.00000000e+00,   4.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]]), y=array([  8.,   1.,   1., ...,  12.,   1.,   2.]), scorer=make_scorer(mape, greater_is_better=False), train=array([ 800,  801,  802, ..., 3997, 3998, 3999]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 792,\n       793, 794, 795, 796, 797, 798, 799]), verbose=3, parameters={'estimate__max_features': 110}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1454 \n   1455     try:\n   1456         if y_train is None:\n   1457             estimator.fit(X_train, **fit_params)\n   1458         else:\n-> 1459             estimator.fit(X_train, y_train, **fit_params)\n   1460 \n   1461     except Exception as e:\n   1462         if error_score == 'raise':\n   1463             raise\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.pyc in fit(self=Pipeline(steps=[('one_hot', OneHotEncoder(catego...random_state=None,\n           splitter='best'))]), X=array([[  0.00000000e+00,   2.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]]), y=array([  5.,   6.,   3., ...,  12.,   1.,   2.]), **fit_params={})\n    135             pipeline.\n    136         y : iterable, default=None\n    137             Training targets. Must fulfill label requirements for all steps of\n    138             the pipeline.\n    139         \"\"\"\n--> 140         Xt, fit_params = self._pre_transform(X, y, **fit_params)\n    141         self.steps[-1][-1].fit(Xt, y, **fit_params)\n    142         return self\n    143 \n    144     def fit_transform(self, X, y=None, **fit_params):\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.pyc in _pre_transform(self=Pipeline(steps=[('one_hot', OneHotEncoder(catego...random_state=None,\n           splitter='best'))]), X=array([[  0.00000000e+00,   2.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]]), y=array([  5.,   6.,   3., ...,  12.,   1.,   2.]), **fit_params={})\n    116             step, param = pname.split('__', 1)\n    117             fit_params_steps[step][param] = pval\n    118         Xt = X\n    119         for name, transform in self.steps[:-1]:\n    120             if hasattr(transform, \"fit_transform\"):\n--> 121                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n    122             else:\n    123                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    124                               .transform(Xt)\n    125         return Xt, fit_params_steps[self.steps[-1][0]]\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/decomposition/pca.pyc in fit_transform(self=PCA(copy=True, n_components=120, whiten=False), X=<3200x192 sparse matrix of type '<type 'numpy.fl...with 419806 stored elements in COOrdinate format>, y=array([  5.,   6.,   3., ...,  12.,   1.,   2.]))\n    233         Returns\n    234         -------\n    235         X_new : array-like, shape (n_samples, n_components)\n    236 \n    237         \"\"\"\n--> 238         U, S, V = self._fit(X)\n    239         U = U[:, :self.n_components_]\n    240 \n    241         if self.whiten:\n    242             # X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/decomposition/pca.pyc in _fit(self=PCA(copy=True, n_components=120, whiten=False), X=<3200x192 sparse matrix of type '<type 'numpy.fl...with 419806 stored elements in COOrdinate format>)\n    260         -------\n    261         U, s, V : ndarrays\n    262             The SVD of the input data, copied and centered when\n    263             requested.\n    264         \"\"\"\n--> 265         X = check_array(X)\n    266         n_samples, n_features = X.shape\n    267         X = as_float_array(X, copy=self.copy)\n    268         # Center data\n    269         self.mean_ = np.mean(X, axis=0)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc in check_array(array=<3200x192 sparse matrix of type '<type 'numpy.fl...with 419806 stored elements in COOrdinate format>, accept_sparse=None, dtype=None, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1)\n    329 \n    330     if sp.issparse(array):\n    331         if dtype_numeric:\n    332             dtype = None\n    333         array = _ensure_sparse_format(array, accept_sparse, dtype, order,\n--> 334                                       copy, force_all_finite)\n        array = <3200x192 sparse matrix of type '<type 'numpy.fl...with 419806 stored elements in COOrdinate format>\n    335     else:\n    336         if ensure_2d:\n    337             array = np.atleast_2d(array)\n    338         if dtype_numeric:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc in _ensure_sparse_format(spmatrix=<3200x192 sparse matrix of type '<type 'numpy.fl...with 419806 stored elements in COOrdinate format>, accept_sparse=None, dtype=None, order=None, copy=False, force_all_finite=True)\n    234     -------\n    235     spmatrix_converted : scipy sparse matrix.\n    236         Matrix that is ensured to have an allowed type.\n    237     \"\"\"\n    238     if accept_sparse is None:\n--> 239         raise TypeError('A sparse matrix was passed, but dense '\n        dtype = None\n    240                         'data is required. Use X.toarray() to '\n    241                         'convert to a dense numpy array.')\n    242     sparse_type = spmatrix.format\n    243     if dtype is None:\n\nTypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mJoblibTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a95ff53b18b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0msearch_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train_original\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \"\"\"\n\u001b[1;32m--> 732\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    503\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 505\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m                 for train, test in cv)\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    664\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    547\u001b[0m                         \u001b[1;31m# Convert this to a JoblibException\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m                         \u001b[0mexception_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_mk_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibTypeError\u001b[0m: JoblibTypeError\n___________________________________________________________________________\nMultiprocessing exception:\n    ...........................................................................\n/usr/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    157     pkg_name = mod_name.rpartition('.')[0]\n    158     main_globals = sys.modules[\"__main__\"].__dict__\n    159     if alter_argv:\n    160         sys.argv[0] = fname\n    161     return _run_code(code, main_globals, None,\n--> 162                      \"__main__\", fname, loader, pkg_name)\n        fname = '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = 'ipykernel'\n    163 \n    164 def run_module(mod_name, init_globals=None,\n    165                run_name=None, alter_sys=False):\n    166     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x7fe0039a1330, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}, init_globals=None, mod_name='__main__', mod_fname='/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='ipykernel')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x7fe0039a1330, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    591         \n    592         If a global instance already exists, this reinitializes and starts it\n    593         \"\"\"\n    594         app = cls.instance(**kwargs)\n    595         app.initialize(argv)\n--> 596         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    597 \n    598 #-----------------------------------------------------------------------------\n    599 # utility functions, for convenience\n    600 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    437         \n    438         if self.poller is not None:\n    439             self.poller.start()\n    440         self.kernel.start()\n    441         try:\n--> 442             ioloop.IOLoop.instance().start()\n    443         except KeyboardInterrupt:\n    444             pass\n    445 \n    446 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    157             PollIOLoop.configure(ZMQIOLoop)\n    158         return PollIOLoop.current(*args, **kwargs)\n    159     \n    160     def start(self):\n    161         try:\n--> 162             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    163         except ZMQError as e:\n    164             if e.errno == ETERM:\n    165                 # quietly return on ETERM\n    166                 pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    878                 self._events.update(event_pairs)\n    879                 while self._events:\n    880                     fd, events = self._events.popitem()\n    881                     try:\n    882                         fd_obj, handler_func = self._handlers[fd]\n--> 883                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    884                     except (OSError, IOError) as e:\n    885                         if errno_from_exception(e) == errno.EPIPE:\n    886                             # Happens when the client closes the connection\n    887                             pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-06-16T19:07:27.115341', u'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', u'msg_type': u'execute_request', u'session': u'3AD45FA1ABDD44C181DDC60AF66E1EAA', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', 'msg_type': u'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['3AD45FA1ABDD44C181DDC60AF66E1EAA']\n        msg = {'buffers': [], 'content': {u'allow_stdin': True, u'code': u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-06-16T19:07:27.115341', u'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', u'msg_type': u'execute_request', u'session': u'3AD45FA1ABDD44C181DDC60AF66E1EAA', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', 'msg_type': u'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['3AD45FA1ABDD44C181DDC60AF66E1EAA'], parent={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-06-16T19:07:27.115341', u'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', u'msg_type': u'execute_request', u'session': u'3AD45FA1ABDD44C181DDC60AF66E1EAA', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'A645D5D0A45A45B48145C4BCFD70CA5C', 'msg_type': u'execute_request', 'parent_header': {}})\n    386         if not silent:\n    387             self.execution_count += 1\n    388             self._publish_execute_input(code, parent, self.execution_count)\n    389 \n    390         reply_content = self.do_execute(code, silent, store_history,\n--> 391                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    392 \n    393         # Flush output before sending the reply.\n    394         sys.stdout.flush()\n    395         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    194 \n    195         reply_content = {}\n    196         # FIXME: the shell calls the exception handler itself.\n    197         shell._reply_content = None\n    198         try:\n--> 199             shell.run_cell(code, store_history=store_history, silent=silent)\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_'\n        store_history = True\n        silent = False\n    200         except:\n    201             status = u'error'\n    202             # FIXME: this code right now isn't being used yet by default,\n    203             # because the run_cell() call above directly fires off exception\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_', store_history=True, silent=False, shell_futures=True)\n   2718                 self.displayhook.exec_result = result\n   2719 \n   2720                 # Execute the user code\n   2721                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2722                 self.run_ast_nodes(code_ast.body, cell_name,\n-> 2723                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2724 \n   2725                 # Reset this so later displayed values do not modify the\n   2726                 # ExecutionResult\n   2727                 self.displayhook.exec_result = None\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Print object>, <_ast.Print object>, <_ast.Print object>, <_ast.Expr object>], cell_name='<ipython-input-18-a95ff53b18b8>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   2820 \n   2821         try:\n   2822             for i, node in enumerate(to_run_exec):\n   2823                 mod = ast.Module([node])\n   2824                 code = compiler(mod, cell_name, \"exec\")\n-> 2825                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fdfcc9020b0, file \"<ipython-input-18-a95ff53b18b8>\", line 49>\n        result = <IPython.core.interactiveshell.ExecutionResult object>\n   2826                     return True\n   2827 \n   2828             for i, node in enumerate(to_run_interactive):\n   2829                 mod = ast.Interactive([node])\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fdfcc9020b0, file \"<ipython-input-18-a95ff53b18b8>\", line 49>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   2880         outflag = 1  # happens in more places, so it's easier as default\n   2881         try:\n   2882             try:\n   2883                 self.hooks.pre_run_code_hook()\n   2884                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2885                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fdfcc9020b0, file \"<ipython-input-18-a95ff53b18b8>\", line 49>\n        self.user_global_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'CustomRegressor': <class '__main__.CustomRegressor'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'EST_PICKLE_FILENAME': 'baseline_final_estimator.pkl', 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'Imputer': <class 'sklearn.preprocessing.imputation.Imputer'>, 'In': ['', u'import pdb\\nimport numpy as np\\nimport gcp.big... features are added.\\nn_features = len(features)', u'# def mape(y, predictions):\\n#   num_timeslots...rer = make_scorer(mape, greater_is_better=False)', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u'def mape(y, predictions):\\n#   num_timeslots =...rer = make_scorer(mape, greater_is_better=False)', u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u'get_ipython().run_cell_magic(u\\'sql\\', u\\'--mo...\\n\\\\n# The above query randomizes its outputs.\")', u\"query = bq.Query(q_all)\\ntableresult = query.r...rcounter)\\nall_data_original = np.copy(all_data)\", u'# This chunk does further wrangling to dataset..._train)\\ndata_test_original = np.copy(data_test)', u\"from sklearn.preprocessing import OneHotEncode...2, 3], n_values='auto')\\none_hot.fit(data_train)\", u'one_hot.n_values_', u'one_hot.feature_indices_', u\"data_train = one_hot.transform(data_train_orig... 'new number of features: {}'.format(n_features)\", u'from sklearn.base import BaseEstimator, Regres...)\\nprint(mape(targets_test, custom_predictions))', u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_'], 'K': <module 'keras.backend' from '/usr/local/lib/python2.7/dist-packages/keras/backend/__init__.pyc'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, ...}\n        self.user_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'CustomRegressor': <class '__main__.CustomRegressor'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'EST_PICKLE_FILENAME': 'baseline_final_estimator.pkl', 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'Imputer': <class 'sklearn.preprocessing.imputation.Imputer'>, 'In': ['', u'import pdb\\nimport numpy as np\\nimport gcp.big... features are added.\\nn_features = len(features)', u'# def mape(y, predictions):\\n#   num_timeslots...rer = make_scorer(mape, greater_is_better=False)', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u'from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))', u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u'def mape(y, predictions):\\n#   num_timeslots =...rer = make_scorer(mape, greater_is_better=False)', u\"from sklearn.linear_model import LogisticRegre... = est.predict(X)\\n# print(mape(y, predictions))\", u'get_ipython().run_cell_magic(u\\'sql\\', u\\'--mo...\\n\\\\n# The above query randomizes its outputs.\")', u\"query = bq.Query(q_all)\\ntableresult = query.r...rcounter)\\nall_data_original = np.copy(all_data)\", u'# This chunk does further wrangling to dataset..._train)\\ndata_test_original = np.copy(data_test)', u\"from sklearn.preprocessing import OneHotEncode...2, 3], n_values='auto')\\none_hot.fit(data_train)\", u'one_hot.n_values_', u'one_hot.feature_indices_', u\"data_train = one_hot.transform(data_train_orig... 'new number of features: {}'.format(n_features)\", u'from sklearn.base import BaseEstimator, Regres...)\\nprint(mape(targets_test, custom_predictions))', u'from sklearn.feature_selection import SelectKB...rams.best_score_)\\nsearch_params.best_estimator_'], 'K': <module 'keras.backend' from '/usr/local/lib/python2.7/dist-packages/keras/backend/__init__.pyc'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, ...}\n   2886             finally:\n   2887                 # Reset our crash handler in place\n   2888                 sys.excepthook = old_excepthook\n   2889         except SystemExit as e:\n\n...........................................................................\n/content/TeguhWPurwanto@gmail.com/Xiaojukeji Algorithm Competition/<ipython-input-18-a95ff53b18b8> in <module>()\n     44   scoring=mape_scorer,\n     45   n_jobs=5,\n     46   verbose=3\n     47 )\n     48 \n---> 49 search_params.fit(data_train_original, targets_train)\n     50 print(search_params.grid_scores_)\n     51 print(search_params.best_params_)\n     52 print(search_params.best_score_)\n     53 search_params.best_estimator_\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...scorer(mape, greater_is_better=False), verbose=3), X=array([[  3.00000000e+00,   4.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]]), y=array([  8.,   1.,   1., ...,  12.,   1.,   2.]))\n    727         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    728             Target relative to X for classification or regression;\n    729             None for unsupervised learning.\n    730 \n    731         \"\"\"\n--> 732         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...corer(mape, greater_is_better=False), verbose=3)>\n        X = array([[  3.00000000e+00,   4.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]])\n        y = array([  8.,   1.,   1., ...,  12.,   1.,   2.])\n        self.param_grid = {'estimate__max_features': [110, 111, 112, 113, 114, 115, 116, 117, 118, 119]}\n    733 \n    734 \n    735 class RandomizedSearchCV(BaseSearchCV):\n    736     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...scorer(mape, greater_is_better=False), verbose=3), X=array([[  3.00000000e+00,   4.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]]), y=array([  8.,   1.,   1., ...,  12.,   1.,   2.]), parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    500         )(\n    501             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    502                                     train, test, self.verbose, parameters,\n    503                                     self.fit_params, return_parameters=True,\n    504                                     error_score=self.error_score)\n--> 505                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    506                 for train, test in cv)\n    507 \n    508         # Out is a list of triplet: score, estimator, n_test_samples\n    509         n_fits = len(out)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=5), iterable=<itertools.islice object>)\n    661             if pre_dispatch == \"all\" or n_jobs == 1:\n    662                 # The iterable was consumed all at once by the above for loop.\n    663                 # No need to wait for async callbacks to trigger to\n    664                 # consumption.\n    665                 self._iterating = False\n--> 666             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=5)>\n    667             # Make sure that we get a last message telling us we are done\n    668             elapsed_time = time.time() - self._start_time\n    669             self._print('Done %3i out of %3i | elapsed: %s finished',\n    670                         (len(self._output),\n\n    ---------------------------------------------------------------------------\n    Sub-process traceback:\n    ---------------------------------------------------------------------------\n    TypeError                                          Thu Jun 16 19:07:27 2016\nPID: 7252                                     Python 2.7.9: /usr/bin/python\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.pyc in _fit_and_score(estimator=Pipeline(steps=[('one_hot', OneHotEncoder(catego...random_state=None,\n           splitter='best'))]), X=array([[  3.00000000e+00,   4.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]]), y=array([  8.,   1.,   1., ...,  12.,   1.,   2.]), scorer=make_scorer(mape, greater_is_better=False), train=array([ 800,  801,  802, ..., 3997, 3998, 3999]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 792,\n       793, 794, 795, 796, 797, 798, 799]), verbose=3, parameters={'estimate__max_features': 110}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1454 \n   1455     try:\n   1456         if y_train is None:\n   1457             estimator.fit(X_train, **fit_params)\n   1458         else:\n-> 1459             estimator.fit(X_train, y_train, **fit_params)\n   1460 \n   1461     except Exception as e:\n   1462         if error_score == 'raise':\n   1463             raise\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.pyc in fit(self=Pipeline(steps=[('one_hot', OneHotEncoder(catego...random_state=None,\n           splitter='best'))]), X=array([[  0.00000000e+00,   2.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]]), y=array([  5.,   6.,   3., ...,  12.,   1.,   2.]), **fit_params={})\n    135             pipeline.\n    136         y : iterable, default=None\n    137             Training targets. Must fulfill label requirements for all steps of\n    138             the pipeline.\n    139         \"\"\"\n--> 140         Xt, fit_params = self._pre_transform(X, y, **fit_params)\n    141         self.steps[-1][-1].fit(Xt, y, **fit_params)\n    142         return self\n    143 \n    144     def fit_transform(self, X, y=None, **fit_params):\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.pyc in _pre_transform(self=Pipeline(steps=[('one_hot', OneHotEncoder(catego...random_state=None,\n           splitter='best'))]), X=array([[  0.00000000e+00,   2.00000000e+00,   4....200000e+03,   1.90900000e+03,   0.00000000e+00]]), y=array([  5.,   6.,   3., ...,  12.,   1.,   2.]), **fit_params={})\n    116             step, param = pname.split('__', 1)\n    117             fit_params_steps[step][param] = pval\n    118         Xt = X\n    119         for name, transform in self.steps[:-1]:\n    120             if hasattr(transform, \"fit_transform\"):\n--> 121                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n    122             else:\n    123                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    124                               .transform(Xt)\n    125         return Xt, fit_params_steps[self.steps[-1][0]]\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/decomposition/pca.pyc in fit_transform(self=PCA(copy=True, n_components=120, whiten=False), X=<3200x192 sparse matrix of type '<type 'numpy.fl...with 419806 stored elements in COOrdinate format>, y=array([  5.,   6.,   3., ...,  12.,   1.,   2.]))\n    233         Returns\n    234         -------\n    235         X_new : array-like, shape (n_samples, n_components)\n    236 \n    237         \"\"\"\n--> 238         U, S, V = self._fit(X)\n    239         U = U[:, :self.n_components_]\n    240 \n    241         if self.whiten:\n    242             # X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/decomposition/pca.pyc in _fit(self=PCA(copy=True, n_components=120, whiten=False), X=<3200x192 sparse matrix of type '<type 'numpy.fl...with 419806 stored elements in COOrdinate format>)\n    260         -------\n    261         U, s, V : ndarrays\n    262             The SVD of the input data, copied and centered when\n    263             requested.\n    264         \"\"\"\n--> 265         X = check_array(X)\n    266         n_samples, n_features = X.shape\n    267         X = as_float_array(X, copy=self.copy)\n    268         # Center data\n    269         self.mean_ = np.mean(X, axis=0)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc in check_array(array=<3200x192 sparse matrix of type '<type 'numpy.fl...with 419806 stored elements in COOrdinate format>, accept_sparse=None, dtype=None, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1)\n    329 \n    330     if sp.issparse(array):\n    331         if dtype_numeric:\n    332             dtype = None\n    333         array = _ensure_sparse_format(array, accept_sparse, dtype, order,\n--> 334                                       copy, force_all_finite)\n        array = <3200x192 sparse matrix of type '<type 'numpy.fl...with 419806 stored elements in COOrdinate format>\n    335     else:\n    336         if ensure_2d:\n    337             array = np.atleast_2d(array)\n    338         if dtype_numeric:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc in _ensure_sparse_format(spmatrix=<3200x192 sparse matrix of type '<type 'numpy.fl...with 419806 stored elements in COOrdinate format>, accept_sparse=None, dtype=None, order=None, copy=False, force_all_finite=True)\n    234     -------\n    235     spmatrix_converted : scipy sparse matrix.\n    236         Matrix that is ensured to have an allowed type.\n    237     \"\"\"\n    238     if accept_sparse is None:\n--> 239         raise TypeError('A sparse matrix was passed, but dense '\n        dtype = None\n    240                         'data is required. Use X.toarray() to '\n    241                         'convert to a dense numpy array.')\n    242     sparse_type = spmatrix.format\n    243     if dtype is None:\n\nTypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], n_values='auto')),\n",
    "#   ('impute', Imputer(0)),\n",
    "#   ('feature_selection', SelectKBest(f_classif)),\n",
    "  ('pca', PCA(n_components=120)),\n",
    "  ('estimate', DecisionTreeRegressor())\n",
    "]\n",
    "\n",
    "est = Pipeline(steps)\n",
    "\n",
    "params = {\n",
    "#   'one_hot__n_values': [7, 10, 20],\n",
    "#   \"feature_selection__k\": [i for i in range(1, n_features - 1)]\n",
    "  'estimate__max_features': [i for i in range(110, 120)],\n",
    "#   'estimate__learning_rate': [0.1, 0.5, 1, 5, 10],\n",
    "#   'estimate__n_estimators': [i for i in range(110, 120, 2)],\n",
    "#   'estimate__loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "# cross_validation_iter = StratifiedShuffleSplit(y=targets_train, test_size=0.3,\n",
    "#                                                random_state=RANDOM_STATE, n_iter=10)\n",
    "# search_params = RandomizedSearchCV(\n",
    "#   estimator=est,\n",
    "#   param_distributions=params,\n",
    "#   cv=5,\n",
    "#   scoring=mape_scorer,\n",
    "#   n_jobs=2,\n",
    "#   verbose=1\n",
    "# )\n",
    "\n",
    "search_params = GridSearchCV(\n",
    "  estimator=est,\n",
    "  param_grid=params,\n",
    "  cv=5,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=5,\n",
    "  verbose=3\n",
    ")\n",
    "\n",
    "search_params.fit(data_train_original, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_est = search_params.best_estimator_\n",
    "test_predictions = final_est.predict(data_test_original)\n",
    "print(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(final_est, open(EST_PICKLE_FILENAME, \"w\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"Process Final Test Data With Final Algorithm\" to use pickled final algorithm against final test data to produce csv required by this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Just testing Imputer. Turns out somehow Imputer causes number of features reduced, weird.\n",
    "\n",
    "# imputer = Imputer()\n",
    "est = DecisionTreeRegressor(max_features=len(features))\n",
    "\n",
    "data_train_i = np.copy(data_train)\n",
    "print(data_train.shape)\n",
    "print(data_train[0:10])\n",
    "# data_train_i = imputer.fit_transform(data_train)\n",
    "data_train_i[np.isnan(data_train_i)] = 0\n",
    "data_train_i.astype('float32')\n",
    "print(data_train_i.shape)\n",
    "print(data_train_i[0:10])\n",
    "est.fit(data_train_i, targets_train)\n",
    "predictions = est.predict(data_test)\n",
    "print(mape(data_test, predictions, targets_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "- DecisionTreeRegressor + all features (31951 training data): 2.141\n",
    "- DecisionTreeRegressor + all features (44202 training data): 3.065\n",
    "- DecisionTreeRegressor + all + one hot encoded features (81832 training data): 5.726\n",
    "- AdaBoostRegressor + all + one hot encoded features + PCA (102592 training data): "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
