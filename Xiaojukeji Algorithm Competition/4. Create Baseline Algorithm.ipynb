{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.pipeline import Pipeline\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME = 'baseline_final_estimator.pkl'\n",
    "\n",
    "# First feature HAS to be 'district_id' for MAPE calculation.\n",
    "fields_str = \"\"\"\n",
    "district_id\ttimeofday_slot\tday_in_week\tis_sunday\tsum_price\tavg_price\tpoi1\tpoi2\tpoi3\n",
    "\tpoi4\tpoi5\ttraffic_tj_level1\ttraffic_tj_level2\ttraffic_tj_level3\ttraffic_tj_level4\n",
    "\tweather\tweather_pm25\tweather_temperature\tgap\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mape(X, predictions, y):\n",
    "  num_timeslots = 43\n",
    "  num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  Xy = np.concatenate((X, y.T, predictions.T), axis=1)\n",
    "  districts = np.unique(X[:,0])\n",
    "  district_scores = np.zeros(len(districts))\n",
    "  for counter, key in enumerate(districts):\n",
    "    group = np.compress((Xy[:,0] == key).flat, Xy, axis=0)\n",
    "    district_scores[counter] = np.sum(np.absolute(\n",
    "        (group[:,-2] -\n",
    "         group[:,-1])/\n",
    "        group[:,-2]\n",
    "      )) / num_timeslots\n",
    "  return np.sum(district_scores) / num_districts\n",
    "\n",
    "def mape_scorer(estimator, X, y):\n",
    "  predictions = estimator.predict(X)\n",
    "  return -mape(X, predictions, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "est = LogisticRegression()\n",
    "X = np.array([[1, 1], [1, 2], [2, 3], [2, 4]])\n",
    "predictions = np.array([1, 2, 3, 4])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "\n",
    "# Should return 0.0\n",
    "mape(X, predictions, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.future_gaps_final1]\n",
    "WHERE gap > 0\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = bq.Query(q_all)\n",
    "tableresult = query.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 102680 rows\n",
      "there are 102680 rows\n",
      "there are 102680 rows\n",
      "processed 0 rows\n",
      "processed 0 rows\n",
      "processed 0 rows\n",
      "processed 5000 rows\n",
      "processed 5000 rows\n",
      "processed 5000 rows\n",
      "processed 10000 rows\n",
      "processed 10000 rows\n",
      "processed 10000 rows\n",
      "processed 15000 rows\n",
      "processed 15000 rows\n",
      "processed 15000 rows\n",
      "processed 20000 rows\n",
      "processed 20000 rows\n",
      "processed 20000 rows\n",
      "processed 25000 rows\n",
      "processed 25000 rows\n",
      "processed 25000 rows\n",
      "processed 30000 rows\n",
      "processed 30000 rows\n",
      "processed 30000 rows\n",
      "processed 35000 rows\n",
      "processed 35000 rows\n",
      "processed 35000 rows\n",
      "processed 40000 rows\n",
      "processed 40000 rows\n",
      "processed 40000 rows\n",
      "processed 45000 rows\n",
      "processed 45000 rows\n",
      "processed 45000 rows\n",
      "processed 50000 rows\n",
      "processed 50000 rows\n",
      "processed 50000 rows\n",
      "processed 55000 rows\n",
      "processed 55000 rows\n",
      "processed 55000 rows\n",
      "processed 60000 rows\n",
      "processed 60000 rows\n",
      "processed 60000 rows\n",
      "processed 65000 rows\n",
      "processed 65000 rows\n",
      "processed 65000 rows\n",
      "processed 70000 rows\n",
      "processed 70000 rows\n",
      "processed 70000 rows\n",
      "processed 75000 rows\n",
      "processed 75000 rows\n",
      "processed 75000 rows\n",
      "processed 80000 rows\n",
      "processed 80000 rows\n",
      "processed 80000 rows\n",
      "processed 85000 rows\n",
      "processed 85000 rows\n",
      "processed 85000 rows\n",
      "processed 90000 rows\n",
      "processed 90000 rows\n",
      "processed 90000 rows\n",
      "processed 95000 rows\n",
      "processed 95000 rows\n",
      "processed 95000 rows\n",
      "processed 100000 rows\n",
      "processed 100000 rows\n",
      "processed 100000 rows\n"
     ]
    }
   ],
   "source": [
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2016-01-22-46','2016-01-22-58','2016-01-22-70','2016-01-22-82','2016-01-22-94','2016-01-22-106','2016-01-22-118','2016-01-22-130','2016-01-22-142','2016-01-24-58','2016-01-24-70','2016-01-24-82','2016-01-24-94','2016-01-24-106','2016-01-24-118','2016-01-24-130','2016-01-24-142','2016-01-26-46','2016-01-26-58','2016-01-26-70','2016-01-26-82','2016-01-26-94','2016-01-26-106','2016-01-26-118','2016-01-26-130','2016-01-26-142','2016-01-28-58','2016-01-28-70','2016-01-28-82','2016-01-28-94','2016-01-28-106','2016-01-28-118','2016-01-28-130','2016-01-28-142','2016-01-30-46','2016-01-30-58','2016-01-30-70','2016-01-30-82','2016-01-30-94','2016-01-30-106','2016-01-30-118','2016-01-30-130','2016-01-30-142'\n"
     ]
    }
   ],
   "source": [
    "# Get timeslots to test from GCS\n",
    "item = storage.Item('datalab-projects-1331-datalab','data/timeslots_to_test.txt')\n",
    "timeslots_to_test = item.read_from().strip().split('\\n')\n",
    "tquery = ','.join(map(lambda x: \"'{}'\".format(x), timeslots_to_test))\n",
    "print(tquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all_t\n",
    "\n",
    "SELECT *\n",
    "FROM [datalab-projects-1331:xjk_algo_comp_test.future_gaps_final1]\n",
    "WHERE gap > 0 AND timeslot NOT IN ('2016-01-22-46','2016-01-22-58','2016-01-22-70','2016-01-22-82',\n",
    "    '2016-01-22-94','2016-01-22-106','2016-01-22-118','2016-01-22-130','2016-01-22-142',\n",
    "    '2016-01-24-58','2016-01-24-70','2016-01-24-82','2016-01-24-94','2016-01-24-106',\n",
    "    '2016-01-24-118','2016-01-24-130','2016-01-24-142','2016-01-26-46','2016-01-26-58',\n",
    "    '2016-01-26-70','2016-01-26-82','2016-01-26-94','2016-01-26-106','2016-01-26-118',\n",
    "    '2016-01-26-130','2016-01-26-142','2016-01-28-58','2016-01-28-70','2016-01-28-82',\n",
    "    '2016-01-28-94','2016-01-28-106','2016-01-28-118','2016-01-28-130','2016-01-28-142',\n",
    "    '2016-01-30-46','2016-01-30-58','2016-01-30-70','2016-01-30-82','2016-01-30-94',\n",
    "    '2016-01-30-106','2016-01-30-118','2016-01-30-130','2016-01-30-142')\n",
    "ORDER BY timeslot, district_id\n",
    "\n",
    "# Test dataset - used to check if estimator can generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 3509 rows\n",
      "there are 3509 rows\n",
      "there are 3509 rows\n",
      "processed 0 rows\n",
      "processed 1000 rows\n",
      "processed 0 rows\n",
      "processed 1000 rows\n",
      "processed 0 rows\n",
      "processed 1000 rows\n",
      "processed 2000 rows\n",
      "processed 2000 rows\n",
      "processed 2000 rows\n",
      "processed 3000 rows\n",
      "processed 3000 rows\n",
      "processed 3000 rows\n"
     ]
    }
   ],
   "source": [
    "query_t = bq.Query(q_all_t)\n",
    "tableresult_t = query_t.results()\n",
    "\n",
    "all_data_t = np.zeros((tableresult_t.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult_t.length)\n",
    "for rcounter, row in enumerate(tableresult_t):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data_t[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 1000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Testing Algorithm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([     2,      2,      2, ..., 102679, 102679, 102679]), array([15, 16, 17, ..., 15, 16, 17]))\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64))\n",
      "np.max= nan\n",
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([     2,      2,      2, ..., 102679, 102679, 102679]), array([15, 16, 17, ..., 15, 16, 17]))\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64))\n",
      "np.max= nan\n"
     ]
    }
   ],
   "source": [
    "# Useful code to check NaN and Inf values. This is needed since these values would\n",
    "# cause \"Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "# errors when left unchecked.\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data))\n",
    "print \"np.max=\", np.max(abs(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data[np.isnan(all_data)] = 0\n",
    "all_data_t[np.isnan(all_data_t)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([], dtype=int64), array([], dtype=int64))\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64))\n",
      "np.max= 708222.866131\n",
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([], dtype=int64), array([], dtype=int64))\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64))\n",
      "np.max= 708222.866131\n",
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([], dtype=int64), array([], dtype=int64))\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64))\n",
      "np.max= 708222.866131\n"
     ]
    }
   ],
   "source": [
    "# Useful code to check NaN and Inf values. This is needed since these values would\n",
    "# cause \"Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "# errors when left unchecked.\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data))\n",
    "print \"np.max=\", np.max(abs(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n",
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n",
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   1 jobs       | elapsed:    2.9s\n",
      "[Parallel(n_jobs=2)]: Done   1 jobs       | elapsed:    2.9s\n",
      "[Parallel(n_jobs=2)]: Done   1 jobs       | elapsed:    2.9s\n",
      "[Parallel(n_jobs=2)]: Done  50 jobs       | elapsed:  7.1min\n",
      "[Parallel(n_jobs=2)]: Done  50 jobs       | elapsed:  7.1min\n",
      "[Parallel(n_jobs=2)]: Done  50 jobs       | elapsed:  7.1min\n",
      "[Parallel(n_jobs=2)]: Done 200 jobs       | elapsed: 23.8min\n",
      "[Parallel(n_jobs=2)]: Done 200 jobs       | elapsed: 23.8min\n",
      "[Parallel(n_jobs=2)]: Done 200 jobs       | elapsed: 23.8min\n",
      "[Parallel(n_jobs=2)]: Done 208 out of 210 | elapsed: 24.2min remaining:   14.0s\n",
      "[Parallel(n_jobs=2)]: Done 208 out of 210 | elapsed: 24.2min remaining:   14.0s\n",
      "[Parallel(n_jobs=2)]: Done 208 out of 210 | elapsed: 24.2min remaining:   14.0s\n",
      "[Parallel(n_jobs=2)]: Done 210 out of 210 | elapsed: 24.3min finished\n",
      "[Parallel(n_jobs=2)]: Done 210 out of 210 | elapsed: 24.3min finished\n",
      "[Parallel(n_jobs=2)]: Done 210 out of 210 | elapsed: 24.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: -5.29049, std: 0.02569, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 5}, mean: -5.28037, std: 0.03104, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 10}, mean: -5.26617, std: 0.02683, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 20}, mean: -5.28501, std: 0.01981, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 50}, mean: -5.28259, std: 0.02318, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 60}, mean: -4.87650, std: 0.27085, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 80}, mean: -5.27115, std: 0.02841, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 5}, mean: -5.24891, std: 0.03472, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 10}, mean: -4.91992, std: 0.45396, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 20}, mean: -6.13432, std: 0.44687, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 50}, mean: -6.22422, std: 0.19429, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 60}, mean: -5.90961, std: 0.37479, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 80}, mean: -5.23093, std: 0.03361, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 5}, mean: -5.30336, std: 0.52450, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 10}, mean: -6.58007, std: 0.14881, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 20}, mean: -5.78781, std: 0.36263, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 50}, mean: -5.61950, std: 0.45422, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 60}, mean: -5.42400, std: 0.24917, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 80}, mean: -4.97349, std: 0.32762, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 5}, mean: -6.03865, std: 0.51711, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 10}, mean: -6.14636, std: 0.08171, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 20}, mean: -5.46231, std: 0.19477, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 50}, mean: -5.43265, std: 0.36597, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 60}, mean: -5.16448, std: 0.16300, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 80}, mean: -6.41823, std: 0.20463, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 5}, mean: -5.52005, std: 0.37602, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 10}, mean: -4.49329, std: 0.62542, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 20}, mean: -4.90395, std: 0.51264, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 50}, mean: -5.17757, std: 1.14955, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 60}, mean: -5.23603, std: 0.39902, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 80}, mean: -10.90628, std: 2.00543, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 5}, mean: -6.75051, std: 1.71083, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 10}, mean: -5.97011, std: 0.52167, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 20}, mean: -7.11787, std: 1.87902, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 50}, mean: -8.27719, std: 3.63419, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 60}, mean: -6.82797, std: 2.83173, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 80}, mean: -7.12818, std: 1.18821, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 5}, mean: -11.87493, std: 10.78721, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 10}, mean: -8.88131, std: 3.19321, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 20}, mean: -19.36896, std: 9.20833, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 50}, mean: -9.49984, std: 5.25723, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 60}, mean: -11.04388, std: 6.98358, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 80}]\n",
      "{'estimate__learning_rate': 3, 'estimate__n_estimators': 20}\n",
      "-4.49328548884\n",
      "[mean: -5.29049, std: 0.02569, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 5}, mean: -5.28037, std: 0.03104, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 10}, mean: -5.26617, std: 0.02683, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 20}, mean: -5.28501, std: 0.01981, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 50}, mean: -5.28259, std: 0.02318, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 60}, mean: -4.87650, std: 0.27085, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 80}, mean: -5.27115, std: 0.02841, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 5}, mean: -5.24891, std: 0.03472, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 10}, mean: -4.91992, std: 0.45396, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 20}, mean: -6.13432, std: 0.44687, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 50}, mean: -6.22422, std: 0.19429, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 60}, mean: -5.90961, std: 0.37479, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 80}, mean: -5.23093, std: 0.03361, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 5}, mean: -5.30336, std: 0.52450, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 10}, mean: -6.58007, std: 0.14881, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 20}, mean: -5.78781, std: 0.36263, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 50}, mean: -5.61950, std: 0.45422, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 60}, mean: -5.42400, std: 0.24917, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 80}, mean: -4.97349, std: 0.32762, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 5}, mean: -6.03865, std: 0.51711, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 10}, mean: -6.14636, std: 0.08171, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 20}, mean: -5.46231, std: 0.19477, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 50}, mean: -5.43265, std: 0.36597, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 60}, mean: -5.16448, std: 0.16300, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 80}, mean: -6.41823, std: 0.20463, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 5}, mean: -5.52005, std: 0.37602, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 10}, mean: -4.49329, std: 0.62542, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 20}, mean: -4.90395, std: 0.51264, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 50}, mean: -5.17757, std: 1.14955, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 60}, mean: -5.23603, std: 0.39902, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 80}, mean: -10.90628, std: 2.00543, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 5}, mean: -6.75051, std: 1.71083, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 10}, mean: -5.97011, std: 0.52167, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 20}, mean: -7.11787, std: 1.87902, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 50}, mean: -8.27719, std: 3.63419, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 60}, mean: -6.82797, std: 2.83173, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 80}, mean: -7.12818, std: 1.18821, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 5}, mean: -11.87493, std: 10.78721, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 10}, mean: -8.88131, std: 3.19321, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 20}, mean: -19.36896, std: 9.20833, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 50}, mean: -9.49984, std: 5.25723, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 60}, mean: -11.04388, std: 6.98358, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 80}]\n",
      "{'estimate__learning_rate': 3, 'estimate__n_estimators': 20}\n",
      "-4.49328548884\n",
      "[mean: -5.29049, std: 0.02569, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 5}, mean: -5.28037, std: 0.03104, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 10}, mean: -5.26617, std: 0.02683, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 20}, mean: -5.28501, std: 0.01981, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 50}, mean: -5.28259, std: 0.02318, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 60}, mean: -4.87650, std: 0.27085, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 80}, mean: -5.27115, std: 0.02841, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 5}, mean: -5.24891, std: 0.03472, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 10}, mean: -4.91992, std: 0.45396, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 20}, mean: -6.13432, std: 0.44687, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 50}, mean: -6.22422, std: 0.19429, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 60}, mean: -5.90961, std: 0.37479, params: {'estimate__learning_rate': 0.3, 'estimate__n_estimators': 80}, mean: -5.23093, std: 0.03361, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 5}, mean: -5.30336, std: 0.52450, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 10}, mean: -6.58007, std: 0.14881, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 20}, mean: -5.78781, std: 0.36263, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 50}, mean: -5.61950, std: 0.45422, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 60}, mean: -5.42400, std: 0.24917, params: {'estimate__learning_rate': 0.7, 'estimate__n_estimators': 80}, mean: -4.97349, std: 0.32762, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 5}, mean: -6.03865, std: 0.51711, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 10}, mean: -6.14636, std: 0.08171, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 20}, mean: -5.46231, std: 0.19477, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 50}, mean: -5.43265, std: 0.36597, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 60}, mean: -5.16448, std: 0.16300, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 80}, mean: -6.41823, std: 0.20463, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 5}, mean: -5.52005, std: 0.37602, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 10}, mean: -4.49329, std: 0.62542, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 20}, mean: -4.90395, std: 0.51264, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 50}, mean: -5.17757, std: 1.14955, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 60}, mean: -5.23603, std: 0.39902, params: {'estimate__learning_rate': 3, 'estimate__n_estimators': 80}, mean: -10.90628, std: 2.00543, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 5}, mean: -6.75051, std: 1.71083, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 10}, mean: -5.97011, std: 0.52167, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 20}, mean: -7.11787, std: 1.87902, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 50}, mean: -8.27719, std: 3.63419, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 60}, mean: -6.82797, std: 2.83173, params: {'estimate__learning_rate': 5, 'estimate__n_estimators': 80}, mean: -7.12818, std: 1.18821, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 5}, mean: -11.87493, std: 10.78721, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 10}, mean: -8.88131, std: 3.19321, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 20}, mean: -19.36896, std: 9.20833, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 50}, mean: -9.49984, std: 5.25723, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 60}, mean: -11.04388, std: 6.98358, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 80}]\n",
      "{'estimate__learning_rate': 3, 'estimate__n_estimators': 20}\n",
      "-4.49328548884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('estimate', AdaBoostRegressor(base_estimator=None, learning_rate=3, loss='linear',\n",
       "         n_estimators=20, random_state=None))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('estimate', AdaBoostRegressor(base_estimator=None, learning_rate=3, loss='linear',\n",
       "         n_estimators=20, random_state=None))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('estimate', AdaBoostRegressor(base_estimator=None, learning_rate=3, loss='linear',\n",
       "         n_estimators=20, random_state=None))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "# from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "steps = [\n",
    "#   ('impute', Imputer(0)),\n",
    "#   ('feature_selection', SelectKBest(f_classif)),\n",
    "  ('estimate', AdaBoostRegressor())\n",
    "]\n",
    "\n",
    "est = Pipeline(steps)\n",
    "\n",
    "data_train = all_data[:,1:]\n",
    "targets_train = all_data[:,0]\n",
    "data_test = all_data_t[:,1:]\n",
    "targets_test = all_data_t[:,0]\n",
    "\n",
    "params = {\n",
    "#   \"feature_selection__k\": [i for i in range(1, len(features) - 1)]\n",
    "#   'estimate__max_features': [i for i in range(1, len(features))],\n",
    "#   'estimate__n_estimators': [5, 10, 15, 20, 30]\n",
    "  'estimate__learning_rate': [0.1, 0.3, 0.7, 1, 3, 5, 10],\n",
    "  'estimate__n_estimators': [5, 10, 20, 50, 60, 80]\n",
    "}\n",
    "# cross_validation_iter = StratifiedShuffleSplit(y=targets_train, test_size=0.3,\n",
    "#                                                random_state=RANDOM_STATE, n_iter=10)\n",
    "# search_params = RandomizedSearchCV(\n",
    "#   estimator=est,\n",
    "#   param_distributions=params,\n",
    "# #   cv=10,\n",
    "#   scoring=mape_scorer,\n",
    "#   n_jobs=2,\n",
    "#   n_iter=5\n",
    "# )\n",
    "\n",
    "search_params = GridSearchCV(\n",
    "  estimator=est,\n",
    "  param_grid=params,\n",
    "  cv=5,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=2,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "search_params.fit(data_train, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.55339248842\n",
      "2.55339248842\n",
      "2.55339248842\n"
     ]
    }
   ],
   "source": [
    "final_est = search_params.best_estimator_\n",
    "test_predictions = final_est.predict(data_test)\n",
    "print(mape(data_test, test_predictions, targets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(final_est, open(EST_PICKLE_FILENAME, \"w\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"Process Final Test Data With Final Algorithm\" to use pickled final algorithm against final test data to produce csv required by this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102680, 18)\n",
      "[[  1.70000000e+01   3.00000000e+00   0.00000000e+00   3.27000000e+02\n",
      "    2.97272727e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   7.28000000e+02   7.40000000e+01\n",
      "    2.30000000e+01   2.90000000e+01   2.00000000e+00   1.06000000e+02\n",
      "    1.00000000e+00   3.00000000e+00]\n",
      " [  1.09000000e+02   4.00000000e+00   0.00000000e+00   1.27000000e+02\n",
      "    1.81428571e+01  -6.11945684e+04   1.66019762e+03   2.40659554e+02\n",
      "    7.81203839e+03   3.14309459e+02   1.23000000e+02   1.90000000e+01\n",
      "    3.00000000e+00   1.00000000e+00   3.00000000e+00   1.24000000e+02\n",
      "    8.00000000e+00   4.00000000e+00]\n",
      " [  9.70000000e+01   4.00000000e+00   0.00000000e+00   2.49000000e+02\n",
      "    1.55625000e+01  -5.73741656e+04  -3.33111579e+03   2.33390366e+03\n",
      "    3.77453856e+03  -7.61532298e+02   2.34000000e+02   6.60000000e+01\n",
      "    1.80000000e+01   8.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.00000000e+00]\n",
      " [  3.50000000e+01   6.00000000e+00   0.00000000e+00   6.00000000e+00\n",
      "    6.00000000e+00  -6.42277681e+04   2.78168341e+03   2.42397866e+03\n",
      "    6.50884795e+03   3.59594688e+02   5.50000000e+01   0.00000000e+00\n",
      "    6.00000000e+00   2.00000000e+00   2.00000000e+00   1.79000000e+02\n",
      "    7.00000000e+00   1.00000000e+00]\n",
      " [  4.00000000e+01   2.00000000e+00   0.00000000e+00   4.65000000e+02\n",
      "    2.90625000e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   1.00100000e+03   1.08000000e+02\n",
      "    4.70000000e+01   4.10000000e+01   1.00000000e+00   1.50000000e+02\n",
      "    0.00000000e+00   6.00000000e+00]\n",
      " [  1.12000000e+02   2.00000000e+00   0.00000000e+00   1.70240000e+03\n",
      "    1.89155556e+01   1.95232410e+04  -4.96296783e+03   1.37424953e+03\n",
      "   -1.60827084e+04  -3.46889739e+03   9.90000000e+02   4.50000000e+02\n",
      "    1.14000000e+02   7.90000000e+01   4.00000000e+00   4.20000000e+01\n",
      "    8.00000000e+00   3.00000000e+00]\n",
      " [  1.05000000e+02   4.00000000e+00   0.00000000e+00   1.55100000e+02\n",
      "    2.58500000e+01  -4.91075399e+04   2.69284708e+03  -9.93523172e+03\n",
      "    3.29862388e+03   5.93549458e+03   4.22000000e+02   7.80000000e+01\n",
      "    2.50000000e+01   2.60000000e+01   3.00000000e+00   1.20000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]\n",
      " [  8.00000000e+01   5.00000000e+00   0.00000000e+00   4.24700000e+02\n",
      "    1.93045455e+01  -5.55286688e+04   3.66578773e+03  -1.21680839e+02\n",
      "    5.69616021e+03   1.43615864e+03   5.49000000e+02   7.20000000e+01\n",
      "    3.00000000e+01   1.50000000e+01   2.00000000e+00   2.32000000e+02\n",
      "    1.00000000e+01   1.00000000e+00]\n",
      " [  9.90000000e+01   1.00000000e+00   0.00000000e+00   2.91030000e+03\n",
      "    1.90215686e+01   7.38786693e+04  -7.54669860e+03  -5.38295572e+03\n",
      "   -3.31725544e+04   1.29374034e+04   1.52000000e+03   3.91000000e+02\n",
      "    1.10000000e+02   1.03000000e+02   4.00000000e+00   5.50000000e+01\n",
      "    6.00000000e+00   6.00000000e+00]\n",
      " [  8.10000000e+01   6.00000000e+00   0.00000000e+00   4.70000000e+01\n",
      "    9.40000000e+00  -6.52416963e+04   3.02562811e+03   3.99735138e+03\n",
      "    9.04914780e+03  -1.38483728e+03   1.07000000e+02   1.10000000e+01\n",
      "    3.00000000e+00   0.00000000e+00   2.00000000e+00   1.57000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]]\n",
      "(102680, 18)\n",
      "[[  1.70000000e+01   3.00000000e+00   0.00000000e+00   3.27000000e+02\n",
      "    2.97272727e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   7.28000000e+02   7.40000000e+01\n",
      "    2.30000000e+01   2.90000000e+01   2.00000000e+00   1.06000000e+02\n",
      "    1.00000000e+00   3.00000000e+00]\n",
      " [  1.09000000e+02   4.00000000e+00   0.00000000e+00   1.27000000e+02\n",
      "    1.81428571e+01  -6.11945684e+04   1.66019762e+03   2.40659554e+02\n",
      "    7.81203839e+03   3.14309459e+02   1.23000000e+02   1.90000000e+01\n",
      "    3.00000000e+00   1.00000000e+00   3.00000000e+00   1.24000000e+02\n",
      "    8.00000000e+00   4.00000000e+00]\n",
      " [  9.70000000e+01   4.00000000e+00   0.00000000e+00   2.49000000e+02\n",
      "    1.55625000e+01  -5.73741656e+04  -3.33111579e+03   2.33390366e+03\n",
      "    3.77453856e+03  -7.61532298e+02   2.34000000e+02   6.60000000e+01\n",
      "    1.80000000e+01   8.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.00000000e+00]\n",
      " [  3.50000000e+01   6.00000000e+00   0.00000000e+00   6.00000000e+00\n",
      "    6.00000000e+00  -6.42277681e+04   2.78168341e+03   2.42397866e+03\n",
      "    6.50884795e+03   3.59594688e+02   5.50000000e+01   0.00000000e+00\n",
      "    6.00000000e+00   2.00000000e+00   2.00000000e+00   1.79000000e+02\n",
      "    7.00000000e+00   1.00000000e+00]\n",
      " [  4.00000000e+01   2.00000000e+00   0.00000000e+00   4.65000000e+02\n",
      "    2.90625000e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   1.00100000e+03   1.08000000e+02\n",
      "    4.70000000e+01   4.10000000e+01   1.00000000e+00   1.50000000e+02\n",
      "    0.00000000e+00   6.00000000e+00]\n",
      " [  1.12000000e+02   2.00000000e+00   0.00000000e+00   1.70240000e+03\n",
      "    1.89155556e+01   1.95232410e+04  -4.96296783e+03   1.37424953e+03\n",
      "   -1.60827084e+04  -3.46889739e+03   9.90000000e+02   4.50000000e+02\n",
      "    1.14000000e+02   7.90000000e+01   4.00000000e+00   4.20000000e+01\n",
      "    8.00000000e+00   3.00000000e+00]\n",
      " [  1.05000000e+02   4.00000000e+00   0.00000000e+00   1.55100000e+02\n",
      "    2.58500000e+01  -4.91075399e+04   2.69284708e+03  -9.93523172e+03\n",
      "    3.29862388e+03   5.93549458e+03   4.22000000e+02   7.80000000e+01\n",
      "    2.50000000e+01   2.60000000e+01   3.00000000e+00   1.20000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]\n",
      " [  8.00000000e+01   5.00000000e+00   0.00000000e+00   4.24700000e+02\n",
      "    1.93045455e+01  -5.55286688e+04   3.66578773e+03  -1.21680839e+02\n",
      "    5.69616021e+03   1.43615864e+03   5.49000000e+02   7.20000000e+01\n",
      "    3.00000000e+01   1.50000000e+01   2.00000000e+00   2.32000000e+02\n",
      "    1.00000000e+01   1.00000000e+00]\n",
      " [  9.90000000e+01   1.00000000e+00   0.00000000e+00   2.91030000e+03\n",
      "    1.90215686e+01   7.38786693e+04  -7.54669860e+03  -5.38295572e+03\n",
      "   -3.31725544e+04   1.29374034e+04   1.52000000e+03   3.91000000e+02\n",
      "    1.10000000e+02   1.03000000e+02   4.00000000e+00   5.50000000e+01\n",
      "    6.00000000e+00   6.00000000e+00]\n",
      " [  8.10000000e+01   6.00000000e+00   0.00000000e+00   4.70000000e+01\n",
      "    9.40000000e+00  -6.52416963e+04   3.02562811e+03   3.99735138e+03\n",
      "    9.04914780e+03  -1.38483728e+03   1.07000000e+02   1.10000000e+01\n",
      "    3.00000000e+00   0.00000000e+00   2.00000000e+00   1.57000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]]\n",
      "(102680, 18)\n",
      "[[  1.70000000e+01   3.00000000e+00   0.00000000e+00   3.27000000e+02\n",
      "    2.97272727e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   7.28000000e+02   7.40000000e+01\n",
      "    2.30000000e+01   2.90000000e+01   2.00000000e+00   1.06000000e+02\n",
      "    1.00000000e+00   3.00000000e+00]\n",
      " [  1.09000000e+02   4.00000000e+00   0.00000000e+00   1.27000000e+02\n",
      "    1.81428571e+01  -6.11945684e+04   1.66019762e+03   2.40659554e+02\n",
      "    7.81203839e+03   3.14309459e+02   1.23000000e+02   1.90000000e+01\n",
      "    3.00000000e+00   1.00000000e+00   3.00000000e+00   1.24000000e+02\n",
      "    8.00000000e+00   4.00000000e+00]\n",
      " [  9.70000000e+01   4.00000000e+00   0.00000000e+00   2.49000000e+02\n",
      "    1.55625000e+01  -5.73741656e+04  -3.33111579e+03   2.33390366e+03\n",
      "    3.77453856e+03  -7.61532298e+02   2.34000000e+02   6.60000000e+01\n",
      "    1.80000000e+01   8.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.00000000e+00]\n",
      " [  3.50000000e+01   6.00000000e+00   0.00000000e+00   6.00000000e+00\n",
      "    6.00000000e+00  -6.42277681e+04   2.78168341e+03   2.42397866e+03\n",
      "    6.50884795e+03   3.59594688e+02   5.50000000e+01   0.00000000e+00\n",
      "    6.00000000e+00   2.00000000e+00   2.00000000e+00   1.79000000e+02\n",
      "    7.00000000e+00   1.00000000e+00]\n",
      " [  4.00000000e+01   2.00000000e+00   0.00000000e+00   4.65000000e+02\n",
      "    2.90625000e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   1.00100000e+03   1.08000000e+02\n",
      "    4.70000000e+01   4.10000000e+01   1.00000000e+00   1.50000000e+02\n",
      "    0.00000000e+00   6.00000000e+00]\n",
      " [  1.12000000e+02   2.00000000e+00   0.00000000e+00   1.70240000e+03\n",
      "    1.89155556e+01   1.95232410e+04  -4.96296783e+03   1.37424953e+03\n",
      "   -1.60827084e+04  -3.46889739e+03   9.90000000e+02   4.50000000e+02\n",
      "    1.14000000e+02   7.90000000e+01   4.00000000e+00   4.20000000e+01\n",
      "    8.00000000e+00   3.00000000e+00]\n",
      " [  1.05000000e+02   4.00000000e+00   0.00000000e+00   1.55100000e+02\n",
      "    2.58500000e+01  -4.91075399e+04   2.69284708e+03  -9.93523172e+03\n",
      "    3.29862388e+03   5.93549458e+03   4.22000000e+02   7.80000000e+01\n",
      "    2.50000000e+01   2.60000000e+01   3.00000000e+00   1.20000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]\n",
      " [  8.00000000e+01   5.00000000e+00   0.00000000e+00   4.24700000e+02\n",
      "    1.93045455e+01  -5.55286688e+04   3.66578773e+03  -1.21680839e+02\n",
      "    5.69616021e+03   1.43615864e+03   5.49000000e+02   7.20000000e+01\n",
      "    3.00000000e+01   1.50000000e+01   2.00000000e+00   2.32000000e+02\n",
      "    1.00000000e+01   1.00000000e+00]\n",
      " [  9.90000000e+01   1.00000000e+00   0.00000000e+00   2.91030000e+03\n",
      "    1.90215686e+01   7.38786693e+04  -7.54669860e+03  -5.38295572e+03\n",
      "   -3.31725544e+04   1.29374034e+04   1.52000000e+03   3.91000000e+02\n",
      "    1.10000000e+02   1.03000000e+02   4.00000000e+00   5.50000000e+01\n",
      "    6.00000000e+00   6.00000000e+00]\n",
      " [  8.10000000e+01   6.00000000e+00   0.00000000e+00   4.70000000e+01\n",
      "    9.40000000e+00  -6.52416963e+04   3.02562811e+03   3.99735138e+03\n",
      "    9.04914780e+03  -1.38483728e+03   1.07000000e+02   1.10000000e+01\n",
      "    3.00000000e+00   0.00000000e+00   2.00000000e+00   1.57000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]]\n",
      "(102680, 18)\n",
      "[[  1.70000000e+01   3.00000000e+00   0.00000000e+00   3.27000000e+02\n",
      "    2.97272727e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   7.28000000e+02   7.40000000e+01\n",
      "    2.30000000e+01   2.90000000e+01   2.00000000e+00   1.06000000e+02\n",
      "    1.00000000e+00   3.00000000e+00]\n",
      " [  1.09000000e+02   4.00000000e+00   0.00000000e+00   1.27000000e+02\n",
      "    1.81428571e+01  -6.11945684e+04   1.66019762e+03   2.40659554e+02\n",
      "    7.81203839e+03   3.14309459e+02   1.23000000e+02   1.90000000e+01\n",
      "    3.00000000e+00   1.00000000e+00   3.00000000e+00   1.24000000e+02\n",
      "    8.00000000e+00   4.00000000e+00]\n",
      " [  9.70000000e+01   4.00000000e+00   0.00000000e+00   2.49000000e+02\n",
      "    1.55625000e+01  -5.73741656e+04  -3.33111579e+03   2.33390366e+03\n",
      "    3.77453856e+03  -7.61532298e+02   2.34000000e+02   6.60000000e+01\n",
      "    1.80000000e+01   8.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.00000000e+00]\n",
      " [  3.50000000e+01   6.00000000e+00   0.00000000e+00   6.00000000e+00\n",
      "    6.00000000e+00  -6.42277681e+04   2.78168341e+03   2.42397866e+03\n",
      "    6.50884795e+03   3.59594688e+02   5.50000000e+01   0.00000000e+00\n",
      "    6.00000000e+00   2.00000000e+00   2.00000000e+00   1.79000000e+02\n",
      "    7.00000000e+00   1.00000000e+00]\n",
      " [  4.00000000e+01   2.00000000e+00   0.00000000e+00   4.65000000e+02\n",
      "    2.90625000e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   1.00100000e+03   1.08000000e+02\n",
      "    4.70000000e+01   4.10000000e+01   1.00000000e+00   1.50000000e+02\n",
      "    0.00000000e+00   6.00000000e+00]\n",
      " [  1.12000000e+02   2.00000000e+00   0.00000000e+00   1.70240000e+03\n",
      "    1.89155556e+01   1.95232410e+04  -4.96296783e+03   1.37424953e+03\n",
      "   -1.60827084e+04  -3.46889739e+03   9.90000000e+02   4.50000000e+02\n",
      "    1.14000000e+02   7.90000000e+01   4.00000000e+00   4.20000000e+01\n",
      "    8.00000000e+00   3.00000000e+00]\n",
      " [  1.05000000e+02   4.00000000e+00   0.00000000e+00   1.55100000e+02\n",
      "    2.58500000e+01  -4.91075399e+04   2.69284708e+03  -9.93523172e+03\n",
      "    3.29862388e+03   5.93549458e+03   4.22000000e+02   7.80000000e+01\n",
      "    2.50000000e+01   2.60000000e+01   3.00000000e+00   1.20000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]\n",
      " [  8.00000000e+01   5.00000000e+00   0.00000000e+00   4.24700000e+02\n",
      "    1.93045455e+01  -5.55286688e+04   3.66578773e+03  -1.21680839e+02\n",
      "    5.69616021e+03   1.43615864e+03   5.49000000e+02   7.20000000e+01\n",
      "    3.00000000e+01   1.50000000e+01   2.00000000e+00   2.32000000e+02\n",
      "    1.00000000e+01   1.00000000e+00]\n",
      " [  9.90000000e+01   1.00000000e+00   0.00000000e+00   2.91030000e+03\n",
      "    1.90215686e+01   7.38786693e+04  -7.54669860e+03  -5.38295572e+03\n",
      "   -3.31725544e+04   1.29374034e+04   1.52000000e+03   3.91000000e+02\n",
      "    1.10000000e+02   1.03000000e+02   4.00000000e+00   5.50000000e+01\n",
      "    6.00000000e+00   6.00000000e+00]\n",
      " [  8.10000000e+01   6.00000000e+00   0.00000000e+00   4.70000000e+01\n",
      "    9.40000000e+00  -6.52416963e+04   3.02562811e+03   3.99735138e+03\n",
      "    9.04914780e+03  -1.38483728e+03   1.07000000e+02   1.10000000e+01\n",
      "    3.00000000e+00   0.00000000e+00   2.00000000e+00   1.57000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]]\n",
      "(102680, 18)\n",
      "[[  1.70000000e+01   3.00000000e+00   0.00000000e+00   3.27000000e+02\n",
      "    2.97272727e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   7.28000000e+02   7.40000000e+01\n",
      "    2.30000000e+01   2.90000000e+01   2.00000000e+00   1.06000000e+02\n",
      "    1.00000000e+00   3.00000000e+00]\n",
      " [  1.09000000e+02   4.00000000e+00   0.00000000e+00   1.27000000e+02\n",
      "    1.81428571e+01  -6.11945684e+04   1.66019762e+03   2.40659554e+02\n",
      "    7.81203839e+03   3.14309459e+02   1.23000000e+02   1.90000000e+01\n",
      "    3.00000000e+00   1.00000000e+00   3.00000000e+00   1.24000000e+02\n",
      "    8.00000000e+00   4.00000000e+00]\n",
      " [  9.70000000e+01   4.00000000e+00   0.00000000e+00   2.49000000e+02\n",
      "    1.55625000e+01  -5.73741656e+04  -3.33111579e+03   2.33390366e+03\n",
      "    3.77453856e+03  -7.61532298e+02   2.34000000e+02   6.60000000e+01\n",
      "    1.80000000e+01   8.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.00000000e+00]\n",
      " [  3.50000000e+01   6.00000000e+00   0.00000000e+00   6.00000000e+00\n",
      "    6.00000000e+00  -6.42277681e+04   2.78168341e+03   2.42397866e+03\n",
      "    6.50884795e+03   3.59594688e+02   5.50000000e+01   0.00000000e+00\n",
      "    6.00000000e+00   2.00000000e+00   2.00000000e+00   1.79000000e+02\n",
      "    7.00000000e+00   1.00000000e+00]\n",
      " [  4.00000000e+01   2.00000000e+00   0.00000000e+00   4.65000000e+02\n",
      "    2.90625000e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   1.00100000e+03   1.08000000e+02\n",
      "    4.70000000e+01   4.10000000e+01   1.00000000e+00   1.50000000e+02\n",
      "    0.00000000e+00   6.00000000e+00]\n",
      " [  1.12000000e+02   2.00000000e+00   0.00000000e+00   1.70240000e+03\n",
      "    1.89155556e+01   1.95232410e+04  -4.96296783e+03   1.37424953e+03\n",
      "   -1.60827084e+04  -3.46889739e+03   9.90000000e+02   4.50000000e+02\n",
      "    1.14000000e+02   7.90000000e+01   4.00000000e+00   4.20000000e+01\n",
      "    8.00000000e+00   3.00000000e+00]\n",
      " [  1.05000000e+02   4.00000000e+00   0.00000000e+00   1.55100000e+02\n",
      "    2.58500000e+01  -4.91075399e+04   2.69284708e+03  -9.93523172e+03\n",
      "    3.29862388e+03   5.93549458e+03   4.22000000e+02   7.80000000e+01\n",
      "    2.50000000e+01   2.60000000e+01   3.00000000e+00   1.20000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]\n",
      " [  8.00000000e+01   5.00000000e+00   0.00000000e+00   4.24700000e+02\n",
      "    1.93045455e+01  -5.55286688e+04   3.66578773e+03  -1.21680839e+02\n",
      "    5.69616021e+03   1.43615864e+03   5.49000000e+02   7.20000000e+01\n",
      "    3.00000000e+01   1.50000000e+01   2.00000000e+00   2.32000000e+02\n",
      "    1.00000000e+01   1.00000000e+00]\n",
      " [  9.90000000e+01   1.00000000e+00   0.00000000e+00   2.91030000e+03\n",
      "    1.90215686e+01   7.38786693e+04  -7.54669860e+03  -5.38295572e+03\n",
      "   -3.31725544e+04   1.29374034e+04   1.52000000e+03   3.91000000e+02\n",
      "    1.10000000e+02   1.03000000e+02   4.00000000e+00   5.50000000e+01\n",
      "    6.00000000e+00   6.00000000e+00]\n",
      " [  8.10000000e+01   6.00000000e+00   0.00000000e+00   4.70000000e+01\n",
      "    9.40000000e+00  -6.52416963e+04   3.02562811e+03   3.99735138e+03\n",
      "    9.04914780e+03  -1.38483728e+03   1.07000000e+02   1.10000000e+01\n",
      "    3.00000000e+00   0.00000000e+00   2.00000000e+00   1.57000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]]\n",
      "(102680, 18)\n",
      "[[  1.70000000e+01   3.00000000e+00   0.00000000e+00   3.27000000e+02\n",
      "    2.97272727e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   7.28000000e+02   7.40000000e+01\n",
      "    2.30000000e+01   2.90000000e+01   2.00000000e+00   1.06000000e+02\n",
      "    1.00000000e+00   3.00000000e+00]\n",
      " [  1.09000000e+02   4.00000000e+00   0.00000000e+00   1.27000000e+02\n",
      "    1.81428571e+01  -6.11945684e+04   1.66019762e+03   2.40659554e+02\n",
      "    7.81203839e+03   3.14309459e+02   1.23000000e+02   1.90000000e+01\n",
      "    3.00000000e+00   1.00000000e+00   3.00000000e+00   1.24000000e+02\n",
      "    8.00000000e+00   4.00000000e+00]\n",
      " [  9.70000000e+01   4.00000000e+00   0.00000000e+00   2.49000000e+02\n",
      "    1.55625000e+01  -5.73741656e+04  -3.33111579e+03   2.33390366e+03\n",
      "    3.77453856e+03  -7.61532298e+02   2.34000000e+02   6.60000000e+01\n",
      "    1.80000000e+01   8.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.00000000e+00]\n",
      " [  3.50000000e+01   6.00000000e+00   0.00000000e+00   6.00000000e+00\n",
      "    6.00000000e+00  -6.42277681e+04   2.78168341e+03   2.42397866e+03\n",
      "    6.50884795e+03   3.59594688e+02   5.50000000e+01   0.00000000e+00\n",
      "    6.00000000e+00   2.00000000e+00   2.00000000e+00   1.79000000e+02\n",
      "    7.00000000e+00   1.00000000e+00]\n",
      " [  4.00000000e+01   2.00000000e+00   0.00000000e+00   4.65000000e+02\n",
      "    2.90625000e+01   1.87671617e+04  -2.45115880e+03  -1.62980717e+04\n",
      "   -2.21621266e+04  -1.19145310e+03   1.00100000e+03   1.08000000e+02\n",
      "    4.70000000e+01   4.10000000e+01   1.00000000e+00   1.50000000e+02\n",
      "    0.00000000e+00   6.00000000e+00]\n",
      " [  1.12000000e+02   2.00000000e+00   0.00000000e+00   1.70240000e+03\n",
      "    1.89155556e+01   1.95232410e+04  -4.96296783e+03   1.37424953e+03\n",
      "   -1.60827084e+04  -3.46889739e+03   9.90000000e+02   4.50000000e+02\n",
      "    1.14000000e+02   7.90000000e+01   4.00000000e+00   4.20000000e+01\n",
      "    8.00000000e+00   3.00000000e+00]\n",
      " [  1.05000000e+02   4.00000000e+00   0.00000000e+00   1.55100000e+02\n",
      "    2.58500000e+01  -4.91075399e+04   2.69284708e+03  -9.93523172e+03\n",
      "    3.29862388e+03   5.93549458e+03   4.22000000e+02   7.80000000e+01\n",
      "    2.50000000e+01   2.60000000e+01   3.00000000e+00   1.20000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]\n",
      " [  8.00000000e+01   5.00000000e+00   0.00000000e+00   4.24700000e+02\n",
      "    1.93045455e+01  -5.55286688e+04   3.66578773e+03  -1.21680839e+02\n",
      "    5.69616021e+03   1.43615864e+03   5.49000000e+02   7.20000000e+01\n",
      "    3.00000000e+01   1.50000000e+01   2.00000000e+00   2.32000000e+02\n",
      "    1.00000000e+01   1.00000000e+00]\n",
      " [  9.90000000e+01   1.00000000e+00   0.00000000e+00   2.91030000e+03\n",
      "    1.90215686e+01   7.38786693e+04  -7.54669860e+03  -5.38295572e+03\n",
      "   -3.31725544e+04   1.29374034e+04   1.52000000e+03   3.91000000e+02\n",
      "    1.10000000e+02   1.03000000e+02   4.00000000e+00   5.50000000e+01\n",
      "    6.00000000e+00   6.00000000e+00]\n",
      " [  8.10000000e+01   6.00000000e+00   0.00000000e+00   4.70000000e+01\n",
      "    9.40000000e+00  -6.52416963e+04   3.02562811e+03   3.99735138e+03\n",
      "    9.04914780e+03  -1.38483728e+03   1.07000000e+02   1.10000000e+01\n",
      "    3.00000000e+00   0.00000000e+00   2.00000000e+00   1.57000000e+02\n",
      "    9.00000000e+00   2.00000000e+00]]\n",
      "2.53971690622\n",
      "2.53971690622\n",
      "2.53971690622\n"
     ]
    }
   ],
   "source": [
    "# Just testing Imputer. Turns out somehow Imputer causes number of features reduced, weird.\n",
    "\n",
    "# imputer = Imputer()\n",
    "est = DecisionTreeRegressor(max_features=len(features))\n",
    "\n",
    "data_train_i = np.copy(data_train)\n",
    "print(data_train.shape)\n",
    "print(data_train[0:10])\n",
    "# data_train_i = imputer.fit_transform(data_train)\n",
    "data_train_i[np.isnan(data_train_i)] = 0\n",
    "data_train_i.astype('float32')\n",
    "print(data_train_i.shape)\n",
    "print(data_train_i[0:10])\n",
    "est.fit(data_train_i, targets_train)\n",
    "predictions = est.predict(data_test)\n",
    "print(mape(data_test, predictions, targets_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "RandomForestRegressor: Returned only one value for all predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
