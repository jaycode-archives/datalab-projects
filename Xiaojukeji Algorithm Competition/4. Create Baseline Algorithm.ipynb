{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.pipeline import Pipeline\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME = 'baseline_final_estimator.pkl'\n",
    "\n",
    "# First feature HAS to be 'district_id' for MAPE calculation.\n",
    "fields_str = \"\"\"\n",
    "district_id\ttimeofday_slot\tday_in_week\tis_sunday\tsum_price\tavg_price\tpoi1\tpoi2\tpoi3\n",
    "poi4\tpoi5\ttraffic_tj_level1\ttraffic_tj_level2\ttraffic_tj_level3\ttraffic_tj_level4\n",
    "weather\tweather_pm25\tweather_temperature\tgap\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mape(X, predictions, y):\n",
    "  num_timeslots = 43\n",
    "  num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  Xy = np.concatenate((X, y.T, predictions.T), axis=1)\n",
    "  districts = np.unique(X[:,0])\n",
    "  district_scores = np.zeros(len(districts))\n",
    "  for counter, key in enumerate(districts):\n",
    "    group = np.compress((Xy[:,0] == key).flat, Xy, axis=0)\n",
    "    district_scores[counter] = np.sum(np.absolute(\n",
    "        (group[:,-2] -\n",
    "         group[:,-1])/\n",
    "        group[:,-2]\n",
    "      )) / num_timeslots\n",
    "  return np.sum(district_scores) / num_districts\n",
    "\n",
    "def mape_scorer(estimator, X, y):\n",
    "  predictions = estimator.predict(X)\n",
    "  return -mape(X, predictions, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "est = LogisticRegression()\n",
    "X = np.array([[1, 1], [1, 2], [2, 3], [2, 4]])\n",
    "predictions = np.array([1, 2, 3, 4])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "\n",
    "# Should return 0.0\n",
    "mape(X, predictions, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.future_gaps_processed]\n",
    "WHERE gap > 0\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = bq.Query(q_all)\n",
    "tableresult = query.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 102680 rows\n",
      "processed 0 rows\n",
      "processed 5000 rows\n",
      "processed 10000 rows\n",
      "processed 15000 rows\n",
      "processed 20000 rows\n",
      "processed 25000 rows\n",
      "processed 30000 rows\n",
      "processed 35000 rows\n",
      "processed 40000 rows\n",
      "processed 45000 rows\n",
      "processed 50000 rows\n",
      "processed 55000 rows\n",
      "processed 60000 rows\n",
      "processed 65000 rows\n",
      "processed 70000 rows\n",
      "processed 75000 rows\n",
      "processed 80000 rows\n",
      "processed 85000 rows\n",
      "processed 90000 rows\n",
      "processed 95000 rows\n",
      "processed 100000 rows\n"
     ]
    }
   ],
   "source": [
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2016-01-22-46','2016-01-22-58','2016-01-22-70','2016-01-22-82','2016-01-22-94','2016-01-22-106','2016-01-22-118','2016-01-22-130','2016-01-22-142','2016-01-24-58','2016-01-24-70','2016-01-24-82','2016-01-24-94','2016-01-24-106','2016-01-24-118','2016-01-24-130','2016-01-24-142','2016-01-26-46','2016-01-26-58','2016-01-26-70','2016-01-26-82','2016-01-26-94','2016-01-26-106','2016-01-26-118','2016-01-26-130','2016-01-26-142','2016-01-28-58','2016-01-28-70','2016-01-28-82','2016-01-28-94','2016-01-28-106','2016-01-28-118','2016-01-28-130','2016-01-28-142','2016-01-30-46','2016-01-30-58','2016-01-30-70','2016-01-30-82','2016-01-30-94','2016-01-30-106','2016-01-30-118','2016-01-30-130','2016-01-30-142'\n"
     ]
    }
   ],
   "source": [
    "# Get timeslots to test from GCS\n",
    "item = storage.Item('datalab-projects-1331-datalab','data/timeslots_to_test.txt')\n",
    "timeslots_to_test = item.read_from().strip().split('\\n')\n",
    "tquery = ','.join(map(lambda x: \"'{}'\".format(x), timeslots_to_test))\n",
    "print(tquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all_t\n",
    "\n",
    "SELECT *\n",
    "FROM [datalab-projects-1331:xjk_algo_comp_test.future_gaps_processed]\n",
    "WHERE gap > 0 AND timeslot NOT IN ('2016-01-22-46','2016-01-22-58','2016-01-22-70','2016-01-22-82',\n",
    "    '2016-01-22-94','2016-01-22-106','2016-01-22-118','2016-01-22-130','2016-01-22-142',\n",
    "    '2016-01-24-58','2016-01-24-70','2016-01-24-82','2016-01-24-94','2016-01-24-106',\n",
    "    '2016-01-24-118','2016-01-24-130','2016-01-24-142','2016-01-26-46','2016-01-26-58',\n",
    "    '2016-01-26-70','2016-01-26-82','2016-01-26-94','2016-01-26-106','2016-01-26-118',\n",
    "    '2016-01-26-130','2016-01-26-142','2016-01-28-58','2016-01-28-70','2016-01-28-82',\n",
    "    '2016-01-28-94','2016-01-28-106','2016-01-28-118','2016-01-28-130','2016-01-28-142',\n",
    "    '2016-01-30-46','2016-01-30-58','2016-01-30-70','2016-01-30-82','2016-01-30-94',\n",
    "    '2016-01-30-106','2016-01-30-118','2016-01-30-130','2016-01-30-142')\n",
    "ORDER BY timeslot, district_id\n",
    "\n",
    "# Test dataset - used to check if estimator can generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_t = bq.Query(q_all_t)\n",
    "tableresult_t = query_t.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 3509 rows\n",
      "processed 0 rows\n",
      "processed 1000 rows\n",
      "processed 2000 rows\n",
      "processed 3000 rows\n"
     ]
    }
   ],
   "source": [
    "all_data_t = np.zeros((tableresult_t.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult_t.length)\n",
    "for rcounter, row in enumerate(tableresult_t):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data_t[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 1000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Testing Algorithm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([   105,    105,    105, ..., 102673, 102673, 102673]), array([ 3,  4,  5, ...,  4,  5, 75]))\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64))\n",
      "np.max= nan\n"
     ]
    }
   ],
   "source": [
    "# Useful code to check NaN and Inf values. This is needed since these values would\n",
    "# cause \"Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "# errors when left unchecked.\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data))\n",
    "print \"np.max=\", np.max(abs(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data[np.isnan(all_data)] = 0\n",
    "all_data_t[np.isnan(all_data_t)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([], dtype=int64), array([], dtype=int64))\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64))\n",
      "np.max= 461563.0\n"
     ]
    }
   ],
   "source": [
    "# Useful code to check NaN and Inf values. This is needed since these values would\n",
    "# cause \"Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "# errors when left unchecked.\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data))\n",
    "print \"np.max=\", np.max(abs(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 87 candidates, totalling 261 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   1 jobs       | elapsed:    1.4s\n",
      "[Parallel(n_jobs=2)]: Done  50 jobs       | elapsed:   39.8s\n",
      "[Parallel(n_jobs=2)]: Done 200 jobs       | elapsed:  5.8min\n",
      "[Parallel(n_jobs=2)]: Done 259 out of 261 | elapsed:  9.2min remaining:    4.3s\n",
      "[Parallel(n_jobs=2)]: Done 261 out of 261 | elapsed:  9.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: -14.93978, std: 0.25863, params: {'estimate__max_features': 1}, mean: -14.31949, std: 0.34719, params: {'estimate__max_features': 2}, mean: -14.48133, std: 0.43900, params: {'estimate__max_features': 3}, mean: -15.19553, std: 0.12873, params: {'estimate__max_features': 4}, mean: -14.76063, std: 0.48873, params: {'estimate__max_features': 5}, mean: -14.42416, std: 0.70852, params: {'estimate__max_features': 6}, mean: -14.48021, std: 0.58702, params: {'estimate__max_features': 7}, mean: -14.66079, std: 0.56975, params: {'estimate__max_features': 8}, mean: -14.55552, std: 0.73364, params: {'estimate__max_features': 9}, mean: -14.54305, std: 0.93113, params: {'estimate__max_features': 10}, mean: -14.33652, std: 0.16920, params: {'estimate__max_features': 11}, mean: -13.96776, std: 0.93149, params: {'estimate__max_features': 12}, mean: -13.88167, std: 0.39308, params: {'estimate__max_features': 13}, mean: -14.06325, std: 0.19752, params: {'estimate__max_features': 14}, mean: -13.36123, std: 0.21235, params: {'estimate__max_features': 15}, mean: -13.59085, std: 0.03823, params: {'estimate__max_features': 16}, mean: -13.24105, std: 0.63039, params: {'estimate__max_features': 17}, mean: -12.73823, std: 0.35542, params: {'estimate__max_features': 18}, mean: -13.46449, std: 0.07830, params: {'estimate__max_features': 19}, mean: -13.15824, std: 0.23538, params: {'estimate__max_features': 20}, mean: -13.04030, std: 0.05894, params: {'estimate__max_features': 21}, mean: -12.94707, std: 0.44495, params: {'estimate__max_features': 22}, mean: -13.28878, std: 0.33138, params: {'estimate__max_features': 23}, mean: -12.83697, std: 0.30767, params: {'estimate__max_features': 24}, mean: -12.75117, std: 0.15762, params: {'estimate__max_features': 25}, mean: -12.44132, std: 0.10279, params: {'estimate__max_features': 26}, mean: -12.45406, std: 0.13415, params: {'estimate__max_features': 27}, mean: -13.00167, std: 0.33480, params: {'estimate__max_features': 28}, mean: -12.40452, std: 0.19403, params: {'estimate__max_features': 29}, mean: -12.76583, std: 0.00891, params: {'estimate__max_features': 30}, mean: -12.71312, std: 0.11398, params: {'estimate__max_features': 31}, mean: -12.41765, std: 0.29346, params: {'estimate__max_features': 32}, mean: -12.63350, std: 0.01524, params: {'estimate__max_features': 33}, mean: -12.67996, std: 0.09901, params: {'estimate__max_features': 34}, mean: -12.48627, std: 0.17436, params: {'estimate__max_features': 35}, mean: -12.43090, std: 0.21992, params: {'estimate__max_features': 36}, mean: -12.58280, std: 0.18717, params: {'estimate__max_features': 37}, mean: -12.52165, std: 0.05887, params: {'estimate__max_features': 38}, mean: -12.57914, std: 0.13621, params: {'estimate__max_features': 39}, mean: -12.27020, std: 0.34650, params: {'estimate__max_features': 40}, mean: -12.50130, std: 0.12680, params: {'estimate__max_features': 41}, mean: -12.52409, std: 0.11211, params: {'estimate__max_features': 42}, mean: -12.46225, std: 0.27220, params: {'estimate__max_features': 43}, mean: -12.40444, std: 0.12702, params: {'estimate__max_features': 44}, mean: -12.44555, std: 0.09500, params: {'estimate__max_features': 45}, mean: -12.24058, std: 0.14573, params: {'estimate__max_features': 46}, mean: -12.58198, std: 0.44624, params: {'estimate__max_features': 47}, mean: -12.37773, std: 0.17472, params: {'estimate__max_features': 48}, mean: -12.41161, std: 0.26936, params: {'estimate__max_features': 49}, mean: -12.45034, std: 0.27919, params: {'estimate__max_features': 50}, mean: -12.41566, std: 0.28133, params: {'estimate__max_features': 51}, mean: -12.48323, std: 0.16659, params: {'estimate__max_features': 52}, mean: -12.47282, std: 0.25527, params: {'estimate__max_features': 53}, mean: -12.48496, std: 0.10720, params: {'estimate__max_features': 54}, mean: -12.57998, std: 0.18167, params: {'estimate__max_features': 55}, mean: -12.41958, std: 0.08139, params: {'estimate__max_features': 56}, mean: -12.37755, std: 0.38086, params: {'estimate__max_features': 57}, mean: -12.62811, std: 0.21104, params: {'estimate__max_features': 58}, mean: -12.55513, std: 0.22677, params: {'estimate__max_features': 59}, mean: -12.46952, std: 0.16176, params: {'estimate__max_features': 60}, mean: -12.50183, std: 0.16709, params: {'estimate__max_features': 61}, mean: -12.62991, std: 0.10826, params: {'estimate__max_features': 62}, mean: -12.47836, std: 0.17717, params: {'estimate__max_features': 63}, mean: -12.57291, std: 0.25764, params: {'estimate__max_features': 64}, mean: -12.54127, std: 0.06591, params: {'estimate__max_features': 65}, mean: -12.50201, std: 0.20663, params: {'estimate__max_features': 66}, mean: -12.52410, std: 0.13665, params: {'estimate__max_features': 67}, mean: -12.67948, std: 0.24782, params: {'estimate__max_features': 68}, mean: -12.49065, std: 0.12432, params: {'estimate__max_features': 69}, mean: -12.41892, std: 0.12707, params: {'estimate__max_features': 70}, mean: -12.62612, std: 0.25152, params: {'estimate__max_features': 71}, mean: -12.63906, std: 0.16241, params: {'estimate__max_features': 72}, mean: -12.67783, std: 0.18056, params: {'estimate__max_features': 73}, mean: -12.52456, std: 0.09406, params: {'estimate__max_features': 74}, mean: -12.69995, std: 0.10581, params: {'estimate__max_features': 75}, mean: -12.51735, std: 0.06081, params: {'estimate__max_features': 76}, mean: -12.53081, std: 0.07912, params: {'estimate__max_features': 77}, mean: -12.62341, std: 0.01843, params: {'estimate__max_features': 78}, mean: -12.55735, std: 0.15260, params: {'estimate__max_features': 79}, mean: -12.55807, std: 0.09242, params: {'estimate__max_features': 80}, mean: -12.55580, std: 0.03307, params: {'estimate__max_features': 81}, mean: -12.54221, std: 0.17005, params: {'estimate__max_features': 82}, mean: -12.47942, std: 0.21664, params: {'estimate__max_features': 83}, mean: -12.55379, std: 0.11305, params: {'estimate__max_features': 84}, mean: -12.57571, std: 0.08952, params: {'estimate__max_features': 85}, mean: -12.54421, std: 0.06783, params: {'estimate__max_features': 86}, mean: -12.55677, std: 0.06459, params: {'estimate__max_features': 87}]\n",
      "{'estimate__max_features': 46}\n",
      "-12.2405773137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=46,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='best'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "# from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "steps = [\n",
    "#   ('impute', Imputer(0)),\n",
    "#   ('feature_selection', SelectKBest(f_classif)),\n",
    "  ('estimate', DecisionTreeRegressor())\n",
    "]\n",
    "\n",
    "est = Pipeline(steps)\n",
    "\n",
    "data_train = all_data[:,1:]\n",
    "targets_train = all_data[:,0]\n",
    "data_test = all_data_t[:,1:]\n",
    "targets_test = all_data_t[:,0]\n",
    "\n",
    "params = {\n",
    "#   \"feature_selection__k\": [i for i in range(1, len(features) - 1)]\n",
    "  'estimate__max_features': [i for i in range(1, len(features))]\n",
    "}\n",
    "# cross_validation_iter = StratifiedShuffleSplit(y=targets_train, test_size=0.3,\n",
    "#                                                random_state=RANDOM_STATE, n_iter=10)\n",
    "# search_params = RandomizedSearchCV(\n",
    "#   estimator=est,\n",
    "#   param_distributions=params,\n",
    "# #   cv=10,\n",
    "#   scoring=mape_scorer,\n",
    "#   n_jobs=2,\n",
    "#   n_iter=5\n",
    "# )\n",
    "\n",
    "search_params = GridSearchCV(\n",
    "  estimator=est,\n",
    "  param_grid=params,\n",
    "#   cv=10,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=2,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "search_params.fit(data_train, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35843611201\n"
     ]
    }
   ],
   "source": [
    "final_est = search_params.best_estimator_\n",
    "test_predictions = final_est.predict(data_test)\n",
    "print(mape(data_test, test_predictions, targets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(final_est, open(EST_PICKLE_FILENAME, \"w\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"Process Final Test Data With Final Algorithm\" to use pickled final algorithm against final test data to produce csv required by this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just testing Imputer. Turns out somehow Imputer causes number of features reduced, weird.\n",
    "\n",
    "# imputer = Imputer()\n",
    "est = DecisionTreeRegressor(max_features=len(features))\n",
    "\n",
    "data_train_i = np.copy(data_train)\n",
    "print(data_train.shape)\n",
    "print(data_train[0:10])\n",
    "# data_train_i = imputer.fit_transform(data_train)\n",
    "data_train_i[np.isnan(data_train_i)] = 0\n",
    "data_train_i.astype('float32')\n",
    "print(data_train_i.shape)\n",
    "print(data_train_i[0:10])\n",
    "est.fit(data_train_i, targets_train)\n",
    "predictions = est.predict(data_test)\n",
    "print(mape(data_test, predictions, targets_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
