{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tables import *\n",
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME = 'GradientBoostingRegressor_grid_best.pkl'\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\tbusy_time\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)\n",
    "\n",
    "datafile_path = 'xjk_pytable.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created, pass...\n"
     ]
    }
   ],
   "source": [
    "fileh = open_file(datafile_path, mode = 'r')\n",
    "try:\n",
    "  fileh.root.train.gaps\n",
    "  print \"file created, pass...\"\n",
    "except:\n",
    "  fileh.close()\n",
    "  fileh = open_file(datafile_path, mode = 'w')\n",
    "  train = fileh.create_group('/', 'train', 'Training tables')\n",
    "  tabledef = {}\n",
    "  for field in fields:\n",
    "    tabledef[field] = Float64Col()\n",
    "  gaps = fileh.create_table(train, 'gaps', tabledef)\n",
    "  gaps.flush()\n",
    "fileh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ', '.join(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT gap, day_in_week, weather_1_slots_ago, weather_2_slots_ago, weather_3_slots_ago,\n",
    "  IF(timeofday_slot >= 50 AND timeofday_slot <= 53, 1, 0) AS busy_time,\n",
    "  tj_level1_1_slots_ago, tj_level2_1_slots_ago, tj_level3_1_slots_ago, tj_level4_1_slots_ago,\n",
    "  tj_level1_2_slots_ago, tj_level2_2_slots_ago, tj_level3_2_slots_ago, tj_level4_2_slots_ago,\n",
    "  tj_level1_3_slots_ago, tj_level2_3_slots_ago, tj_level3_3_slots_ago, tj_level4_3_slots_ago,\n",
    "  temperature_1_slots_ago, pm25_1_slots_ago, temperature_2_slots_ago, pm25_2_slots_ago,\n",
    "  temperature_3_slots_ago, pm25_3_slots_ago, gap_1_slots_ago, sum_price_1_slots_ago,\n",
    "  gap_2_slots_ago, sum_price_2_slots_ago, gap_3_slots_ago, sum_price_3_slots_ago, f1, f11, f11_1,\n",
    "  f11_2, f11_3, f11_4, f11_5, f11_6, f11_7, f11_8, f13_4, f13_8, f14, f14_1, f14_10, f14_2, f14_3,\n",
    "  f14_6, f14_8, f15, f15_1, f15_2, f15_3, f15_4, f15_6, f15_7, f15_8, f16, f16_1, f16_10, f16_11,\n",
    "  f16_12, f16_3, f16_4, f16_6, f17, f17_2, f17_3, f17_4, f17_5, f19, f19_1, f19_2, f19_3, f19_4,\n",
    "  f1_1, f1_10, f1_11, f1_2, f1_3, f1_4, f1_5, f1_6, f1_7, f1_8, f20, f20_1, f20_2, f20_4, f20_5,\n",
    "  f20_6, f20_7, f20_8, f20_9, f21_1, f21_2, f22, f22_1, f22_2, f22_3, f22_4, f22_5, f23, f23_1,\n",
    "  f23_2, f23_3, f23_4, f23_5, f23_6, f24, f24_1, f24_2, f24_3, f25, f25_1, f25_3, f25_7, f25_8,\n",
    "  f25_9, f2_1, f2_10, f2_11, f2_12, f2_13, f2_2, f2_4, f2_5, f2_6, f2_7, f2_8, f3_1, f3_2, f3_3,\n",
    "  f4, f4_1, f4_10, f4_11, f4_13, f4_14, f4_16, f4_17, f4_18, f4_2, f4_3, f4_5, f4_6, f4_7, f4_8,\n",
    "  f4_9, f5, f5_1, f5_3, f5_4, f6, f6_1, f6_2, f6_4, f7, f8, f8_1, f8_2, f8_3, f8_4, f8_5\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "fileh = open_file(datafile_path, mode = 'r')\n",
    "gaps_table = fileh.root.train.gaps\n",
    "\n",
    "if gaps_table.nrows == 0:\n",
    "  query = bq.Query(q_all)\n",
    "  tableresult = query.results()\n",
    "\n",
    "  gap = gaps_table.row\n",
    "  print 'there are {} rows'.format(tableresult.length)\n",
    "  for rcounter, row in enumerate(tableresult):\n",
    "    for field in fields:\n",
    "      gap[field] = row[field]\n",
    "    gap.append()\n",
    "    if rcounter % 5000 == 0:\n",
    "      print 'processed {} rows'.format(rcounter)\n",
    "  gaps_table.flush()\n",
    "      \n",
    "object = fileh.get_node('/train', 'gaps')\n",
    "all_data = object.read()\n",
    "all_data_original = np.copy(all_data)\n",
    "fileh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(fields))\n",
    "print(len(row.values()))\n",
    "print(row.keys())\n",
    "for key in row.keys():\n",
    "  print \"checking\", key\n",
    "  if key not in fields:\n",
    "    print \"does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This chunk does further wrangling to dataset to produce training and test sets.\n",
    "\n",
    "# Useful code to check NaN and Inf values. This is needed since these values would\n",
    "# cause \"Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "# errors when left unchecked.\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data_original))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data_original))\n",
    "print \"np.max=\", np.max(abs(all_data_original))\n",
    "\n",
    "# Impute all NaN with numbers (not sure what to replace inf yet)\n",
    "all_data[np.isnan(all_data_original)] = 0\n",
    "# all_data[np.isinf(all_data)] = 0\n",
    "\n",
    "# See that NaN and Inf values replaced\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data))\n",
    "print \"np.max=\", np.max(abs(all_data))\n",
    "\n",
    "# Split the data into train and test sets.\n",
    "data_size = all_data.shape[0]\n",
    "training_size = data_size * 80/100\n",
    "indices = np.random.permutation(data_size)\n",
    "training_idx, test_idx = indices[:training_size], indices[training_size:]\n",
    "all_data_train, all_data_test = all_data[training_idx,:], all_data[test_idx,:]\n",
    "\n",
    "data_train = all_data_train[:,1:]\n",
    "targets_train = all_data_train[:,0]\n",
    "data_test = all_data_test[:,1:]\n",
    "targets_test = all_data_test[:,0]\n",
    "data_train_original = np.copy(data_train)\n",
    "data_test_original = np.copy(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration - Find NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is how to get position of NaNs\n",
    "\n",
    "nulls = np.isnan(all_data_original)\n",
    "nullspos = np.column_stack(np.where(nulls==True))\n",
    "nullspos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [[np.NaN, 1, 2, 3],\n",
    "     [1, 2, 3, np.NaN]]\n",
    "xn = np.isnan(x)\n",
    "xnp = np.column_stack(np.where(xn==True))\n",
    "xnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "print \"total data points:\", (all_data_original.shape[0] * all_data_original.shape[1])\n",
    "print \"number of missing values:\", nullspos.shape[0]\n",
    "missing_features = itemgetter(*np.unique(nullspos[:,1]).tolist())(fields)\n",
    "missing_features_table = pd.DataFrame(columns=['id', 'field', 'missing data points'])\n",
    "\n",
    "for id, field in enumerate(fields):\n",
    "  total_missing = len(np.where(nullspos[:,1]==id)[0])\n",
    "  if total_missing > 0:\n",
    "    missing_features_table = missing_features_table.append({\n",
    "        'id': id,\n",
    "        'field': field,\n",
    "        'missing data points': total_missing\n",
    "      }, ignore_index=True)\n",
    "missing_features_table['missing data points'] = \\\n",
    "  missing_features_table['missing data points'].astype('int64')\n",
    "missing_features_table['id'] = \\\n",
    "  missing_features_table['id'].astype('int64')\n",
    "missing_features_table.sort_values(['missing data points', 'id'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "def rand_jitter(arr):\n",
    "    stdev = .005*(max(arr)-min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev\n",
    "_ = plt.scatter(nullspos[:,0], rand_jitter(nullspos[:,1]), s=0.5)\n",
    "_ = plt.title('Missing Data Points')\n",
    "_ = plt.ylabel('Feature ID')\n",
    "_ = plt.xlabel('Observation ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT COUNT(*) AS count FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE IS_NAN(sum_price_1_slots_ago) = true\n",
    "AND gap > 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
