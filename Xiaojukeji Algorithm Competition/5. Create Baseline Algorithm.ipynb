{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 163\n",
      "Features:\n",
      "['timeofday_slot', 'day_in_week', 'weather_1_slots_ago', 'weather_2_slots_ago', 'weather_3_slots_ago', 'tj_level1_1_slots_ago', 'tj_level2_1_slots_ago', 'tj_level3_1_slots_ago', 'tj_level4_1_slots_ago', 'tj_level1_2_slots_ago', 'tj_level2_2_slots_ago', 'tj_level3_2_slots_ago', 'tj_level4_2_slots_ago', 'tj_level1_3_slots_ago', 'tj_level2_3_slots_ago', 'tj_level3_3_slots_ago', 'tj_level4_3_slots_ago', 'temperature_1_slots_ago', 'pm25_1_slots_ago', 'temperature_2_slots_ago', 'pm25_2_slots_ago', 'temperature_3_slots_ago', 'pm25_3_slots_ago', 'gap_1_slots_ago', 'sum_price_1_slots_ago', 'gap_2_slots_ago', 'sum_price_2_slots_ago', 'gap_3_slots_ago', 'sum_price_3_slots_ago', 'f1', 'f11', 'f11_1', 'f11_2', 'f11_3', 'f11_4', 'f11_5', 'f11_6', 'f11_7', 'f11_8', 'f13_4', 'f13_8', 'f14', 'f14_1', 'f14_10', 'f14_2', 'f14_3', 'f14_6', 'f14_8', 'f15', 'f15_1', 'f15_2', 'f15_3', 'f15_4', 'f15_6', 'f15_7', 'f15_8', 'f16', 'f16_1', 'f16_10', 'f16_11', 'f16_12', 'f16_3', 'f16_4', 'f16_6', 'f17', 'f17_2', 'f17_3', 'f17_4', 'f17_5', 'f19', 'f19_1', 'f19_2', 'f19_3', 'f19_4', 'f1_1', 'f1_10', 'f1_11', 'f1_2', 'f1_3', 'f1_4', 'f1_5', 'f1_6', 'f1_7', 'f1_8', 'f20', 'f20_1', 'f20_2', 'f20_4', 'f20_5', 'f20_6', 'f20_7', 'f20_8', 'f20_9', 'f21_1', 'f21_2', 'f22', 'f22_1', 'f22_2', 'f22_3', 'f22_4', 'f22_5', 'f23', 'f23_1', 'f23_2', 'f23_3', 'f23_4', 'f23_5', 'f23_6', 'f24', 'f24_1', 'f24_2', 'f24_3', 'f25', 'f25_1', 'f25_3', 'f25_7', 'f25_8', 'f25_9', 'f2_1', 'f2_10', 'f2_11', 'f2_12', 'f2_13', 'f2_2', 'f2_4', 'f2_5', 'f2_6', 'f2_7', 'f2_8', 'f3_1', 'f3_2', 'f3_3', 'f4', 'f4_1', 'f4_10', 'f4_11', 'f4_13', 'f4_14', 'f4_16', 'f4_17', 'f4_18', 'f4_2', 'f4_3', 'f4_5', 'f4_6', 'f4_7', 'f4_8', 'f4_9', 'f5', 'f5_1', 'f5_3', 'f5_4', 'f6', 'f6_1', 'f6_2', 'f6_4', 'f7', 'f8', 'f8_1', 'f8_2', 'f8_3', 'f8_4', 'f8_5']\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "from tables import *\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "try:\n",
    "  import cPickle as pickle\n",
    "except:\n",
    "  import pickle\n",
    "FIELDS_PICKLE = 'fields-4.pkl'\n",
    "DATAFILE_PATH = 'xjk_pytable.h5'\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "\n",
    "fields = pickle.load(open(FIELDS_PICKLE, \"r\") )\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)\n",
    "\n",
    "print \"Number of features: {}\".format(len(features))\n",
    "print \"Features:\"\n",
    "print features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 102592 rows\n",
      "processed 0 rows\n",
      "processed 5000 rows\n",
      "processed 10000 rows\n",
      "processed 15000 rows\n",
      "processed 20000 rows\n",
      "processed 25000 rows\n",
      "processed 30000 rows\n",
      "processed 35000 rows\n",
      "processed 40000 rows\n",
      "processed 45000 rows\n",
      "processed 50000 rows\n",
      "processed 55000 rows\n",
      "processed 60000 rows\n",
      "processed 65000 rows\n",
      "processed 70000 rows\n",
      "processed 75000 rows\n",
      "processed 80000 rows\n",
      "processed 85000 rows\n",
      "processed 90000 rows\n",
      "processed 95000 rows\n",
      "processed 100000 rows\n"
     ]
    }
   ],
   "source": [
    "fileh1 = open_file(DATAFILE_PATH, mode = 'r')\n",
    "\n",
    "object = fileh1.get_node('/train', 'gaps')\n",
    "object_array_data = object.read()\n",
    "fileh1.close()\n",
    "\n",
    "# Convert to vectorized array that we can use in further processing.\n",
    "all_data = np.zeros((object_array_data.shape[0], len(fields)))\n",
    "print 'there are {} rows'.format(object_array_data.shape[0])\n",
    "for rcounter, row in enumerate(object_array_data):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "One hot encoding is a process of converting categorical data into binary i.e. from `day_in_week` to `day_in_week_0`, `day_in_week_1`, ... , `day_in_week_6`, each contains value 0 or 1. The former version cannot be used for machine learning as the numerical data were meaningless e.g. Sunday is 0 and Tuesday is 2, but it doesn't mean that Tuesday is \"higher\" or \"larger\" in mathematical sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.nan= (array([  1343,   1343,   1343, ..., 102591, 102591, 102591]), array([ 3,  4,  5, ..., 21, 22, 23]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=[0, 1, 2, 3, 4], dtype=<type 'float'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print \"np.nan=\", np.where(np.isnan(all_data_original))\n",
    "\n",
    "# Impute all NaN with numbers\n",
    "all_data[np.isnan(all_data)] = 0\n",
    "\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]\n",
    "\n",
    "one_hot = OneHotEncoder(categorical_features=[0, 1, 2, 3, 4], sparse=False)\n",
    "one_hot.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([145,   7,  10,  10,  10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.n_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 145, 152, 162, 172, 182])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.feature_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new number of features: 336\n"
     ]
    }
   ],
   "source": [
    "data = one_hot.transform(data_original)\n",
    "n_features = data.shape[1]\n",
    "print 'new number of features: {}'.format(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.145833333333\n",
      "254.604166667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "predictions = np.array([1.0, 2.0, 3.0, 4.0]).astype('float32')\n",
    "y = np.array([1.0, 2.0, 3.0, 4.0]).astype('float32')\n",
    "\n",
    "# Should return 0.0\n",
    "print mape(y, predictions)\n",
    "\n",
    "# Should return higher score\n",
    "predictions = np.array([1.0, 2.0, 2.0, 3.0]).astype('float32')\n",
    "print(mape(y, predictions))\n",
    "\n",
    "# Should return highest score\n",
    "predictions = np.array([1000.0, 22.0, 11.0, 31.0]).astype('float32')\n",
    "print(mape(y, predictions))\n",
    "\n",
    "# est = LogisticRegression()\n",
    "# X = np.random.rand(10,4)\n",
    "# y = X.sum(axis=1)\n",
    "# est.fit(X, y)\n",
    "# predictions = est.predict(X)\n",
    "# print(mape(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Testing Algorithm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519787107909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "class CustomRegressor(BaseEstimator, RegressorMixin):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, X, y):\n",
    "#     self.classes_, indices = np.unique([\"foo\", \"bar\", \"foo\"],\n",
    "#                                     return_inverse=True)\n",
    "#     self.majority_ = np.argmax(np.bincount(indices))\n",
    "    return self\n",
    "\n",
    "  def predict(self, X):\n",
    "    # 56: gap_1_slots_ago\n",
    "    # 58: gap_2_slots_ago\n",
    "    # 60: gap_3_slots_ago\n",
    "#     X = X.tocsr()\n",
    "#     v1 = coo_matrix(np.asmatrix(np.ones(X.shape[0])).T)\n",
    "    v1 = np.asmatrix(np.ones(X.shape[0]))\n",
    "    v2 = np.asmatrix((X[:, 23]*0.65+X[:, 25]*0.25+X[:, 27]*0.15)/2)\n",
    "    predictions = np.asarray(np.concatenate((v1, v2), axis=0).max(axis=0))\n",
    "    \n",
    "    return predictions\n",
    "  \n",
    "custom_est = CustomRegressor()\n",
    "custom_est.fit(data, targets)\n",
    "custom_predictions = custom_est.predict(data)\n",
    "print(mape(targets, custom_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training fold 1\n",
      "Score: 1.27281437221\n",
      "Now training fold 2\n",
      "Score: 1.3630237399\n",
      "Now training fold 3\n",
      "Score: 1.42335299024\n",
      "Now training fold 4\n",
      "Score: 1.42799111235\n",
      "Now training fold 5\n",
      "Score: 1.45308008865\n",
      "Now training fold 6\n",
      "Score: 1.34971090885\n",
      "Now training fold 7\n",
      "Score: 1.50255585105\n",
      "Now training fold 8\n",
      "Score: 1.3823985588\n",
      "Now training fold 9\n",
      "Score: 1.39588625226\n",
      "Now training fold 10\n",
      "Score: 1.38658224032\n"
     ]
    }
   ],
   "source": [
    "all_data = np.copy(all_data_original)\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [\n",
    "  # n_values needs to be set to avoid problems where not all classes exist in training data.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3, 4], sparse=False,\n",
    "                            n_values=[145, 7, 10, 10, 10])),\n",
    "  ('estimate', DecisionTreeRegressor(random_state=seed))\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 1.39573961147\n"
     ]
    }
   ],
   "source": [
    "print \"Mean score: {}\".format(sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
