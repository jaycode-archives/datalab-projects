{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'fields-4.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-edc4203af8fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFIELDS_PICKLE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'fields-4.pkl'"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "from tables import *\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME = 'baseline_final_estimator.pkl'\n",
    "FIELDS_PICKLE = 'fields-4.pkl'\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "\n",
    "fields = pickle.load(open(FIELDS_PICKLE, \"r\") )\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)\n",
    "datafile_path = 'xjk_pytable.h5'\n",
    "\n",
    "print \"Number of features: {}\".format(len(features))\n",
    "print \"Features:\"\n",
    "print features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 102592 rows\n",
      "processed 0 rows\n",
      "processed 5000 rows\n",
      "processed 10000 rows\n",
      "processed 15000 rows\n",
      "processed 20000 rows\n",
      "processed 25000 rows\n",
      "processed 30000 rows\n",
      "processed 35000 rows\n",
      "processed 40000 rows\n",
      "processed 45000 rows\n",
      "processed 50000 rows\n",
      "processed 55000 rows\n",
      "processed 60000 rows\n",
      "processed 65000 rows\n",
      "processed 70000 rows\n",
      "processed 75000 rows\n",
      "processed 80000 rows\n",
      "processed 85000 rows\n",
      "processed 90000 rows\n",
      "processed 95000 rows\n",
      "processed 100000 rows\n"
     ]
    }
   ],
   "source": [
    "fileh1 = open_file(datafile_path, mode = 'r')\n",
    "\n",
    "object = fileh1.get_node('/train', 'gaps')\n",
    "object_array_data = object.read()\n",
    "fileh1.close()\n",
    "\n",
    "# Convert to vectorized array that we can use in further processing.\n",
    "all_data = np.zeros((object_array_data.shape[0], len(fields)))\n",
    "print 'there are {} rows'.format(object_array_data.shape[0])\n",
    "for rcounter, row in enumerate(object_array_data):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)\n",
    "\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "One hot encoding is s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot = OneHotEncoder(categorical_features=[0, 1, 2, 3, 4], sparse=False)\n",
    "one_hot.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 5, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.n_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  9, 14, 19])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.feature_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new number of features: 169\n"
     ]
    }
   ],
   "source": [
    "data_train = one_hot.transform(data_train_original)\n",
    "data_test = one_hot.transform(data_test_original)\n",
    "n_features = data_train.shape[1]\n",
    "print 'new number of features: {}'.format(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.145833333333\n",
      "254.604166667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "predictions = np.array([1.0, 2.0, 3.0, 4.0]).astype('float32')\n",
    "y = np.array([1.0, 2.0, 3.0, 4.0]).astype('float32')\n",
    "\n",
    "# Should return 0.0\n",
    "print mape(y, predictions)\n",
    "\n",
    "# Should return higher score\n",
    "predictions = np.array([1.0, 2.0, 2.0, 3.0]).astype('float32')\n",
    "print(mape(y, predictions))\n",
    "\n",
    "# Should return highest score\n",
    "predictions = np.array([1000.0, 22.0, 11.0, 31.0]).astype('float32')\n",
    "print(mape(y, predictions))\n",
    "\n",
    "# est = LogisticRegression()\n",
    "# X = np.random.rand(10,4)\n",
    "# y = X.sum(axis=1)\n",
    "# est.fit(X, y)\n",
    "# predictions = est.predict(X)\n",
    "# print(mape(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Testing Algorithm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39539351749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "class CustomRegressor(BaseEstimator, RegressorMixin):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, X, y):\n",
    "#     self.classes_, indices = np.unique([\"foo\", \"bar\", \"foo\"],\n",
    "#                                     return_inverse=True)\n",
    "#     self.majority_ = np.argmax(np.bincount(indices))\n",
    "    return self\n",
    "\n",
    "  def predict(self, X):\n",
    "    # 56: gap_1_slots_ago\n",
    "    # 58: gap_2_slots_ago\n",
    "    # 60: gap_3_slots_ago\n",
    "#     X = X.tocsr()\n",
    "#     v1 = coo_matrix(np.asmatrix(np.ones(X.shape[0])).T)\n",
    "    v1 = np.asmatrix(np.ones(X.shape[0]))\n",
    "    v2 = np.asmatrix((X[:, 23]*0.65+X[:, 25]*0.25+X[:, 27]*0.15)/2)\n",
    "    predictions = np.asarray(np.concatenate((v1, v2), axis=0).max(axis=0))\n",
    "    \n",
    "    return predictions\n",
    "  \n",
    "custom_est = CustomRegressor()\n",
    "custom_est.fit(data_train_original, data_test_original)\n",
    "custom_predictions = custom_est.predict(data_test_original)\n",
    "print(mape(targets_test, custom_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] estimate__max_features=110 ......................................\n",
      "[CV] estimate__max_features=110 ......................................\n",
      "[CV] estimate__max_features=110 ......................................\n",
      "[CV] estimate__max_features=110 ......................................\n",
      "[CV] estimate__max_features=110 ......................................\n",
      "[CV] ............ estimate__max_features=110, score=-1.015761 -   0.9s\n",
      "[CV] estimate__max_features=111 ......................................\n",
      "[CV] ............ estimate__max_features=110, score=-0.997429 -   0.9s\n",
      "[CV] estimate__max_features=111 ......................................\n",
      "[CV] ............ estimate__max_features=110, score=-1.007373 -   0.9s\n",
      "[CV] estimate__max_features=111 ......................................\n",
      "[CV] ............ estimate__max_features=110, score=-0.774039 -   1.0s\n",
      "[CV] estimate__max_features=111 ......................................\n",
      "[CV] ............ estimate__max_features=110, score=-1.318165 -   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done   1 jobs       | elapsed:    0.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] estimate__max_features=111 ......................................\n",
      "[CV] ............ estimate__max_features=111, score=-1.153101 -   0.8s\n",
      "[CV] estimate__max_features=112 ......................................\n",
      "[CV] ............ estimate__max_features=111, score=-1.121552 -   0.8s\n",
      "[CV] estimate__max_features=112 ......................................\n",
      "[CV] ............ estimate__max_features=111, score=-0.903246 -   0.9s\n",
      "[CV] estimate__max_features=112 ......................................\n",
      "[CV] ............ estimate__max_features=111, score=-0.817320 -   0.9s\n",
      "[CV] estimate__max_features=112 ......................................\n",
      "[CV] ............ estimate__max_features=111, score=-1.487765 -   0.9s\n",
      "[CV] estimate__max_features=112 ......................................\n",
      "[CV] ............ estimate__max_features=112, score=-1.011377 -   0.9s\n",
      "[CV] ............ estimate__max_features=112, score=-0.706411 -   0.8s\n",
      "[CV] estimate__max_features=113 ......................................\n",
      "[CV] estimate__max_features=113 ......................................\n",
      "[CV] ............ estimate__max_features=112, score=-0.963112 -   0.8s\n",
      "[CV] estimate__max_features=113 ......................................\n",
      "[CV] ............ estimate__max_features=112, score=-1.085963 -   0.9s\n",
      "[CV] estimate__max_features=113 ......................................\n",
      "[CV] ............ estimate__max_features=112, score=-1.165414 -   1.0s\n",
      "[CV] estimate__max_features=113 ......................................\n",
      "[CV] ............ estimate__max_features=113, score=-1.074745 -   0.8s\n",
      "[CV] estimate__max_features=114 ......................................\n",
      "[CV] ............ estimate__max_features=113, score=-0.849087 -   0.9s\n",
      "[CV] estimate__max_features=114 ......................................\n",
      "[CV] ............ estimate__max_features=113, score=-1.261958 -   0.8s\n",
      "[CV] estimate__max_features=114 ......................................\n",
      "[CV] ............ estimate__max_features=113, score=-1.039479 -   0.9s\n",
      "[CV] estimate__max_features=114 ......................................\n",
      "[CV] ............ estimate__max_features=113, score=-1.338239 -   0.7s\n",
      "[CV] estimate__max_features=114 ......................................\n",
      "[CV] ............ estimate__max_features=114, score=-0.733301 -   0.7s\n",
      "[CV] ............ estimate__max_features=114, score=-1.136518 -   0.8s\n",
      "[CV] estimate__max_features=115 ......................................\n",
      "[CV] estimate__max_features=115 ......................................\n",
      "[CV] ............ estimate__max_features=114, score=-0.938794 -   0.9s\n",
      "[CV] estimate__max_features=115 ......................................\n",
      "[CV] ............ estimate__max_features=114, score=-1.129468 -   0.8s\n",
      "[CV] estimate__max_features=115 ......................................\n",
      "[CV] ............ estimate__max_features=114, score=-1.200624 -   0.8s\n",
      "[CV] estimate__max_features=115 ......................................\n",
      "[CV] ............ estimate__max_features=115, score=-1.079064 -   0.8s\n",
      "[CV] estimate__max_features=116 ......................................\n",
      "[CV] ............ estimate__max_features=115, score=-0.854139 -   0.8s\n",
      "[CV] estimate__max_features=116 ......................................\n",
      "[CV] ............ estimate__max_features=115, score=-0.983594 -   0.8s\n",
      "[CV] estimate__max_features=116 ......................................\n",
      "[CV] ............ estimate__max_features=115, score=-1.068311 -   0.8s\n",
      "[CV] estimate__max_features=116 ......................................\n",
      "[CV] ............ estimate__max_features=115, score=-1.465256 -   0.8s\n",
      "[CV] estimate__max_features=116 ......................................\n",
      "[CV] ............ estimate__max_features=116, score=-0.927773 -   0.8s\n",
      "[CV] estimate__max_features=117 ......................................\n",
      "[CV] ............ estimate__max_features=116, score=-0.787895 -   0.9s\n",
      "[CV] estimate__max_features=117 ......................................\n",
      "[CV] ............ estimate__max_features=116, score=-1.010874 -   0.9s\n",
      "[CV] estimate__max_features=117 ......................................\n",
      "[CV] ............ estimate__max_features=116, score=-1.146107 -   0.8s\n",
      "[CV] estimate__max_features=117 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  32 jobs       | elapsed:    6.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ estimate__max_features=116, score=-1.271926 -   0.8s\n",
      "[CV] estimate__max_features=117 ......................................\n",
      "[CV] ............ estimate__max_features=117, score=-1.026548 -   0.9s\n",
      "[CV] estimate__max_features=118 ......................................\n",
      "[CV] ............ estimate__max_features=117, score=-0.746136 -   0.8s\n",
      "[CV] estimate__max_features=118 ......................................\n",
      "[CV] ............ estimate__max_features=117, score=-1.111784 -   0.8s\n",
      "[CV] ............ estimate__max_features=117, score=-1.270591 -   0.8s\n",
      "[CV] estimate__max_features=118 ......................................\n",
      "[CV] estimate__max_features=118 ......................................\n",
      "[CV] ............ estimate__max_features=117, score=-1.301783 -   0.8s\n",
      "[CV] estimate__max_features=118 ......................................\n",
      "[CV] ............ estimate__max_features=118, score=-1.043956 -   0.7s\n",
      "[CV] estimate__max_features=119 ......................................\n",
      "[CV] ............ estimate__max_features=118, score=-0.896408 -   0.8s\n",
      "[CV] estimate__max_features=119 ......................................\n",
      "[CV] ............ estimate__max_features=118, score=-1.018806 -   0.8s\n",
      "[CV] estimate__max_features=119 ......................................\n",
      "[CV] ............ estimate__max_features=118, score=-0.724651 -   1.0s\n",
      "[CV] estimate__max_features=119 ......................................\n",
      "[CV] ............ estimate__max_features=118, score=-1.567244 -   0.9s\n",
      "[CV] estimate__max_features=119 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  42 out of  50 | elapsed:    8.1s remaining:    1.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ estimate__max_features=119, score=-1.013921 -   0.8s\n",
      "[CV] ............ estimate__max_features=119, score=-0.742392 -   0.7s\n",
      "[CV] ............ estimate__max_features=119, score=-1.124674 -   0.6s\n",
      "[CV] ............ estimate__max_features=119, score=-1.101916 -   0.8s\n",
      "[CV] ............ estimate__max_features=119, score=-1.400818 -   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  50 out of  50 | elapsed:    8.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: -1.02255, std: 0.17324, params: {'estimate__max_features': 110}, mean: -1.09660, std: 0.23333, params: {'estimate__max_features': 111}, mean: -0.98646, std: 0.15590, params: {'estimate__max_features': 112}, mean: -1.11270, std: 0.17288, params: {'estimate__max_features': 113}, mean: -1.02774, std: 0.17128, params: {'estimate__max_features': 114}, mean: -1.09007, std: 0.20414, params: {'estimate__max_features': 115}, mean: -1.02891, std: 0.16819, params: {'estimate__max_features': 116}, mean: -1.09137, std: 0.20010, params: {'estimate__max_features': 117}, mean: -1.05021, std: 0.28213, params: {'estimate__max_features': 118}, mean: -1.07674, std: 0.21152, params: {'estimate__max_features': 119}]\n",
      "{'estimate__max_features': 112}\n",
      "-0.986455231943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=112,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='best'))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False)),\n",
    "#   ('impute', Imputer(0)),\n",
    "#   ('feature_selection', SelectKBest(f_classif)),\n",
    "  ('pca', PCA(n_components=120)),\n",
    "  ('estimate', DecisionTreeRegressor())\n",
    "]\n",
    "\n",
    "est = Pipeline(steps)\n",
    "\n",
    "params = {\n",
    "#   'one_hot__n_values': [7, 10, 20],\n",
    "#   \"feature_selection__k\": [i for i in range(1, n_features - 1)]\n",
    "  'estimate__max_features': [i for i in range(110, 120)],\n",
    "#   'estimate__learning_rate': [0.1, 0.5, 1, 5, 10],\n",
    "#   'estimate__n_estimators': [i for i in range(110, 120, 2)],\n",
    "#   'estimate__loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "# cross_validation_iter = StratifiedShuffleSplit(y=targets_train, test_size=0.3,\n",
    "#                                                random_state=RANDOM_STATE, n_iter=10)\n",
    "# search_params = RandomizedSearchCV(\n",
    "#   estimator=est,\n",
    "#   param_distributions=params,\n",
    "#   cv=5,\n",
    "#   scoring=mape_scorer,\n",
    "#   n_jobs=2,\n",
    "#   verbose=1\n",
    "# )\n",
    "\n",
    "search_params = GridSearchCV(\n",
    "  estimator=est,\n",
    "  param_grid=params,\n",
    "  cv=5,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=5,\n",
    "  verbose=3\n",
    ")\n",
    "\n",
    "search_params.fit(data_train_original, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923959635952\n"
     ]
    }
   ],
   "source": [
    "final_est = search_params.best_estimator_\n",
    "test_predictions = final_est.predict(data_test_original)\n",
    "print(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(final_est, open(EST_PICKLE_FILENAME, \"w\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"Process Final Test Data With Final Algorithm\" to use pickled final algorithm against final test data to produce csv required by this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 169)\n",
      "[[  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   1.74300000e+03\n",
      "    2.07500000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   1.00000000e+00   0.00000000e+00 ...,   1.66000000e+02\n",
      "    7.47000000e+02   1.66000000e+02]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   1.90900000e+03\n",
      "    3.40300000e+03   0.00000000e+00]\n",
      " ..., \n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   7.30400000e+03\n",
      "    9.04700000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   5.89300000e+03\n",
      "    9.62800000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   7.30400000e+03\n",
      "    9.04700000e+03   0.00000000e+00]]\n",
      "(800, 169)\n",
      "[[  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   1.74300000e+03\n",
      "    2.07500000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   1.00000000e+00   0.00000000e+00 ...,   1.66000000e+02\n",
      "    7.47000000e+02   1.66000000e+02]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   1.90900000e+03\n",
      "    3.40300000e+03   0.00000000e+00]\n",
      " ..., \n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   7.30400000e+03\n",
      "    9.04700000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   5.89300000e+03\n",
      "    9.62800000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   7.30400000e+03\n",
      "    9.04700000e+03   0.00000000e+00]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mape() takes exactly 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-127cf5008272>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: mape() takes exactly 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Just testing Imputer. Turns out somehow Imputer causes number of features reduced, weird.\n",
    "\n",
    "# imputer = Imputer()\n",
    "est = DecisionTreeRegressor(max_features=len(features))\n",
    "\n",
    "data_train_i = np.copy(data_train)\n",
    "print(data_train.shape)\n",
    "print(data_train[0:10])\n",
    "# data_train_i = imputer.fit_transform(data_train)\n",
    "data_train_i[np.isnan(data_train_i)] = 0\n",
    "data_train_i.astype('float32')\n",
    "print(data_train_i.shape)\n",
    "print(data_train_i[0:10])\n",
    "est.fit(data_train_i, targets_train)\n",
    "predictions = est.predict(data_test)\n",
    "print(mape(data_test, predictions, targets_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=112,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "           \n",
    "1000 training data, Score: 0.924\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=114,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "\n",
    "5000 training data, Score: 0.992\n",
    "\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=111,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "           \n",
    "20000 training data, Score: 1.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
