{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME = 'final_estimator.pkl'\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\tbusy_time\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample, IF(timeofday_slot >= 50 AND timeofday_slot <= 53, 1, 0) AS busy_time\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0\n",
    "LIMIT 1000\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1000 rows\n",
      "processed 0 rows\n"
     ]
    }
   ],
   "source": [
    "query = bq.Query(q_all)\n",
    "\n",
    "tableresult = query.results()\n",
    "\n",
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_values_:\n",
      "[ 7 10 10 10]\n",
      "feature_indices_:\n",
      "[ 0  7 17 27 37]\n",
      "new number of features: 196\n"
     ]
    }
   ],
   "source": [
    "one_hot = OneHotEncoder(categorical_features=[1, 2, 3, 4], sparse=False,\n",
    "                        n_values=[7, 10, 10, 10])\n",
    "one_hot.fit(Imputer().fit_transform(all_data_original))\n",
    "print \"n_values_:\"\n",
    "print one_hot.n_values_\n",
    "print \"feature_indices_:\"\n",
    "print one_hot.feature_indices_\n",
    "all_data = one_hot.fit_transform(Imputer().fit_transform(all_data_original))\n",
    "n_features = all_data.shape[1] - 1\n",
    "print 'new number of features: {}'.format(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets.\n",
    "data_size = all_data.shape[0]\n",
    "training_size = data_size * 80/100\n",
    "indices = np.random.permutation(data_size)\n",
    "training_idx, test_idx = indices[:training_size], indices[training_size:]\n",
    "all_data_train, all_data_test = all_data_original[training_idx,:], all_data_original[test_idx,:]\n",
    "\n",
    "data_train = all_data_train[:,1:]\n",
    "targets_train = all_data_train[:,0]\n",
    "data_test = all_data_test[:,1:]\n",
    "targets_test = all_data_test[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Testing Algorithm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('estimate', GradientBoostingRegressor(n_estimators=n_features, learning_rate=0.5))\n",
    "\n",
    "]\n",
    "\n",
    "est = Pipeline(steps)\n",
    "\n",
    "steps_bagging = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, was removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('estimate', BaggingRegressor(\n",
    "      GradientBoostingRegressor(\n",
    "        n_estimators=n_features, learning_rate=0.5,\n",
    "        verbose=1, warm_start=True\n",
    "      )))\n",
    "\n",
    "]\n",
    "\n",
    "bagging_est = Pipeline(steps_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1         851.4335            4.28s\n",
      "         2         269.7370            3.31s\n",
      "         3         109.4936            2.94s\n",
      "         4          59.9242            2.74s\n",
      "         5          42.9556            2.63s\n",
      "         6          33.4628            2.55s\n",
      "         7          26.5944            2.49s\n",
      "         8          23.1426            2.43s\n",
      "         9          19.7273            2.40s\n",
      "        10          18.9794            2.35s\n",
      "        20           7.3881            2.22s\n",
      "        30           3.3510            2.06s\n",
      "        40           1.8712            1.97s\n",
      "        50           1.1953            1.83s\n",
      "        60           0.6876            1.72s\n",
      "        70           0.4754            1.59s\n",
      "        80           0.3227            1.47s\n",
      "        90           0.2201            1.34s\n",
      "       100           0.1474            1.22s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         670.0924            4.01s\n",
      "         2         224.1312            3.14s\n",
      "         3         102.6394            2.82s\n",
      "         4          59.5365            2.68s\n",
      "         5          44.5434            2.58s\n",
      "         6          38.1147            2.52s\n",
      "         7          33.0971            2.51s\n",
      "         8          28.9253            2.48s\n",
      "         9          26.6641            2.46s\n",
      "        10          24.2499            2.46s\n",
      "        20           6.0437            2.30s\n",
      "        30           2.8320            2.14s\n",
      "        40           1.4860            2.11s\n",
      "        50           0.9112            1.95s\n",
      "        60           0.5706            1.82s\n",
      "        70           0.3638            1.68s\n",
      "        80           0.2174            1.55s\n",
      "        90           0.1511            1.41s\n",
      "       100           0.1018            1.28s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1036.1103            3.97s\n",
      "         2         314.6356            3.11s\n",
      "         3         118.0353            2.83s\n",
      "         4          59.1249            2.72s\n",
      "         5          41.8505            2.67s\n",
      "         6          32.8680            2.59s\n",
      "         7          27.0136            2.55s\n",
      "         8          24.2732            2.50s\n",
      "         9          21.3404            2.47s\n",
      "        10          18.7904            2.43s\n",
      "        20           5.8467            2.26s\n",
      "        30           2.4943            2.15s\n",
      "        40           1.5461            2.06s\n",
      "        50           0.9583            1.94s\n",
      "        60           0.5533            1.80s\n",
      "        70           0.3666            1.67s\n",
      "        80           0.2373            1.54s\n",
      "        90           0.1442            1.40s\n",
      "       100           0.0978            1.27s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         906.6932            3.78s\n",
      "         2         280.3555            3.00s\n",
      "         3         111.3238            2.75s\n",
      "         4          61.2865            2.64s\n",
      "         5          41.0961            2.56s\n",
      "         6          30.4932            2.50s\n",
      "         7          25.3330            2.49s\n",
      "         8          22.6483            2.47s\n",
      "         9          20.0343            2.42s\n",
      "        10          18.0952            2.39s\n",
      "        20           5.6069            2.29s\n",
      "        30           2.6875            2.15s\n",
      "        40           1.6406            2.04s\n",
      "        50           0.9713            1.89s\n",
      "        60           0.5698            1.76s\n",
      "        70           0.3867            1.62s\n",
      "        80           0.2260            1.50s\n",
      "        90           0.1579            1.37s\n",
      "       100           0.0986            1.25s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         607.1854            3.97s\n",
      "         2         219.9174            3.20s\n",
      "         3         109.4054            2.88s\n",
      "         4          67.4301            2.72s\n",
      "         5          53.5386            2.63s\n",
      "         6          43.2981            2.58s\n",
      "         7          34.2775            2.54s\n",
      "         8          29.4659            2.51s\n",
      "         9          21.1051            2.48s\n",
      "        10          18.2731            2.44s\n",
      "        20           8.3469            2.31s\n",
      "        30           3.7296            2.18s\n",
      "        40           1.9525            2.06s\n",
      "        50           1.4014            1.91s\n",
      "        60           0.8159            1.78s\n",
      "        70           0.5129            1.64s\n",
      "        80           0.3369            1.52s\n",
      "        90           0.2389            1.40s\n",
      "       100           0.1731            1.27s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         539.8216            4.00s\n",
      "         2         193.6779            3.15s\n",
      "         3          91.4142            2.82s\n",
      "         4          53.9421            2.69s\n",
      "         5          34.2038            2.63s\n",
      "         6          25.9523            2.56s\n",
      "         7          20.1012            2.52s\n",
      "         8          16.3210            2.48s\n",
      "         9          14.9529            2.44s\n",
      "        10          13.3594            2.43s\n",
      "        20           4.3891            2.32s\n",
      "        30           1.9873            2.19s\n",
      "        40           1.0288            2.03s\n",
      "        50           0.6156            1.88s\n",
      "        60           0.3923            1.74s\n",
      "        70           0.2586            1.61s\n",
      "        80           0.1643            1.47s\n",
      "        90           0.1175            1.35s\n",
      "       100           0.0795            1.23s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         631.7003            3.80s\n",
      "         2         225.9218            3.03s\n",
      "         3         108.3940            2.78s\n",
      "         4          58.3001            2.64s\n",
      "         5          40.1233            2.54s\n",
      "         6          30.7173            2.49s\n",
      "         7          26.2604            2.42s\n",
      "         8          23.3971            2.40s\n",
      "         9          18.8697            2.39s\n",
      "        10          16.4647            2.35s\n",
      "        20           6.3803            2.19s\n",
      "        30           2.9892            2.06s\n",
      "        40           1.6361            1.94s\n",
      "        50           1.0176            1.81s\n",
      "        60           0.6241            1.68s\n",
      "        70           0.3936            1.56s\n",
      "        80           0.2730            1.43s\n",
      "        90           0.1843            1.31s\n",
      "       100           0.1228            1.18s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         716.9926            3.90s\n",
      "         2         231.8629            3.04s\n",
      "         3         104.6255            2.78s\n",
      "         4          61.5581            2.63s\n",
      "         5          45.8757            2.55s\n",
      "         6          37.4771            2.50s\n",
      "         7          31.3208            2.45s\n",
      "         8          28.7054            2.40s\n",
      "         9          25.0521            2.39s\n",
      "        10          19.7318            2.37s\n",
      "        20           5.8857            2.24s\n",
      "        30           2.8941            2.29s\n",
      "        40           1.4224            2.10s\n",
      "        50           0.8948            1.94s\n",
      "        60           0.5521            1.80s\n",
      "        70           0.3627            1.65s\n",
      "        80           0.2536            1.51s\n",
      "        90           0.1610            1.38s\n",
      "       100           0.1072            1.24s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         594.9922            3.83s\n",
      "         2         198.4807            3.03s\n",
      "         3          88.4360            2.75s\n",
      "         4          48.1177            2.59s\n",
      "         5          33.0595            2.49s\n",
      "         6          24.1875            2.44s\n",
      "         7          21.6866            2.39s\n",
      "         8          16.2719            2.35s\n",
      "         9          14.0447            2.32s\n",
      "        10          11.5171            2.30s\n",
      "        20           4.7233            2.14s\n",
      "        30           2.1269            2.00s\n",
      "        40           1.1434            1.89s\n",
      "        50           0.7061            1.76s\n",
      "        60           0.4258            1.65s\n",
      "        70           0.2725            1.54s\n",
      "        80           0.1708            1.42s\n",
      "        90           0.1051            1.30s\n",
      "       100           0.0719            1.18s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         369.4277            4.01s\n",
      "         2         161.7225            3.14s\n",
      "         3          97.5710            2.84s\n",
      "         4          67.5421            2.70s\n",
      "         5          52.4444            2.63s\n",
      "         6          45.4443            2.58s\n",
      "         7          37.6831            2.54s\n",
      "         8          30.1777            2.52s\n",
      "         9          27.6141            2.48s\n",
      "        10          23.3366            2.46s\n",
      "        20           8.1192            2.28s\n",
      "        30           3.9257            2.15s\n",
      "        40           2.0552            2.07s\n",
      "        50           1.1895            1.93s\n",
      "        60           0.7145            1.80s\n",
      "        70           0.4616            1.66s\n",
      "        80           0.2930            1.53s\n",
      "        90           0.1988            1.40s\n",
      "       100           0.1184            1.27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
       "       handle_unknown='error', n_values=[7, 10, 10, 10], sparse=False)), ('scale', StandardScaler(copy=True, with_mean=T...ax_samples=1.0, n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_est.fit(data_train, targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
       "       handle_unknown='error', n_values=[7, 10, 10, 10], sparse=False)), ('scale', StandardScaler(copy=True, with_mean=T...0.0, n_estimators=196,\n",
       "             random_state=None, subsample=1.0, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.fit(data_train, targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796995443096\n"
     ]
    }
   ],
   "source": [
    "test_predictions = bagging_est.predict(data_test)\n",
    "print(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.877282931762\n"
     ]
    }
   ],
   "source": [
    "test_predictions = est.predict(data_test)\n",
    "print(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_est = search_params.best_estimator_\n",
    "test_predictions = final_est.predict(data_test_original)\n",
    "print(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(final_est, open(EST_PICKLE_FILENAME, \"w\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare baseline with final algorithm\n",
    "\n",
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import time\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dropout\n",
    "\n",
    "HDF_FILENAME = 'final_model3.hdf5'\n",
    "JSON_MODEL_FILENAME = 'final_model.json'\n",
    "\n",
    "\n",
    "steps = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, is removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "#   ('pca', PCA(n_components=120)),\n",
    "#   ('estimate', final_model)\n",
    "]\n",
    "transformer = Pipeline(steps)\n",
    "data_test2 = transformer.fit_transform(data_test_original)\n",
    "\n",
    "\n",
    "\n",
    "def mape(y, predictions):\n",
    "  return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "def get_optimizer(epochs=50):\n",
    "  learning_rate = 0.2\n",
    "  decay_rate = learning_rate / epochs\n",
    "  momentum = 0.8\n",
    "  return SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "from keras.models import model_from_json\n",
    "json_file = open(JSON_MODEL_FILENAME, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(HDF_FILENAME)\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss=mape, optimizer=get_optimizer(epochs))\n",
    "\n",
    "score = loaded_model.evaluate(data_test2, targets_test, verbose=0)\n",
    "print \"%s: %.2f\" % (loaded_model.metrics_names[0], score)\n",
    "test_predictions2 = loaded_model.predict(data_test2)\n",
    "print 'sample predictions:'\n",
    "print(test_predictions2[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"Process Final Test Data With Final Algorithm\" to use pickled final algorithm against final test data to produce csv required by this competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=112,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "           \n",
    "1000 training data, Score: 0.924\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=114,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "\n",
    "5000 training data, Score: 0.992\n",
    "\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=111,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "           \n",
    "20000 training data, Score: 1.001\n",
    "\n",
    "Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values=[7, 10, 10, 10], sparse=False)), ('scale', StandardScaler(copy=True, with_mean=T...s_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "           \n",
    "80000 training data, Score: 0.977 (submitted date 17)\n",
    "\n",
    "[('impute',\n",
    "  Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)),\n",
    " ('one_hot',\n",
    "  OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "         handle_unknown='error', n_values=[7, 10, 10, 10], sparse=False)),\n",
    " ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    " ('estimate',\n",
    "  GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.5, loss='ls',\n",
    "               max_depth=3, max_features=None, max_leaf_nodes=None,\n",
    "               min_samples_leaf=1, min_samples_split=2,\n",
    "               min_weight_fraction_leaf=0.0, n_estimators=190,\n",
    "               random_state=None, subsample=1.0, verbose=0, warm_start=False))]\n",
    "               \n",
    "80000 training data, Score: 0.798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
