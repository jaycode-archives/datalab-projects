{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME = 'baseline_final_estimator.pkl'\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\tbusy_time\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.145833333333\n",
      "254.604166667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "predictions = np.array([1.0, 2.0, 3.0, 4.0]).astype('float32')\n",
    "y = np.array([1.0, 2.0, 3.0, 4.0]).astype('float32')\n",
    "\n",
    "# Should return 0.0\n",
    "print mape(y, predictions)\n",
    "\n",
    "# Should return higher score\n",
    "predictions = np.array([1.0, 2.0, 2.0, 3.0]).astype('float32')\n",
    "print(mape(y, predictions))\n",
    "\n",
    "# Should return highest score\n",
    "predictions = np.array([1000.0, 22.0, 11.0, 31.0]).astype('float32')\n",
    "print(mape(y, predictions))\n",
    "\n",
    "# est = LogisticRegression()\n",
    "# X = np.random.rand(10,4)\n",
    "# y = X.sum(axis=1)\n",
    "# est.fit(X, y)\n",
    "# predictions = est.predict(X)\n",
    "# print(mape(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample, IF(timeofday_slot >= 50 AND timeofday_slot <= 53, 1, 0) AS busy_time\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0\n",
    "LIMIT 80000\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 80000 rows\n",
      "processed 0 rows\n",
      "processed 5000 rows\n",
      "processed 10000 rows\n",
      "processed 15000 rows\n",
      "processed 20000 rows\n",
      "processed 25000 rows\n",
      "processed 30000 rows\n",
      "processed 35000 rows\n",
      "processed 40000 rows\n",
      "processed 45000 rows\n",
      "processed 50000 rows\n",
      "processed 55000 rows\n",
      "processed 60000 rows\n",
      "processed 65000 rows\n",
      "processed 70000 rows\n",
      "processed 75000 rows\n"
     ]
    }
   ],
   "source": [
    "query = bq.Query(q_all)\n",
    "tableresult = query.results()\n",
    "\n",
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([   25,    25,    32, ..., 79980, 79994, 79994]), array([24, 25,  6, ..., 27, 28, 29]))\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64))\n",
      "np.max= nan\n",
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([], dtype=int64), array([], dtype=int64))\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64))\n",
      "np.max= 461563.0\n"
     ]
    }
   ],
   "source": [
    "# This chunk does further wrangling to dataset to produce training and test sets.\n",
    "\n",
    "# Useful code to check NaN and Inf values. This is needed since these values would\n",
    "# cause \"Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "# errors when left unchecked.\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data))\n",
    "print \"np.max=\", np.max(abs(all_data))\n",
    "\n",
    "# Impute all NaN with numbers (not sure what to replace inf yet)\n",
    "all_data[np.isnan(all_data)] = 0\n",
    "# all_data[np.isinf(all_data)] = 0\n",
    "\n",
    "# See that NaN and Inf values replaced\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data))\n",
    "print \"np.max=\", np.max(abs(all_data))\n",
    "\n",
    "# Split the data into train and test sets.\n",
    "data_size = all_data.shape[0]\n",
    "training_size = data_size * 80/100\n",
    "indices = np.random.permutation(data_size)\n",
    "training_idx, test_idx = indices[:training_size], indices[training_size:]\n",
    "all_data_train, all_data_test = all_data[training_idx,:], all_data[test_idx,:]\n",
    "\n",
    "data_train = all_data_train[:,1:]\n",
    "targets_train = all_data_train[:,0]\n",
    "data_test = all_data_test[:,1:]\n",
    "targets_test = all_data_test[:,0]\n",
    "data_train_original = np.copy(data_train)\n",
    "data_test_original = np.copy(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# one_hot = OneHotEncoder(categorical_features=[0, 1, 14, 17, 20], n_values='auto')\n",
    "one_hot = OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False)\n",
    "one_hot.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 10, 10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.n_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7, 17, 27, 37])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.feature_indices_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Testing Algorithm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.451136351423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "class CustomRegressor(BaseEstimator, RegressorMixin):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, X, y):\n",
    "#     self.classes_, indices = np.unique([\"foo\", \"bar\", \"foo\"],\n",
    "#                                     return_inverse=True)\n",
    "#     self.majority_ = np.argmax(np.bincount(indices))\n",
    "    return self\n",
    "\n",
    "  def predict(self, X):\n",
    "    # 56: gap_1_slots_ago\n",
    "    # 58: gap_2_slots_ago\n",
    "    # 60: gap_3_slots_ago\n",
    "#     X = X.tocsr()\n",
    "#     v1 = coo_matrix(np.asmatrix(np.ones(X.shape[0])).T)\n",
    "    v1 = np.asmatrix(np.ones(X.shape[0]))\n",
    "    v2 = np.asmatrix((X[:, 23]*0.65+X[:, 25]*0.25+X[:, 27]*0.15)/2)\n",
    "    predictions = np.asarray(np.concatenate((v1, v2), axis=0).max(axis=0))\n",
    "    \n",
    "    return predictions\n",
    "  \n",
    "custom_est = CustomRegressor()\n",
    "custom_est.fit(data_train_original, data_test_original)\n",
    "custom_predictions = custom_est.predict(data_test_original)\n",
    "print(mape(targets_test, custom_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=110 .........\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=110 .........\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=110 .........\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=110 .........\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=110 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=110, score=-0.874537 -21.9min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=110, score=-0.886750 -21.9min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=110, score=-0.867296 -22.0min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=110, score=-0.878946 -22.5min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=110, score=-0.866456 -22.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done   1 jobs       | elapsed: 22.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=130, score=-0.863527 -29.7min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=130, score=-0.855026 -30.0min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=130, score=-0.862286 -30.4min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=130, score=-0.868889 -30.3min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=130, score=-0.846937 -30.4min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=150, score=-0.852371 -37.3min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=150, score=-0.849532 -37.5min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=150, score=-0.850113 -37.4min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=150, score=-0.862197 -37.8min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=150, score=-0.832220 -38.3min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=170, score=-0.837837 -40.5min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=170, score=-0.839305 -41.3min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=170, score=-0.847533 -41.9min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=170, score=-0.853956 -41.4min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=170, score=-0.833629 -41.6min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=190 .........\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "steps = [\n",
    "#   ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False)),\n",
    "#   ('impute', Imputer(0)),\n",
    "#   ('feature_selection', SelectKBest(f_classif)),\n",
    "#   ('pca', PCA(n_components=120)),\n",
    "  \n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, is removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('estimate', GradientBoostingRegressor())\n",
    "\n",
    "]\n",
    "\n",
    "est = Pipeline(steps)\n",
    "\n",
    "params = {\n",
    "#   'one_hot__n_values': [7, 10, 20],\n",
    "#   \"feature_selection__k\": [i for i in range(1, n_features - 1)]\n",
    "#   'estimate__max_features': [i for i in range(110, n_features, 10)],\n",
    "  'estimate__learning_rate': [0.1, 0.5, 1, 10],\n",
    "  'estimate__n_estimators': [i for i in range(110, n_features, 20)],\n",
    "#   'estimate__loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "# cross_validation_iter = StratifiedShuffleSplit(y=targets_train, test_size=0.3,\n",
    "#                                                random_state=RANDOM_STATE, n_iter=10)\n",
    "search_params = RandomizedSearchCV(\n",
    "  estimator=est,\n",
    "  param_distributions=params,\n",
    "  cv=5,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=2,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "search_params = GridSearchCV(\n",
    "  estimator=est,\n",
    "  param_grid=params,\n",
    "  cv=5,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=5,\n",
    "  verbose=3\n",
    ")\n",
    "\n",
    "search_params.fit(data_train_original, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_est = search_params.best_estimator_\n",
    "test_predictions = final_est.predict(data_test_original)\n",
    "print(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(final_est, open(EST_PICKLE_FILENAME, \"w\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "loss: 10.10\n",
      "sample predictions:\n",
      "[[  6.47371387e+00]\n",
      " [  9.99732614e-01]\n",
      " [  8.96018438e+04]\n",
      " [  9.99730945e-01]\n",
      " [  4.54779688e+04]\n",
      " [  2.72998505e+01]\n",
      " [  9.99730945e-01]\n",
      " [  3.97196693e+01]\n",
      " [  3.78494692e+00]\n",
      " [  9.99730945e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compare baseline with final algorithm\n",
    "\n",
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import time\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dropout\n",
    "\n",
    "HDF_FILENAME = 'final_model3.hdf5'\n",
    "JSON_MODEL_FILENAME = 'final_model.json'\n",
    "\n",
    "\n",
    "steps = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, is removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "#   ('pca', PCA(n_components=120)),\n",
    "#   ('estimate', final_model)\n",
    "]\n",
    "transformer = Pipeline(steps)\n",
    "data_test2 = transformer.fit_transform(data_test_original)\n",
    "\n",
    "\n",
    "\n",
    "def mape(y, predictions):\n",
    "  return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "def get_optimizer(epochs=50):\n",
    "  learning_rate = 0.2\n",
    "  decay_rate = learning_rate / epochs\n",
    "  momentum = 0.8\n",
    "  return SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "from keras.models import model_from_json\n",
    "json_file = open(JSON_MODEL_FILENAME, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(HDF_FILENAME)\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss=mape, optimizer=get_optimizer(epochs))\n",
    "\n",
    "score = loaded_model.evaluate(data_test2, targets_test, verbose=0)\n",
    "print \"%s: %.2f\" % (loaded_model.metrics_names[0], score)\n",
    "test_predictions2 = loaded_model.predict(data_test2)\n",
    "print 'sample predictions:'\n",
    "print(test_predictions2[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"Process Final Test Data With Final Algorithm\" to use pickled final algorithm against final test data to produce csv required by this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 169)\n",
      "[[  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   1.74300000e+03\n",
      "    2.07500000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   1.00000000e+00   0.00000000e+00 ...,   1.66000000e+02\n",
      "    7.47000000e+02   1.66000000e+02]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   1.90900000e+03\n",
      "    3.40300000e+03   0.00000000e+00]\n",
      " ..., \n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   7.30400000e+03\n",
      "    9.04700000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   5.89300000e+03\n",
      "    9.62800000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   7.30400000e+03\n",
      "    9.04700000e+03   0.00000000e+00]]\n",
      "(800, 169)\n",
      "[[  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   1.74300000e+03\n",
      "    2.07500000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   1.00000000e+00   0.00000000e+00 ...,   1.66000000e+02\n",
      "    7.47000000e+02   1.66000000e+02]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   1.90900000e+03\n",
      "    3.40300000e+03   0.00000000e+00]\n",
      " ..., \n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   7.30400000e+03\n",
      "    9.04700000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   5.89300000e+03\n",
      "    9.62800000e+03   0.00000000e+00]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   7.30400000e+03\n",
      "    9.04700000e+03   0.00000000e+00]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mape() takes exactly 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-127cf5008272>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: mape() takes exactly 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Just testing Imputer. Turns out somehow Imputer causes number of features reduced, weird.\n",
    "\n",
    "# imputer = Imputer()\n",
    "est = DecisionTreeRegressor(max_features=len(features))\n",
    "\n",
    "data_train_i = np.copy(data_train)\n",
    "print(data_train.shape)\n",
    "print(data_train[0:10])\n",
    "# data_train_i = imputer.fit_transform(data_train)\n",
    "data_train_i[np.isnan(data_train_i)] = 0\n",
    "data_train_i.astype('float32')\n",
    "print(data_train_i.shape)\n",
    "print(data_train_i[0:10])\n",
    "est.fit(data_train_i, targets_train)\n",
    "predictions = est.predict(data_test)\n",
    "print(mape(data_test, predictions, targets_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=112,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "           \n",
    "1000 training data, Score: 0.924\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=114,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "\n",
    "5000 training data, Score: 0.992\n",
    "\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=111,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "           \n",
    "20000 training data, Score: 1.001\n",
    "\n",
    "Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values=[7, 10, 10, 10], sparse=False)), ('scale', StandardScaler(copy=True, with_mean=T...s_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "           \n",
    "80000 training data, Score: 0.977 (submitted date 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
