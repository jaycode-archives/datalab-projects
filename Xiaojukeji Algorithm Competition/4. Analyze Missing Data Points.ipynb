{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tables import *\n",
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\ttimeofday_slot\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)\n",
    "\n",
    "\n",
    "datafile_path = 'xjk_pytable.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(train) file created, pass...\n",
      "(test) file doesn't exist, create table\n",
      "flushed\n",
      "\n",
      "filedata information:\n",
      "xjk_pytable.h5 (File) ''\n",
      "Last modif.: 'Sun Jul  3 11:39:49 2016'\n",
      "Object Tree: \n",
      "/ (RootGroup) ''\n",
      "/test (Group) 'Test tables'\n",
      "/test/gaps (Table(0,)) ''\n",
      "/train (Group) 'Training tables'\n",
      "/train/gaps (Table(0,)) ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RECREATE_TABLE = False\n",
    "\n",
    "# Training data\n",
    "\n",
    "def create_train_table(fileh, datafile_path, fields):\n",
    "  train = fileh.create_group('/', 'train', 'Training tables')\n",
    "  tabledef = {}\n",
    "  for field in fields:\n",
    "    tabledef[field] = Float64Col()\n",
    "  gaps = fileh.create_table(train, 'gaps', tabledef)\n",
    "  gaps.flush()\n",
    "  print \"flushed\"\n",
    "  return fileh\n",
    "\n",
    "try:\n",
    "  fileh = open_file(datafile_path, mode = 'r')\n",
    "  fileh.root.train.gaps\n",
    "  if RECREATE_TABLE:\n",
    "    print \"(train) recreate table\"\n",
    "    fileh.close()\n",
    "    fileh = open_file(datafile_path, mode = 'w')\n",
    "    fileh = create_train_table(fileh, datafile_path, fields)\n",
    "  else:\n",
    "    print \"(train) file created, pass...\"\n",
    "except:\n",
    "  print \"(train) file doesn't exist, create table\"\n",
    "  if 'fileh' in vars() or 'fileh' in globals():\n",
    "    fileh.close()\n",
    "  fileh = open_file(datafile_path, mode = 'a')\n",
    "  fileh = create_train_table(fileh, datafile_path, fields)\n",
    "\n",
    "fileh.close()\n",
    "\n",
    "# Test data\n",
    "\n",
    "def create_test_table(fileh, datafile_path, fields):\n",
    "  test = fileh.create_group('/', 'test', 'Test tables')\n",
    "  tabledef = {}\n",
    "  for field in fields:\n",
    "    tabledef[field] = Float64Col()\n",
    "  gaps = fileh.create_table(test, 'gaps', tabledef)\n",
    "  gaps.flush()\n",
    "  print \"flushed\"\n",
    "  return fileh\n",
    "\n",
    "try:\n",
    "  fileh = open_file(datafile_path, mode = 'r')\n",
    "  fileh.root.test.gaps\n",
    "  if RECREATE_TABLE:\n",
    "    print \"(test) recreate table\"\n",
    "    fileh.close()\n",
    "    fileh = open_file(datafile_path, mode = 'a')\n",
    "    fileh = create_test_table(fileh, datafile_path, fields)\n",
    "  else:\n",
    "    print \"(test) file created, pass...\"\n",
    "except:\n",
    "  print \"(test) file doesn't exist, create table\"\n",
    "  if 'fileh' in vars() or 'fileh' in globals():\n",
    "    fileh.close()\n",
    "  fileh = open_file(datafile_path, mode = 'a')\n",
    "  fileh = create_test_table(fileh, datafile_path, fields)\n",
    "\n",
    "print \"\\nfiledata information:\"\n",
    "print(fileh)\n",
    "fileh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gap, timeofday_slot, day_in_week, weather_1_slots_ago, weather_2_slots_ago, weather_3_slots_ago, tj_level1_1_slots_ago, tj_level2_1_slots_ago, tj_level3_1_slots_ago, tj_level4_1_slots_ago, tj_level1_2_slots_ago, tj_level2_2_slots_ago, tj_level3_2_slots_ago, tj_level4_2_slots_ago, tj_level1_3_slots_ago, tj_level2_3_slots_ago, tj_level3_3_slots_ago, tj_level4_3_slots_ago, temperature_1_slots_ago, pm25_1_slots_ago, temperature_2_slots_ago, pm25_2_slots_ago, temperature_3_slots_ago, pm25_3_slots_ago, gap_1_slots_ago, sum_price_1_slots_ago, gap_2_slots_ago, sum_price_2_slots_ago, gap_3_slots_ago, sum_price_3_slots_ago, f1, f11, f11_1, f11_2, f11_3, f11_4, f11_5, f11_6, f11_7, f11_8, f13_4, f13_8, f14, f14_1, f14_10, f14_2, f14_3, f14_6, f14_8, f15, f15_1, f15_2, f15_3, f15_4, f15_6, f15_7, f15_8, f16, f16_1, f16_10, f16_11, f16_12, f16_3, f16_4, f16_6, f17, f17_2, f17_3, f17_4, f17_5, f19, f19_1, f19_2, f19_3, f19_4, f1_1, f1_10, f1_11, f1_2, f1_3, f1_4, f1_5, f1_6, f1_7, f1_8, f20, f20_1, f20_2, f20_4, f20_5, f20_6, f20_7, f20_8, f20_9, f21_1, f21_2, f22, f22_1, f22_2, f22_3, f22_4, f22_5, f23, f23_1, f23_2, f23_3, f23_4, f23_5, f23_6, f24, f24_1, f24_2, f24_3, f25, f25_1, f25_3, f25_7, f25_8, f25_9, f2_1, f2_10, f2_11, f2_12, f2_13, f2_2, f2_4, f2_5, f2_6, f2_7, f2_8, f3_1, f3_2, f3_3, f4, f4_1, f4_10, f4_11, f4_13, f4_14, f4_16, f4_17, f4_18, f4_2, f4_3, f4_5, f4_6, f4_7, f4_8, f4_9, f5, f5_1, f5_3, f5_4, f6, f6_1, f6_2, f6_4, f7, f8, f8_1, f8_2, f8_3, f8_4, f8_5\n"
     ]
    }
   ],
   "source": [
    "print ', '.join(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 102592 rows\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'no such column: timeofday_slot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-06618603ac97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'-n 1 -r 1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'\\nfileh1 = open_file(datafile_path, mode = \\'r\\')\\n\\nif fileh1.root.train.gaps.nrows == 0:\\n  query = bq.Query(q_all)\\n  tableresult = query.results()\\n  fileh1.close()\\n  fileh1 = open_file(datafile_path, mode = \\'a\\')\\n  gaps_table = fileh1.root.train.gaps\\n\\n  gap = gaps_table.row\\n  print \\'there are {} rows\\'.format(tableresult.length)\\n  for rcounter, row in enumerate(tableresult):\\n    for field in fields:\\n      gap[field] = row[field]\\n    gap.append()\\n    if rcounter % 5000 == 0:\\n      print \\'processed {} rows\\'.format(rcounter)\\n  gaps_table.flush()\\n  \\nelse:\\n  print \"datafile not empty, pass...\"\\n\\nfileh1.close()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/gcp/datalab/__init__.pyc\u001b[0m in \u001b[0;36m_run_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# IPython will complain if cell is empty string but not if it is None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_orig_run_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[0m_shell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveShell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_cell_magic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m         \u001b[0mall_runs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m         \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/timeit.pyc\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(self, repeat, number)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;32mtables/tableextension.pyx\u001b[0m in \u001b[0;36mtables.tableextension.Row.__setitem__ (tables/tableextension.c:17072)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mtables/tableextension.pyx\u001b[0m in \u001b[0;36mtables.tableextension.get_nested_field_cache (tables/tableextension.c:2412)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mtables/utilsextension.pyx\u001b[0m in \u001b[0;36mtables.utilsextension.get_nested_field (tables/utilsextension.c:8604)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'no such column: timeofday_slot'"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "fileh1 = open_file(datafile_path, mode = 'r')\n",
    "\n",
    "if fileh1.root.train.gaps.nrows == 0:\n",
    "  query = bq.Query(q_all)\n",
    "  tableresult = query.results()\n",
    "  fileh1.close()\n",
    "  fileh1 = open_file(datafile_path, mode = 'a')\n",
    "  gaps_table = fileh1.root.train.gaps\n",
    "\n",
    "  gap = gaps_table.row\n",
    "  print 'there are {} rows'.format(tableresult.length)\n",
    "  for rcounter, row in enumerate(tableresult):\n",
    "    for field in fields:\n",
    "      gap[field] = row[field]\n",
    "    gap.append()\n",
    "    if rcounter % 5000 == 0:\n",
    "      print 'processed {} rows'.format(rcounter)\n",
    "  gaps_table.flush()\n",
    "  \n",
    "else:\n",
    "  print \"datafile not empty, pass...\"\n",
    "\n",
    "fileh1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileh1 = open_file(datafile_path, mode = 'r')\n",
    "\n",
    "object = fileh1.get_node('/train', 'gaps')\n",
    "object_array_data = object.read()\n",
    "print fileh1.root.train.gaps.nrows\n",
    "fileh1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample, should return nan\n",
    "object_array_data[5]['gap_1_slots_ago']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "# Convert to vectorized array that we can use in further processing.\n",
    "all_data = np.zeros((object_array_data.shape[0], len(fields)))\n",
    "print 'there are {} rows'.format(object_array_data.shape[0])\n",
    "for rcounter, row in enumerate(object_array_data):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This chunk does further wrangling to dataset to produce training and test sets.\n",
    "\n",
    "# Useful code to check NaN and Inf values. This is needed since these values would\n",
    "# cause \"Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "# errors when left unchecked.\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data_original))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data_original))\n",
    "print \"np.max=\", np.max(abs(all_data_original))\n",
    "\n",
    "# Impute all NaN with numbers (not sure what to replace inf yet)\n",
    "# all_data[np.isnan(all_data_original)] = 0\n",
    "# all_data[np.isinf(all_data)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration - Find NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is how to get position of NaNs\n",
    "\n",
    "nulls = np.isnan(all_data_original)\n",
    "nullspos = np.column_stack(np.where(nulls==True))\n",
    "nullspos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [[np.NaN, 1, 2, 3],\n",
    "     [1, 2, 3, np.NaN]]\n",
    "xn = np.isnan(x)\n",
    "xnp = np.column_stack(np.where(xn==True))\n",
    "xnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "print \"total data points:\", (all_data_original.shape[0] * all_data_original.shape[1])\n",
    "print \"number of missing values:\", nullspos.shape[0]\n",
    "missing_features = itemgetter(*np.unique(nullspos[:,1]).tolist())(fields)\n",
    "missing_features_table = pd.DataFrame(columns=['id', 'field', 'missing data points'])\n",
    "\n",
    "for id, field in enumerate(fields):\n",
    "  total_missing = len(np.where(nullspos[:,1]==id)[0])\n",
    "  if total_missing > 0:\n",
    "    missing_features_table = missing_features_table.append({\n",
    "        'id': id,\n",
    "        'field': field,\n",
    "        'missing data points': total_missing\n",
    "      }, ignore_index=True)\n",
    "missing_features_table['missing data points'] = \\\n",
    "  missing_features_table['missing data points'].astype('int64')\n",
    "missing_features_table['id'] = \\\n",
    "  missing_features_table['id'].astype('int64')\n",
    "missing_features_table.sort_values(['missing data points', 'id'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "def rand_jitter(arr):\n",
    "    stdev = .005*(max(arr)-min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev\n",
    "_ = plt.scatter(nullspos[:,0], rand_jitter(nullspos[:,1]), s=0.5)\n",
    "_ = plt.title('Missing Data Points')\n",
    "_ = plt.ylabel('Feature ID')\n",
    "_ = plt.xlabel('Observation ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-check with dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT COUNT(*) AS count FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE IS_NAN(sum_price_1_slots_ago) = true\n",
    "AND gap > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine observation with NaN value in detail\n",
    "\n",
    "In this step we are analyzing NaN value from one row to find out what the problem exactly was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE IS_NAN(sum_price_1_slots_ago) = true\n",
    "AND gap > 0\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there really was no orders at all 1 slot before timeslot 2016-01-18-72, that caused its `sum_price_1_slots_ago` to have a NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT COUNT(*) AS num_of_orders, SUM(price) AS total_price\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.orders] AS orders\n",
    "LEFT JOIN [datalab-projects-1331:xjk_algo_comp.districts] AS districts\n",
    "  ON orders.start_district_hash = districts.district_hash\n",
    "WHERE timeslot = '2016-01-18-71' AND district_id = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, now we can conclude that in this case it is correct to change orders and price to 0. How about weather?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE IS_NAN(weather_1_slots_ago) = true\n",
    "AND gap > 0\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM [datalab-projects-1331:xjk_algo_comp.weather]\n",
    "WHERE timeslot IN('2016-01-21-104', '2016-01-21-103', '2016-01-21-102')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no weather information on all three previous timeslots, which caused the missing values. I assumed traffic should have the same deal. I thought it was more appropriate to not including observations where there were no weather or traffic information, but let's check whether test dataset also have missing data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Missing Data Points in Test Data\n",
    "\n",
    "This step was needed to decide whether it was more appropriate to either include the observations with missing weather and/or traffic data or exclude them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data - Analyze Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *\n",
    "FROM [datalab-projects-1331:xjk_algo_comp_test.gaps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "fileh1 = open_file(datafile_path, mode = 'r')\n",
    "\n",
    "if fileh1.root.test.gaps.nrows == 0:\n",
    "  query = bq.Query(q_all)\n",
    "  tableresult = query.results()\n",
    "  fileh1.close()\n",
    "  fileh1 = open_file(datafile_path, mode = 'a')\n",
    "  gaps_table = fileh1.root.test.gaps\n",
    "\n",
    "  gap = gaps_table.row\n",
    "  print 'there are {} rows'.format(tableresult.length)\n",
    "  for rcounter, row in enumerate(tableresult):\n",
    "    for field in fields:\n",
    "      gap[field] = row[field]\n",
    "    gap.append()\n",
    "    if rcounter % 1000 == 0:\n",
    "      print 'processed {} rows'.format(rcounter)\n",
    "  gaps_table.flush()\n",
    "  \n",
    "else:\n",
    "  print \"datafile not empty, pass...\"\n",
    "\n",
    "fileh1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileh1 = open_file(datafile_path, mode = 'r')\n",
    "\n",
    "object = fileh1.get_node('/test', 'gaps')\n",
    "object_array_data = object.read()\n",
    "print fileh1.root.test.gaps.nrows\n",
    "fileh1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "# Convert to vectorized array that we can use in further processing.\n",
    "all_data = np.zeros((object_array_data.shape[0], len(fields)))\n",
    "print 'there are {} rows'.format(object_array_data.shape[0])\n",
    "for rcounter, row in enumerate(object_array_data):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 1000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This chunk does further wrangling to dataset to produce training and test sets.\n",
    "\n",
    "# Useful code to check NaN and Inf values. This is needed since these values would\n",
    "# cause \"Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "# errors when left unchecked.\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data_original))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data_original))\n",
    "print \"np.max=\", np.max(abs(all_data_original))\n",
    "\n",
    "# Impute all NaN with numbers (not sure what to replace inf yet)\n",
    "all_data[np.isnan(all_data_original)] = 0\n",
    "# all_data[np.isinf(all_data)] = 0\n",
    "\n",
    "# See that NaN and Inf values replaced\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data))\n",
    "print \"np.max=\", np.max(abs(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is how to get position of NaNs\n",
    "\n",
    "nulls = np.isnan(all_data_original)\n",
    "nullspos = np.column_stack(np.where(nulls==True))\n",
    "nullspos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "print \"total data points:\", (all_data_original.shape[0] * all_data_original.shape[1])\n",
    "print \"number of missing values:\", nullspos.shape[0]\n",
    "missing_features = itemgetter(*np.unique(nullspos[:,1]).tolist())(fields)\n",
    "missing_features_table = pd.DataFrame(columns=['id', 'field', 'missing data points'])\n",
    "\n",
    "for id, field in enumerate(fields):\n",
    "  total_missing = len(np.where(nullspos[:,1]==id)[0])\n",
    "  if total_missing > 0:\n",
    "    missing_features_table = missing_features_table.append({\n",
    "        'id': id,\n",
    "        'field': field,\n",
    "        'missing data points': total_missing\n",
    "      }, ignore_index=True)\n",
    "missing_features_table['missing data points'] = \\\n",
    "  missing_features_table['missing data points'].astype('int64')\n",
    "missing_features_table['id'] = \\\n",
    "  missing_features_table['id'].astype('int64')\n",
    "missing_features_table.sort_values(['missing data points', 'id'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "def rand_jitter(arr):\n",
    "    stdev = .005*(max(arr)-min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev\n",
    "_ = plt.scatter(nullspos[:,0], rand_jitter(nullspos[:,1]), s=1)\n",
    "_ = plt.title('Missing Data Points')\n",
    "_ = plt.ylabel('Feature ID')\n",
    "_ = plt.xlabel('Observation ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see one of them in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM [datalab-projects-1331:xjk_algo_comp_test.gaps]\n",
    "WHERE IS_NAN(gap_1_slots_ago) = true\n",
    "  AND IS_NAN(gap_2_slots_ago) = true AND IS_NAN(gap_3_slots_ago) = true\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, `2016-01-27-46` is one of the test data to predict alright, as analyzed previously. Let's see orders from its past three slots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT COUNT(*) AS num_of_orders, SUM(price) AS total_price\n",
    "FROM [datalab-projects-1331:xjk_algo_comp_test.orders] AS orders\n",
    "LEFT JOIN [datalab-projects-1331:xjk_algo_comp.districts] AS districts\n",
    "  ON orders.start_district_hash = districts.district_hash\n",
    "WHERE timeslot IN ('2016-01-18-45', '2016-01-18-44', '2016-01-18-43') AND district_id = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no order at these days, so we can confirm that these missing values should be replaced with zeroes. Let's analyze weather and traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data - Analyze Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM [datalab-projects-1331:xjk_algo_comp_test.gaps]\n",
    "WHERE IS_NAN(weather_1_slots_ago) = true\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When was the closest date (from `2016-01-23-58`) where weather information exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module weather_by_timeslot\n",
    "SELECT FIRST(timeslot), COUNT(*) AS num_weather FROM [datalab-projects-1331:xjk_algo_comp_test.weather]\n",
    "WHERE date IN ('2016-01-23')\n",
    "GROUP BY date, timeofday_slot\n",
    "ORDER BY date, timeofday_slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%chart bars --data weather_by_timeslot\n",
    "title: Number of weather information by timeslots for date 2016-01-23\n",
    "height: 500\n",
    "legend:\n",
    "  textStyle:\n",
    "    fontSize: 12\n",
    "hAxis:\n",
    "  format: \"#\"\n",
    "  ticks: [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closest was from slot 43 it seems. By the way, did weather from slot 43 correctly used for 44 and 45? Let's see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT weather\n",
    "FROM [datalab-projects-1331:xjk_algo_comp_test.weather]\n",
    "WHERE timeslot = '2016-01-23-43'\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT weather_1_slots_ago, weather_2_slots_ago, weather_3_slots_ago\n",
    "FROM [datalab-projects-1331:xjk_algo_comp_test.gaps]\n",
    "WHERE district_id = 1 AND timeslot = '2016-01-23-46'\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, confirmed. The big question here is, is it better to assume that weather doesn't change from previous readings? I don't currently know the answer, but I do know that it is inappropriate to set weather types, temperatures, and pollution levels to 0.\n",
    "\n",
    "There are two ways to handle this:\n",
    "1. Use past readings, or\n",
    "2. Remove observations with missing weather information.\n",
    "\n",
    "Let's see how weather information correlate with `gap` when handled by each method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Past Readings\n",
    "\n",
    "Let's see how weather types, temperatures, and pollution levels change throughout the day. We need this to decide whether to impute missing values with past readings directly or should we use means or other methods. We shall use training data for this since it has arguably more observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module weather_line\n",
    "SELECT FIRST(weather.timeslot) AS timeslot, FIRST(weather) AS weather,\n",
    "  FIRST(temperature) AS temperature, FIRST(pm25) AS pm25, SUM(gap) AS gap\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.weather] AS weather\n",
    "  LEFT JOIN [datalab-projects-1331:xjk_algo_comp.gaps] AS gaps \n",
    "    ON gaps.timeslot = weather.timeslot\n",
    "GROUP BY weather.date, weather.timeofday_slot\n",
    "ORDER BY weather.date, weather.timeofday_slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%chart line --data weather_line --fields timeslot,weather,gap\n",
    "height: 500\n",
    "vAxis:\n",
    "  scaleType: log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be that many variations with weather types, and I don't think it is appropriate anyway to get mean of categorical data (`weather` is categorical), so in this case we can use previous reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%chart line --data weather_line --fields timeslot,temperature,gap\n",
    "{\n",
    "  \"height\": 500,\n",
    "  \"vAxis\": {\n",
    "    \"scaleType\": \"log\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%chart line --data weather_line --fields timeslot,pm25,gap\n",
    "{\n",
    "  \"height\": 500,\n",
    "  \"vAxis\": {\n",
    "    \"scaleType\": \"log\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be any correlation at all between weather types, temperatures, and pollution levels, with gap. Considering all the missing data as well, let's just not use this in model creation at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tried to plot trendline with pylab but didn't work\n",
    "\n",
    "# import pylab\n",
    "\n",
    "# df = bq.Query(weather_line).to_dataframe()\n",
    "\n",
    "# # plot the data itself\n",
    "# x = range(len(df['timeslot']))\n",
    "# y = df['gap']\n",
    "# pylab.xticks(x, df['timeslot'])\n",
    "# pylab.plot(x,y)\n",
    "\n",
    "# # calc the trendline\n",
    "# z = np.polyfit(x, y, 1)\n",
    "# p = np.poly1d(z)\n",
    "# pylab.plot(x,p(x),\"r--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do similar analysis with traffic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data - Analyze Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM [datalab-projects-1331:xjk_algo_comp_test.gaps]\n",
    "WHERE IS_NAN(tj_level1_1_slots_ago) = true AND IS_NAN(tj_level1_2_slots_ago) = true\n",
    "  AND IS_NAN(tj_level1_3_slots_ago) = true\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module traffic_by_timeslot\n",
    "SELECT FIRST(timeslot), COUNT(*) AS num_traffic\n",
    "FROM [datalab-projects-1331:xjk_algo_comp_test.traffic] AS traffic\n",
    "JOIN [datalab-projects-1331:xjk_algo_comp_test.districts] AS districts\n",
    "  ON traffic.district_hash = districts.district_hash\n",
    "WHERE date IN ('2016-01-29') AND district_id = 54\n",
    "GROUP BY date, timeofday_slot, district_id\n",
    "ORDER BY date, timeofday_slot, district_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%chart bars --data traffic_by_timeslot\n",
    "title: Number of traffic information by timeslots\n",
    "height: 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like district 54 had no traffic information on that date. Let's see if this district had any traffic information at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT FIRST(timeslot) AS timeslot, COUNT(*) AS num_traffic\n",
    "FROM [datalab-projects-1331:xjk_algo_comp_test.traffic] AS traffic\n",
    "JOIN [datalab-projects-1331:xjk_algo_comp_test.districts] AS districts\n",
    "  ON traffic.district_hash = districts.district_hash\n",
    "WHERE district_id = 54\n",
    "GROUP BY date, timeofday_slot\n",
    "ORDER BY date, timeofday_slot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, seems like district 54 never had any traffic information. What other districts do not have traffic information as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT district_id, COUNT(*) AS num_traffic\n",
    "FROM [datalab-projects-1331:xjk_algo_comp_test.traffic] AS traffic\n",
    "RIGHT OUTER JOIN EACH [datalab-projects-1331:xjk_algo_comp_test.districts] AS districts\n",
    "  ON traffic.district_hash = districts.district_hash\n",
    "GROUP BY district_id\n",
    "ORDER BY num_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one row, what happened? Turns out it is an empty row as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM [datalab-projects-1331:xjk_algo_comp_test.traffic] AS traffic\n",
    "RIGHT JOIN EACH [datalab-projects-1331:xjk_algo_comp_test.districts] AS districts\n",
    "  ON traffic.district_hash = districts.district_hash\n",
    "WHERE district_id = 54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about in training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT district_id, COUNT(*) AS num_traffic\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.traffic] AS traffic\n",
    "RIGHT OUTER JOIN EACH [datalab-projects-1331:xjk_algo_comp_test.districts] AS districts\n",
    "  ON traffic.district_hash = districts.district_hash\n",
    "GROUP BY district_id\n",
    "ORDER BY num_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion Thus Far\n",
    "\n",
    "For traffic information, tn test data district 54 has none of it and district 15 has 4 missing traffic information. In training data, district 54 has no traffic information, 15 has 153 missing information, 63 has 13, and 32 has 1.\n",
    "\n",
    "Perhaps it is better not to include district 54 when training the model, or train it separately.\n",
    "\n",
    "There was a prevalent trend where weather information was missing from test data, as it had been seen on training data, but this trend is irrelevant since there was no correlation between weather and gaps. Further analysis may show different conclusion, but let's not include weather information at all for now.\n",
    "\n",
    "I suspect that these missing traffic and weather info on some districts caused the pattern when these missing data points were plotted. We shall confirm this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Missing Data\n",
    "In the next couple of sections, we will see how the handling of missing data will affect performance.\n",
    "\n",
    "We will do a quick spot-checking, that is to create a baseline model with fast-running algorithm, then compare its performance with and without preprocessing missing data as explained above.\n",
    "\n",
    "The data will be tested with DecisionTree algorithm, and it will be cross-validated with 10-kfold splits. We won't do any grid search at this step as we only want to see if missing data handling works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store list of fields in a Pickle object so we don't have to rewrite everything.\n",
    "try:\n",
    "  import cPickle as pickle\n",
    "except:\n",
    "  import pickle\n",
    "\n",
    "FIELDS_PICKLE = 'fields-4.pkl'\n",
    "pickle.dump(fields, open(FIELDS_PICKLE, \"w\") )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
