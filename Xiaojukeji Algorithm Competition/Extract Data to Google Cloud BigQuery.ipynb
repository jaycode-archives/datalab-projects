{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import gcp\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from cStringIO import StringIO\n",
    "import gzip\n",
    "import os\n",
    "import glob\n",
    "BQ_DATASET_NAME = 'datalab-projects-1331:xjk_algo_comp'\n",
    "dataset = bq.DataSet(BQ_DATASET_NAME)\n",
    "\n",
    "project = gcp.Context.default().project_id\n",
    "bucket_name = project + '-datalab'\n",
    "bucket_path = 'gs://' + bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql --module districts\n",
    "SELECT *\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.districts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_districts = bq.Query(districts).to_dataframe()\n",
    "df_districts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and extract data files from cloud storage\n",
    "Data files are stored within a compressed file in Google Cloud Storage (GCS). We need to download it, extract the data files into local drive, then upload them back into GCS before being used in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tar_filename = 'citydata.tar'\n",
    "datadir = 'season_1'\n",
    "\n",
    "# If data has not been extracted, extract it.\n",
    "if not os.path.isdir(datadir):\n",
    "  # If citydata.tar has not been downloaded, download it.\n",
    "  if not os.path.isfile(tar_filename):\n",
    "    \n",
    "    # Import zip file from Google Cloud Storage\n",
    "    bucket_object = bucket_path + '/data/citydata.tar.gz'\n",
    "    print 'Bucket: ' + bucket_path\n",
    "    print 'Object: ' + bucket_object\n",
    "\n",
    "    %storage read --object $bucket_object --variable compressed_file\n",
    "    \n",
    "    gzip_file = gzip.GzipFile(fileobj=StringIO(compressed_file))\n",
    "    \n",
    "    del compressed_file\n",
    "\n",
    "    import shutil\n",
    "    with open(tar_filename, 'wb') as f_out:\n",
    "      shutil.copyfileobj(gzip_file, f_out)\n",
    "\n",
    "  import tarfile\n",
    "  tar = tarfile.open(tar_filename, \"r\")\n",
    "  tar.extractall()\n",
    "  tar.close()\n",
    "  os.remove(tar_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy all files to GCS and load them into BigQuery\n",
    "\n",
    "In here we will copy the files currently locally stored to GCS then tell BigQuery to load them. BigQuery can't load directly from Datalab it seems so this is a workaround we need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(localpath, storagepath, table):\n",
    "  '''Copy data file located in local path to GCS storagepath, then into given BigQuery table.\n",
    "  \n",
    "  Args:\n",
    "    localpath(string): Local data file path.\n",
    "    storagepath(string): Google Cloud Storage's data file path \n",
    "                         e.g. \"gs://project_name/data/datafile.csv\"\n",
    "    table(gcp.bigquery.Table): BigQuery table's `instance\n",
    "      <http://googlecloudplatform.github.io/datalab/gcp.bigquery.html#gcp.bigquery.Table>`.\n",
    "  '''\n",
    "  fo = open(localpath, 'rb')\n",
    "  file_str = fo.read()\n",
    "  fo.close()\n",
    "  bucketpath = '/'.join(storagepath.split('/')[0:3])\n",
    "  keypath = '/'.join(storagepath.split('/')[3:])\n",
    "  item = gcp.storage.Item(bucketpath, keypath, context=gcp.Context.default())\n",
    "  item.write_to(file_str, 'text/plain')\n",
    "  %storage write -v file_str -o $storagepath\n",
    "#   tablename = '{}:{}.{}'.format(table.name.project_id, table.name.dataset_id, table.name.table_id)\n",
    "#   %bigquery load -m append -f csv -S $storagepath -D $tablename -i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = bq.Table(BQ_DATASET_NAME + '.districts')\n",
    "\n",
    "# Create or overwrite the existing table if it exists\n",
    "table_schema = bq.Schema([{'name': 'district_hash', 'type': 'STRING'},\n",
    "                          {'name': 'district_id', 'type': 'INTEGER'}])\n",
    "table.create(schema = table_schema, overwrite = True)\n",
    "\n",
    "path = r'season_1/training_data/cluster_map/cluster_map'\n",
    "\n",
    "load(path, os.path.join(bucket_path, 'data', path), table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Orders\n",
    "Loading orders data. Orders data are stored one day per csv file and we need to keep them all in one big **orders** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module orders_count\n",
    "SELECT count(*)\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.orders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bigquery load -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = r'data/season_1/training_data/order_data/order_data_2016-01-16'\n",
    "orders_path = os.path.join(bucket_path, path)\n",
    "print(orders_path)\n",
    "dest = 'datalab-projects-1331:xjk_algo_comp.orders'\n",
    "%bigquery load -m append -f csv -S $orders_path -D $dest -i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gsutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = bq.Table(BQ_DATASET_NAME + '.orders')\n",
    "\n",
    "# Create or overwrite the existing table if it exists\n",
    "table_schema = bq.Schema([{'name': 'order_id', 'type': 'STRING'},\n",
    "                          {'name': 'driver_id', 'type': 'STRING'},\n",
    "                          {'name': 'passenger_id', 'type': 'STRING'},\n",
    "                          {'name': 'start_district_hash', 'type': 'STRING'},\n",
    "                          {'name': 'dest_district_hash', 'type': 'STRING'},\n",
    "                          {'name': 'price', 'type': 'FLOAT'},\n",
    "                          {'name': 'time', 'type': 'TIMESTAMP'}])\n",
    "table.create(schema = table_schema, overwrite = True)\n",
    "\n",
    "path = r'season_1/training_data/order_data'\n",
    "all_files = glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "list_html = %storage list -o $bucket_path\n",
    "list_str = list_html.data\n",
    "\n",
    "for file_ in all_files:\n",
    "  fo = open(file_, 'rb')\n",
    "  file_str = fo.read()\n",
    "  fo.close()\n",
    "  datafile_path = os.path.join(bucket_path, file_)\n",
    "  \n",
    "  # If object does not exist...\n",
    "  filename = os.path.basename(file_)\n",
    "  print('reading', file_)\n",
    "  if filename not in list_str: \n",
    "    # Upload to storage\n",
    "    %storage write -v file_str -o $datafile_path\n",
    "\n",
    "  %bigquery load -m append -S source -D orders --dataset xjk_algo_comp\n",
    "#     df_ = pd.read_csv(file_,index_col=0, sep='\\t',\n",
    "#                       parse_dates=[6], date_parser=dateparse,\n",
    "#   #                     converters={6:dateparse},\n",
    "#                       header=None, names=['order_id', 'driver_id', 'passenger_id',\n",
    "#                                           'start_district_hash', 'dest_district_hash',\n",
    "#                                           'price', 'time'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = 'hello'\n",
    "path = os.path.join(bucket_path, 'zing', 'test.txt')\n",
    "print(path)\n",
    "%storage write -v test -o $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bigquery load -h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
