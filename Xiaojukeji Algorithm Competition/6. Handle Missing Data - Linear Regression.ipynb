{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 163\n",
      "Fields:\n",
      "0. gap\n",
      "1. timeofday_slot\n",
      "2. day_in_week\n",
      "3. weather_1_slots_ago\n",
      "4. weather_2_slots_ago\n",
      "5. weather_3_slots_ago\n",
      "6. tj_level1_1_slots_ago\n",
      "7. tj_level2_1_slots_ago\n",
      "8. tj_level3_1_slots_ago\n",
      "9. tj_level4_1_slots_ago\n",
      "10. tj_level1_2_slots_ago\n",
      "11. tj_level2_2_slots_ago\n",
      "12. tj_level3_2_slots_ago\n",
      "13. tj_level4_2_slots_ago\n",
      "14. tj_level1_3_slots_ago\n",
      "15. tj_level2_3_slots_ago\n",
      "16. tj_level3_3_slots_ago\n",
      "17. tj_level4_3_slots_ago\n",
      "18. temperature_1_slots_ago\n",
      "19. pm25_1_slots_ago\n",
      "20. temperature_2_slots_ago\n",
      "21. pm25_2_slots_ago\n",
      "22. temperature_3_slots_ago\n",
      "23. pm25_3_slots_ago\n",
      "24. gap_1_slots_ago\n",
      "25. sum_price_1_slots_ago\n",
      "26. gap_2_slots_ago\n",
      "27. sum_price_2_slots_ago\n",
      "28. gap_3_slots_ago\n",
      "29. sum_price_3_slots_ago\n",
      "30. f1\n",
      "31. f11\n",
      "32. f11_1\n",
      "33. f11_2\n",
      "34. f11_3\n",
      "35. f11_4\n",
      "36. f11_5\n",
      "37. f11_6\n",
      "38. f11_7\n",
      "39. f11_8\n",
      "40. f13_4\n",
      "41. f13_8\n",
      "42. f14\n",
      "43. f14_1\n",
      "44. f14_10\n",
      "45. f14_2\n",
      "46. f14_3\n",
      "47. f14_6\n",
      "48. f14_8\n",
      "49. f15\n",
      "50. f15_1\n",
      "51. f15_2\n",
      "52. f15_3\n",
      "53. f15_4\n",
      "54. f15_6\n",
      "55. f15_7\n",
      "56. f15_8\n",
      "57. f16\n",
      "58. f16_1\n",
      "59. f16_10\n",
      "60. f16_11\n",
      "61. f16_12\n",
      "62. f16_3\n",
      "63. f16_4\n",
      "64. f16_6\n",
      "65. f17\n",
      "66. f17_2\n",
      "67. f17_3\n",
      "68. f17_4\n",
      "69. f17_5\n",
      "70. f19\n",
      "71. f19_1\n",
      "72. f19_2\n",
      "73. f19_3\n",
      "74. f19_4\n",
      "75. f1_1\n",
      "76. f1_10\n",
      "77. f1_11\n",
      "78. f1_2\n",
      "79. f1_3\n",
      "80. f1_4\n",
      "81. f1_5\n",
      "82. f1_6\n",
      "83. f1_7\n",
      "84. f1_8\n",
      "85. f20\n",
      "86. f20_1\n",
      "87. f20_2\n",
      "88. f20_4\n",
      "89. f20_5\n",
      "90. f20_6\n",
      "91. f20_7\n",
      "92. f20_8\n",
      "93. f20_9\n",
      "94. f21_1\n",
      "95. f21_2\n",
      "96. f22\n",
      "97. f22_1\n",
      "98. f22_2\n",
      "99. f22_3\n",
      "100. f22_4\n",
      "101. f22_5\n",
      "102. f23\n",
      "103. f23_1\n",
      "104. f23_2\n",
      "105. f23_3\n",
      "106. f23_4\n",
      "107. f23_5\n",
      "108. f23_6\n",
      "109. f24\n",
      "110. f24_1\n",
      "111. f24_2\n",
      "112. f24_3\n",
      "113. f25\n",
      "114. f25_1\n",
      "115. f25_3\n",
      "116. f25_7\n",
      "117. f25_8\n",
      "118. f25_9\n",
      "119. f2_1\n",
      "120. f2_10\n",
      "121. f2_11\n",
      "122. f2_12\n",
      "123. f2_13\n",
      "124. f2_2\n",
      "125. f2_4\n",
      "126. f2_5\n",
      "127. f2_6\n",
      "128. f2_7\n",
      "129. f2_8\n",
      "130. f3_1\n",
      "131. f3_2\n",
      "132. f3_3\n",
      "133. f4\n",
      "134. f4_1\n",
      "135. f4_10\n",
      "136. f4_11\n",
      "137. f4_13\n",
      "138. f4_14\n",
      "139. f4_16\n",
      "140. f4_17\n",
      "141. f4_18\n",
      "142. f4_2\n",
      "143. f4_3\n",
      "144. f4_5\n",
      "145. f4_6\n",
      "146. f4_7\n",
      "147. f4_8\n",
      "148. f4_9\n",
      "149. f5\n",
      "150. f5_1\n",
      "151. f5_3\n",
      "152. f5_4\n",
      "153. f6\n",
      "154. f6_1\n",
      "155. f6_2\n",
      "156. f6_4\n",
      "157. f7\n",
      "158. f8\n",
      "159. f8_1\n",
      "160. f8_2\n",
      "161. f8_3\n",
      "162. f8_4\n",
      "163. f8_5\n",
      "Number of features: 163\n",
      "Fields:\n",
      "0. gap\n",
      "1. timeofday_slot\n",
      "2. day_in_week\n",
      "3. weather_1_slots_ago\n",
      "4. weather_2_slots_ago\n",
      "5. weather_3_slots_ago\n",
      "6. tj_level1_1_slots_ago\n",
      "7. tj_level2_1_slots_ago\n",
      "8. tj_level3_1_slots_ago\n",
      "9. tj_level4_1_slots_ago\n",
      "10. tj_level1_2_slots_ago\n",
      "11. tj_level2_2_slots_ago\n",
      "12. tj_level3_2_slots_ago\n",
      "13. tj_level4_2_slots_ago\n",
      "14. tj_level1_3_slots_ago\n",
      "15. tj_level2_3_slots_ago\n",
      "16. tj_level3_3_slots_ago\n",
      "17. tj_level4_3_slots_ago\n",
      "18. temperature_1_slots_ago\n",
      "19. pm25_1_slots_ago\n",
      "20. temperature_2_slots_ago\n",
      "21. pm25_2_slots_ago\n",
      "22. temperature_3_slots_ago\n",
      "23. pm25_3_slots_ago\n",
      "24. gap_1_slots_ago\n",
      "25. sum_price_1_slots_ago\n",
      "26. gap_2_slots_ago\n",
      "27. sum_price_2_slots_ago\n",
      "28. gap_3_slots_ago\n",
      "29. sum_price_3_slots_ago\n",
      "30. f1\n",
      "31. f11\n",
      "32. f11_1\n",
      "33. f11_2\n",
      "34. f11_3\n",
      "35. f11_4\n",
      "36. f11_5\n",
      "37. f11_6\n",
      "38. f11_7\n",
      "39. f11_8\n",
      "40. f13_4\n",
      "41. f13_8\n",
      "42. f14\n",
      "43. f14_1\n",
      "44. f14_10\n",
      "45. f14_2\n",
      "46. f14_3\n",
      "47. f14_6\n",
      "48. f14_8\n",
      "49. f15\n",
      "50. f15_1\n",
      "51. f15_2\n",
      "52. f15_3\n",
      "53. f15_4\n",
      "54. f15_6\n",
      "55. f15_7\n",
      "56. f15_8\n",
      "57. f16\n",
      "58. f16_1\n",
      "59. f16_10\n",
      "60. f16_11\n",
      "61. f16_12\n",
      "62. f16_3\n",
      "63. f16_4\n",
      "64. f16_6\n",
      "65. f17\n",
      "66. f17_2\n",
      "67. f17_3\n",
      "68. f17_4\n",
      "69. f17_5\n",
      "70. f19\n",
      "71. f19_1\n",
      "72. f19_2\n",
      "73. f19_3\n",
      "74. f19_4\n",
      "75. f1_1\n",
      "76. f1_10\n",
      "77. f1_11\n",
      "78. f1_2\n",
      "79. f1_3\n",
      "80. f1_4\n",
      "81. f1_5\n",
      "82. f1_6\n",
      "83. f1_7\n",
      "84. f1_8\n",
      "85. f20\n",
      "86. f20_1\n",
      "87. f20_2\n",
      "88. f20_4\n",
      "89. f20_5\n",
      "90. f20_6\n",
      "91. f20_7\n",
      "92. f20_8\n",
      "93. f20_9\n",
      "94. f21_1\n",
      "95. f21_2\n",
      "96. f22\n",
      "97. f22_1\n",
      "98. f22_2\n",
      "99. f22_3\n",
      "100. f22_4\n",
      "101. f22_5\n",
      "102. f23\n",
      "103. f23_1\n",
      "104. f23_2\n",
      "105. f23_3\n",
      "106. f23_4\n",
      "107. f23_5\n",
      "108. f23_6\n",
      "109. f24\n",
      "110. f24_1\n",
      "111. f24_2\n",
      "112. f24_3\n",
      "113. f25\n",
      "114. f25_1\n",
      "115. f25_3\n",
      "116. f25_7\n",
      "117. f25_8\n",
      "118. f25_9\n",
      "119. f2_1\n",
      "120. f2_10\n",
      "121. f2_11\n",
      "122. f2_12\n",
      "123. f2_13\n",
      "124. f2_2\n",
      "125. f2_4\n",
      "126. f2_5\n",
      "127. f2_6\n",
      "128. f2_7\n",
      "129. f2_8\n",
      "130. f3_1\n",
      "131. f3_2\n",
      "132. f3_3\n",
      "133. f4\n",
      "134. f4_1\n",
      "135. f4_10\n",
      "136. f4_11\n",
      "137. f4_13\n",
      "138. f4_14\n",
      "139. f4_16\n",
      "140. f4_17\n",
      "141. f4_18\n",
      "142. f4_2\n",
      "143. f4_3\n",
      "144. f4_5\n",
      "145. f4_6\n",
      "146. f4_7\n",
      "147. f4_8\n",
      "148. f4_9\n",
      "149. f5\n",
      "150. f5_1\n",
      "151. f5_3\n",
      "152. f5_4\n",
      "153. f6\n",
      "154. f6_1\n",
      "155. f6_2\n",
      "156. f6_4\n",
      "157. f7\n",
      "158. f8\n",
      "159. f8_1\n",
      "160. f8_2\n",
      "161. f8_3\n",
      "162. f8_4\n",
      "163. f8_5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import numpy as np\n",
    "from tables import *\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "try:\n",
    "  import cPickle as pickle\n",
    "except:\n",
    "  import pickle\n",
    "FIELDS_PICKLE = 'fields-4.pkl'\n",
    "DATAFILE_PATH = 'xjk_pytable.h5'\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "\n",
    "fields = pickle.load(open(FIELDS_PICKLE, \"r\") )\n",
    "fields_original = fields[:]\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)\n",
    "\n",
    "print \"Number of features: {}\".format(len(features))\n",
    "print \"Fields:\"\n",
    "for a, b in enumerate(fields):\n",
    "  print '{}. {}'.format(a, b)\n",
    "weather_field_ids = [3, 4, 5, 18, 19, 20, 21, 22, 23]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this section, we will see how the handling of missing data will affect performance. We will do a quick spot-checking, that is to create a baseline model with fast-running algorithm, then compare its performance with and without preprocessing missing data as explained above.\n",
    "\n",
    "The data will be tested with DecisionTree algorithm, and it will be cross-validated with 10-kfold splits. We won't do any grid search at this step as we only want to see if missing data handling works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 102592 rows\n",
      "processed 0 rows\n",
      "there are 102592 rows\n",
      "processed 0 rows\n",
      "there are 102592 rows\n",
      "processed 0 rows\n",
      "there are 102592 rows\n",
      "processed 0 rows\n",
      "processed 5000 rows\n",
      "processed 5000 rows\n",
      "processed 5000 rows\n",
      "processed 5000 rows\n",
      "processed 10000 rows\n",
      "processed 10000 rows\n",
      "processed 10000 rows\n",
      "processed 10000 rows\n",
      "processed 15000 rows\n",
      "processed 15000 rows\n",
      "processed 15000 rows\n",
      "processed 15000 rows\n",
      "processed 20000 rows\n",
      "processed 20000 rows\n",
      "processed 20000 rows\n",
      "processed 20000 rows\n",
      "processed 25000 rows\n",
      "processed 25000 rows\n",
      "processed 25000 rows\n",
      "processed 25000 rows\n",
      "processed 30000 rows\n",
      "processed 30000 rows\n",
      "processed 30000 rows\n",
      "processed 30000 rows\n",
      "processed 35000 rows\n",
      "processed 35000 rows\n",
      "processed 35000 rows\n",
      "processed 35000 rows\n",
      "processed 40000 rows\n",
      "processed 40000 rows\n",
      "processed 40000 rows\n",
      "processed 40000 rows\n",
      "processed 45000 rows\n",
      "processed 45000 rows\n",
      "processed 45000 rows\n",
      "processed 45000 rows\n",
      "processed 50000 rows\n",
      "processed 50000 rows\n",
      "processed 50000 rows\n",
      "processed 50000 rows\n",
      "processed 55000 rows\n",
      "processed 55000 rows\n",
      "processed 55000 rows\n",
      "processed 55000 rows\n",
      "processed 60000 rows\n",
      "processed 60000 rows\n",
      "processed 60000 rows\n",
      "processed 60000 rows\n",
      "processed 65000 rows\n",
      "processed 65000 rows\n",
      "processed 65000 rows\n",
      "processed 65000 rows\n",
      "processed 70000 rows\n",
      "processed 70000 rows\n",
      "processed 70000 rows\n",
      "processed 70000 rows\n",
      "processed 75000 rows\n",
      "processed 75000 rows\n",
      "processed 75000 rows\n",
      "processed 75000 rows\n",
      "processed 80000 rows\n",
      "processed 80000 rows\n",
      "processed 80000 rows\n",
      "processed 80000 rows\n",
      "processed 85000 rows\n",
      "processed 85000 rows\n",
      "processed 85000 rows\n",
      "processed 85000 rows\n",
      "processed 90000 rows\n",
      "processed 90000 rows\n",
      "processed 90000 rows\n",
      "processed 90000 rows\n",
      "processed 95000 rows\n",
      "processed 95000 rows\n",
      "processed 95000 rows\n",
      "processed 95000 rows\n",
      "processed 100000 rows\n",
      "processed 100000 rows\n",
      "processed 100000 rows\n",
      "processed 100000 rows\n"
     ]
    }
   ],
   "source": [
    "fileh1 = open_file(DATAFILE_PATH, mode = 'r')\n",
    "\n",
    "object = fileh1.get_node('/train', 'gaps')\n",
    "object_array_data = object.read()\n",
    "fileh1.close()\n",
    "\n",
    "# Convert to vectorized array that we can use in further processing.\n",
    "all_data = np.zeros((object_array_data.shape[0], len(fields)))\n",
    "print 'there are {} rows'.format(object_array_data.shape[0])\n",
    "for rcounter, row in enumerate(object_array_data):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training fold 1\n",
      "Now training fold 1\n",
      "Now training fold 1\n",
      "Now training fold 1\n",
      "Score: 1.58425959602\n",
      "Now training fold 2\n",
      "Score: 1.58425959602\n",
      "Now training fold 2\n",
      "Score: 1.58425959602\n",
      "Now training fold 2\n",
      "Score: 1.58425959602\n",
      "Now training fold 2\n",
      "Score: 1.61331229877\n",
      "Now training fold 3\n",
      "Score: 1.61331229877\n",
      "Now training fold 3\n",
      "Score: 1.61331229877\n",
      "Now training fold 3\n",
      "Score: 1.61331229877\n",
      "Now training fold 3\n",
      "Score: 1.65189546002\n",
      "Now training fold 4\n",
      "Score: 1.65189546002\n",
      "Now training fold 4\n",
      "Score: 1.65189546002\n",
      "Now training fold 4\n",
      "Score: 1.65189546002\n",
      "Now training fold 4\n",
      "Score: 1.60147081944\n",
      "Now training fold 5\n",
      "Score: 1.60147081944\n",
      "Now training fold 5\n",
      "Score: 1.60147081944\n",
      "Now training fold 5\n",
      "Score: 1.60147081944\n",
      "Now training fold 5\n",
      "Score: 1.63485116423\n",
      "Now training fold 6\n",
      "Score: 1.63485116423\n",
      "Now training fold 6\n",
      "Score: 1.63485116423\n",
      "Now training fold 6\n",
      "Score: 1.63485116423\n",
      "Now training fold 6\n"
     ]
    }
   ],
   "source": [
    "all_data = np.copy(all_data_original)\n",
    "all_data[np.isnan(all_data)] = 0\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3, 4], sparse=False,\n",
    "                            n_values=[145, 7, 10, 10, 10])),\n",
    "  ('estimate', LinearRegression())\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_ = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Missing Data\n",
    "\n",
    "1. Remove weather features.\n",
    "2. Remove all data of district 54.\n",
    "3. Replace missing `tj_level` data points with values from past timeslots.\n",
    "4. Replace the rest of missing data (which would be gap related) with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove weather features and district 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql --module distnums\n",
    "SELECT district_id, rownum FROM (\n",
    "    SELECT district_id, ROW_NUMBER() OVER (ORDER BY district_id, timeslot) AS rownum\n",
    "    FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "    WHERE gap > 0\n",
    ") WHERE district_id = 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = np.copy(all_data_original)\n",
    "fields = fields_original[:]\n",
    "print \"Shape of all data before: {}\".format(all_data_original.shape)\n",
    "\n",
    "# Remove weather features\n",
    "fields = np.delete(fields, weather_field_ids, 0).tolist()\n",
    "features = fields[1:]\n",
    "n_features = len(features)\n",
    "print \"Number of features: {}\".format(len(features))\n",
    "print \"Features:\"\n",
    "print features\n",
    "\n",
    "all_data = np.delete(all_data, weather_field_ids, 1)\n",
    "\n",
    "# Remove district 54\n",
    "rows = bq.Query(distnums).to_dataframe()['rownum'].tolist()\n",
    "print \"number of observations removed: {}\".format(len(rows))\n",
    "all_data = np.delete(all_data, rows, 0)\n",
    "\n",
    "# Replace tj_levels with past values\n",
    "to_replace_list = [\n",
    "  ['tj_level1_1_slots_ago', 0],\n",
    "  ['tj_level2_1_slots_ago', 0],\n",
    "  ['tj_level3_1_slots_ago', 0],\n",
    "  ['tj_level4_1_slots_ago', 0],\n",
    "  ['tj_level1_2_slots_ago', 0],\n",
    "  ['tj_level2_2_slots_ago', 0],\n",
    "  ['tj_level3_2_slots_ago', 0],\n",
    "  ['tj_level4_2_slots_ago', 0],\n",
    "  ['tj_level1_3_slots_ago', 0],\n",
    "  ['tj_level2_3_slots_ago', 0],\n",
    "  ['tj_level3_3_slots_ago', 0],\n",
    "  ['tj_level4_3_slots_ago', 0],\n",
    "]\n",
    "for datum in all_data:\n",
    "  for id, (title, value) in enumerate(to_replace_list):\n",
    "    if math.isnan(datum[fields.index(title)]):\n",
    "      datum[fields.index(title)] = to_replace_list[id][1]\n",
    "    to_replace_list[id][1] = datum[fields.index(title)]\n",
    "\n",
    "print \"Shape of all data after: {}\".format(all_data.shape)\n",
    "\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create table showing features with null values when exist.\n",
    "\n",
    "nulls = np.isnan(all_data)\n",
    "nullspos = np.column_stack(np.where(nulls==True))\n",
    "from operator import itemgetter\n",
    "print \"total data points:\", (all_data.shape[0] * all_data.shape[1])\n",
    "print \"number of missing values:\", nullspos.shape[0]\n",
    "if nullspos.shape[0] > 0:\n",
    "  missing_features = itemgetter(*np.unique(nullspos[:,1]).tolist())(fields)\n",
    "  missing_features_table = pd.DataFrame(columns=['id', 'field', 'missing data points'])\n",
    "\n",
    "  for id, field in enumerate(fields):\n",
    "    total_missing = len(np.where(nullspos[:,1]==id)[0])\n",
    "    if total_missing > 0:\n",
    "      missing_features_table = missing_features_table.append({\n",
    "          'id': id,\n",
    "          'field': field,\n",
    "          'missing data points': total_missing\n",
    "        }, ignore_index=True)\n",
    "  missing_features_table['missing data points'] = \\\n",
    "    missing_features_table['missing data points'].astype('int64')\n",
    "  missing_features_table['id'] = \\\n",
    "    missing_features_table['id'].astype('int64')\n",
    "  print missing_features_table.sort_values(['missing data points', 'id'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot features with null values when exist.\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "if nullspos.shape[0] > 0:\n",
    "  matplotlib.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "  def rand_jitter(arr):\n",
    "      stdev = .005*(max(arr)-min(arr))\n",
    "      return arr + np.random.randn(len(arr)) * stdev\n",
    "  _ = plt.scatter(nullspos[:,0], rand_jitter(nullspos[:,1]), s=0.5)\n",
    "  _ = plt.title('Missing Data Points')\n",
    "  _ = plt.ylabel('Feature ID')\n",
    "  _ = plt.xlabel('Observation ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace rest of data with 0    \n",
    "all_data[np.isnan(all_data)] = 0\n",
    "\n",
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1], sparse=False,\n",
    "                            n_values=[145, 7])),\n",
    "  ('estimate', LinearRegression())\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_wd = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only remove district 54 observations and replace tj_levels\n",
    "What if we keep weather features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = np.copy(all_data_original)\n",
    "fields = fields_original[:]\n",
    "print \"Shape of all data before: {}\".format(all_data_original.shape)\n",
    "\n",
    "# Remove district 54\n",
    "rows = bq.Query(distnums).to_dataframe()['rownum'].tolist()\n",
    "all_data = np.delete(all_data, rows, 0)\n",
    "\n",
    "# Replace tj_levels with past values\n",
    "to_replace_list = [\n",
    "  ['tj_level1_1_slots_ago', 0],\n",
    "  ['tj_level2_1_slots_ago', 0],\n",
    "  ['tj_level3_1_slots_ago', 0],\n",
    "  ['tj_level4_1_slots_ago', 0],\n",
    "  ['tj_level1_2_slots_ago', 0],\n",
    "  ['tj_level2_2_slots_ago', 0],\n",
    "  ['tj_level3_2_slots_ago', 0],\n",
    "  ['tj_level4_2_slots_ago', 0],\n",
    "  ['tj_level1_3_slots_ago', 0],\n",
    "  ['tj_level2_3_slots_ago', 0],\n",
    "  ['tj_level3_3_slots_ago', 0],\n",
    "  ['tj_level4_3_slots_ago', 0],\n",
    "]\n",
    "for datum in all_data:\n",
    "  for id, (title, value) in enumerate(to_replace_list):\n",
    "    if math.isnan(datum[fields.index(title)]):\n",
    "      datum[fields.index(title)] = to_replace_list[id][1]\n",
    "    to_replace_list[id][1] = datum[fields.index(title)]\n",
    "\n",
    "# Replace rest of data with 0    \n",
    "all_data[np.isnan(all_data)] = 0\n",
    "\n",
    "print \"Shape of all data after: {}\".format(all_data.shape)\n",
    "\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3, 4], sparse=False,\n",
    "                            n_values=[145, 7, 10, 10, 10])),\n",
    "  ('estimate', LinearRegression())\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_d = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only remove weather features\n",
    "And what if we keep district 54?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = np.copy(all_data_original)\n",
    "fields = fields_original[:]\n",
    "print \"Shape of all data before: {}\".format(all_data_original.shape)\n",
    "\n",
    "# Remove weather features\n",
    "fields = np.delete(fields, weather_field_ids, 0).tolist()\n",
    "features = fields[1:]\n",
    "n_features = len(features)\n",
    "print \"Number of features: {}\".format(len(features))\n",
    "print \"Features:\"\n",
    "print features\n",
    "all_data = np.delete(all_data, weather_field_ids, 1)\n",
    "\n",
    "# Replace rest of data with 0    \n",
    "all_data[np.isnan(all_data)] = 0\n",
    "\n",
    "print \"Shape of all data after: {}\".format(all_data.shape)\n",
    "\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1], sparse=False,\n",
    "                            n_values=[145, 7])),\n",
    "  ('estimate', LinearRegression())\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_w = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace missing weather features instead of removing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = np.copy(all_data_original)\n",
    "fields = fields_original[:]\n",
    "print \"Shape of all data before: {}\".format(all_data_original.shape)\n",
    "\n",
    "# Remove district 54\n",
    "# rows = bq.Query(distnums).to_dataframe()['rownum'].tolist()\n",
    "# all_data = np.delete(all_data, rows, 0)\n",
    "\n",
    "# Replace weather data with past values\n",
    "to_replace_list = [\n",
    "  ['weather_1_slots_ago', 0],\n",
    "  ['weather_2_slots_ago', 0],\n",
    "  ['weather_3_slots_ago', 0],\n",
    "  ['temperature_1_slots_ago', 0],\n",
    "  ['pm25_1_slots_ago', 0],\n",
    "  ['temperature_2_slots_ago', 0],\n",
    "  ['pm25_2_slots_ago', 0],\n",
    "  ['temperature_3_slots_ago', 0],\n",
    "  ['pm25_3_slots_ago', 0]\n",
    "]\n",
    "for datum in all_data:\n",
    "  for id, (title, value) in enumerate(to_replace_list):\n",
    "    if math.isnan(datum[fields.index(title)]):\n",
    "      datum[fields.index(title)] = to_replace_list[id][1]\n",
    "    to_replace_list[id][1] = datum[fields.index(title)]\n",
    "\n",
    "# Replace rest of data with 0    \n",
    "all_data[np.isnan(all_data)] = 0\n",
    "\n",
    "print \"Shape of all data after: {}\".format(all_data.shape)\n",
    "\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3, 4], sparse=False,\n",
    "                            n_values=[145, 7, 10, 10, 10])),\n",
    "  ('estimate', LinearRegression())\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_w2 = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print score_\n",
    "print score_w\n",
    "print score_w2\n",
    "print score_d\n",
    "print score_wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Impute all missing data with 0 (baseline):\n",
    "  - DecisionTreeRegressor: 0.903058987437\n",
    "  - LinearRegression: 1.61928719402\n",
    "- Remove weather features:\n",
    "  - DecisionTreeRegressor: 0.897465409548\n",
    "  - LinearRegression: 1.60911699966\n",
    "- Replace missing weather features:\n",
    "  - DecisionTreeRegressor: 0.903488596483\n",
    "  - LinearRegression: 1.6213712183\n",
    "- Remove district 54 and replace missing traffic info:\n",
    "  - DecisionTreeRegressor: 0.901123545134\n",
    "  - LinearRegression: 1.61540261804\n",
    "- Remove weather features and district 54 and replace missing traffic info:\n",
    "  - DecisionTreeRegressor: 0.899022152778\n",
    "  - LinearRegression: 1.60133826485\n",
    "  \n",
    "This result gives better understanding in addition to previous one.\n",
    "\n",
    "With DecisionTree algorithm, removing weather features while making no change to traffic info produced the best model for this problem. This is probably because decision tree algorithm was able to conditionally choose not to include district 54 in its branch splits. In LinearRegression, on the contrary, the missing values from district 54 skewed the predictions of other features.\n",
    "\n",
    "In next step, we will do automated feature selection, to further remove irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Replace rest of data with 0    \n",
    "all_data[np.isnan(all_data)] = 0\n",
    "\n",
    "steps = [\n",
    "  ('scaling', MinMaxScaler()),\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1], sparse=False,\n",
    "                            n_values=[145, 7])),\n",
    "  ('pca', PCA()),\n",
    "  ('estimate', LinearRegression())\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_wd_pca = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_wd_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Replace rest of data with 0    \n",
    "all_data[np.isnan(all_data)] = 0\n",
    "\n",
    "steps = [\n",
    "  ('scaling', MinMaxScaler()),\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1], sparse=False,\n",
    "                            n_values=[145, 7])),\n",
    "  ('pca', PCA()),\n",
    "  ('estimate', DecisionTreeRegressor(random_state=seed))\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_wd_pca_tree = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_wd_pca_tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
