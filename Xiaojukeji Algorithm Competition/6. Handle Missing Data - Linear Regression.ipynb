{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tables",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7ad31b6cc90f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtables\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbigquery\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbq\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named tables"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import numpy as np\n",
    "from tables import *\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "try:\n",
    "  import cPickle as pickle\n",
    "except:\n",
    "  import pickle\n",
    "FIELDS_PICKLE = 'fields-4.pkl'\n",
    "DATAFILE_PATH = 'xjk_pytable.h5'\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "\n",
    "fields = pickle.load(open(FIELDS_PICKLE, \"r\") )\n",
    "fields_original = fields[:]\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)\n",
    "\n",
    "print \"Number of features: {}\".format(len(features))\n",
    "print \"Fields:\"\n",
    "for a, b in enumerate(fields):\n",
    "  print '{}. {}'.format(a, b)\n",
    "weather_field_ids = [3, 4, 5, 18, 19, 20, 21, 22, 23]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this section, we will see how the handling of missing data will affect performance. We will do a quick spot-checking, that is to create a baseline model with fast-running algorithm, then compare its performance with and without preprocessing missing data as explained above.\n",
    "\n",
    "The data will be tested with DecisionTree algorithm, and it will be cross-validated with 10-kfold splits. We won't do any grid search at this step as we only want to see if missing data handling works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'open_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-40b1a2bc86b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfileh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATAFILE_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfileh1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'gaps'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mobject_array_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfileh1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'open_file' is not defined"
     ]
    }
   ],
   "source": [
    "fileh1 = open_file(DATAFILE_PATH, mode = 'r')\n",
    "\n",
    "object = fileh1.get_node('/train', 'gaps')\n",
    "object_array_data = object.read()\n",
    "fileh1.close()\n",
    "\n",
    "# Convert to vectorized array that we can use in further processing.\n",
    "all_data = np.zeros((object_array_data.shape[0], len(fields)))\n",
    "print 'there are {} rows'.format(object_array_data.shape[0])\n",
    "for rcounter, row in enumerate(object_array_data):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = np.copy(all_data_original)\n",
    "all_data[np.isnan(all_data)] = 0\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3, 4], sparse=False,\n",
    "                            n_values=[145, 7, 10, 10, 10])),\n",
    "  ('estimate', LinearRegression())\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_ = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Missing Data\n",
    "\n",
    "1. Remove weather features.\n",
    "2. Remove all data of district 54.\n",
    "3. Replace missing `tj_level` data points with values from past timeslots.\n",
    "4. Replace the rest of missing data (which would be gap related) with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove weather features and district 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql --module distnums\n",
    "SELECT district_id, rownum FROM (\n",
    "    SELECT district_id, ROW_NUMBER() OVER (ORDER BY district_id, timeslot) AS rownum\n",
    "    FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "    WHERE gap > 0\n",
    ") WHERE district_id = 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = np.copy(all_data_original)\n",
    "fields = fields_original[:]\n",
    "print \"Shape of all data before: {}\".format(all_data_original.shape)\n",
    "\n",
    "# Remove weather features\n",
    "fields = np.delete(fields, weather_field_ids, 0).tolist()\n",
    "features = fields[1:]\n",
    "n_features = len(features)\n",
    "print \"Number of features: {}\".format(len(features))\n",
    "print \"Features:\"\n",
    "print features\n",
    "\n",
    "all_data = np.delete(all_data, weather_field_ids, 1)\n",
    "\n",
    "# Remove district 54\n",
    "rows = bq.Query(distnums).to_dataframe()['rownum'].tolist()\n",
    "print \"number of observations removed: {}\".format(len(rows))\n",
    "all_data = np.delete(all_data, rows, 0)\n",
    "\n",
    "# Replace tj_levels with past values\n",
    "to_replace_list = [\n",
    "  ['tj_level1_1_slots_ago', 0],\n",
    "  ['tj_level2_1_slots_ago', 0],\n",
    "  ['tj_level3_1_slots_ago', 0],\n",
    "  ['tj_level4_1_slots_ago', 0],\n",
    "  ['tj_level1_2_slots_ago', 0],\n",
    "  ['tj_level2_2_slots_ago', 0],\n",
    "  ['tj_level3_2_slots_ago', 0],\n",
    "  ['tj_level4_2_slots_ago', 0],\n",
    "  ['tj_level1_3_slots_ago', 0],\n",
    "  ['tj_level2_3_slots_ago', 0],\n",
    "  ['tj_level3_3_slots_ago', 0],\n",
    "  ['tj_level4_3_slots_ago', 0],\n",
    "]\n",
    "for datum in all_data:\n",
    "  for id, (title, value) in enumerate(to_replace_list):\n",
    "    if math.isnan(datum[fields.index(title)]):\n",
    "      datum[fields.index(title)] = to_replace_list[id][1]\n",
    "    to_replace_list[id][1] = datum[fields.index(title)]\n",
    "\n",
    "print \"Shape of all data after: {}\".format(all_data.shape)\n",
    "\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create table showing features with null values when exist.\n",
    "\n",
    "nulls = np.isnan(all_data)\n",
    "nullspos = np.column_stack(np.where(nulls==True))\n",
    "from operator import itemgetter\n",
    "print \"total data points:\", (all_data.shape[0] * all_data.shape[1])\n",
    "print \"number of missing values:\", nullspos.shape[0]\n",
    "if nullspos.shape[0] > 0:\n",
    "  missing_features = itemgetter(*np.unique(nullspos[:,1]).tolist())(fields)\n",
    "  missing_features_table = pd.DataFrame(columns=['id', 'field', 'missing data points'])\n",
    "\n",
    "  for id, field in enumerate(fields):\n",
    "    total_missing = len(np.where(nullspos[:,1]==id)[0])\n",
    "    if total_missing > 0:\n",
    "      missing_features_table = missing_features_table.append({\n",
    "          'id': id,\n",
    "          'field': field,\n",
    "          'missing data points': total_missing\n",
    "        }, ignore_index=True)\n",
    "  missing_features_table['missing data points'] = \\\n",
    "    missing_features_table['missing data points'].astype('int64')\n",
    "  missing_features_table['id'] = \\\n",
    "    missing_features_table['id'].astype('int64')\n",
    "  print missing_features_table.sort_values(['missing data points', 'id'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot features with null values when exist.\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "if nullspos.shape[0] > 0:\n",
    "  matplotlib.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "  def rand_jitter(arr):\n",
    "      stdev = .005*(max(arr)-min(arr))\n",
    "      return arr + np.random.randn(len(arr)) * stdev\n",
    "  _ = plt.scatter(nullspos[:,0], rand_jitter(nullspos[:,1]), s=0.5)\n",
    "  _ = plt.title('Missing Data Points')\n",
    "  _ = plt.ylabel('Feature ID')\n",
    "  _ = plt.xlabel('Observation ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace rest of data with 0    \n",
    "all_data[np.isnan(all_data)] = 0\n",
    "\n",
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1], sparse=False,\n",
    "                            n_values=[145, 7])),\n",
    "  ('estimate', LinearRegression())\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_wd = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only remove district 54 observations and replace tj_levels\n",
    "What if we keep weather features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = np.copy(all_data_original)\n",
    "fields = fields_original[:]\n",
    "print \"Shape of all data before: {}\".format(all_data_original.shape)\n",
    "\n",
    "# Remove district 54\n",
    "rows = bq.Query(distnums).to_dataframe()['rownum'].tolist()\n",
    "all_data = np.delete(all_data, rows, 0)\n",
    "\n",
    "# Replace tj_levels with past values\n",
    "to_replace_list = [\n",
    "  ['tj_level1_1_slots_ago', 0],\n",
    "  ['tj_level2_1_slots_ago', 0],\n",
    "  ['tj_level3_1_slots_ago', 0],\n",
    "  ['tj_level4_1_slots_ago', 0],\n",
    "  ['tj_level1_2_slots_ago', 0],\n",
    "  ['tj_level2_2_slots_ago', 0],\n",
    "  ['tj_level3_2_slots_ago', 0],\n",
    "  ['tj_level4_2_slots_ago', 0],\n",
    "  ['tj_level1_3_slots_ago', 0],\n",
    "  ['tj_level2_3_slots_ago', 0],\n",
    "  ['tj_level3_3_slots_ago', 0],\n",
    "  ['tj_level4_3_slots_ago', 0],\n",
    "]\n",
    "for datum in all_data:\n",
    "  for id, (title, value) in enumerate(to_replace_list):\n",
    "    if math.isnan(datum[fields.index(title)]):\n",
    "      datum[fields.index(title)] = to_replace_list[id][1]\n",
    "    to_replace_list[id][1] = datum[fields.index(title)]\n",
    "\n",
    "# Replace rest of data with 0    \n",
    "all_data[np.isnan(all_data)] = 0\n",
    "\n",
    "print \"Shape of all data after: {}\".format(all_data.shape)\n",
    "\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3, 4], sparse=False,\n",
    "                            n_values=[145, 7, 10, 10, 10])),\n",
    "  ('estimate', LinearRegression())\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_d = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only remove weather features\n",
    "And what if we keep district 54?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = np.copy(all_data_original)\n",
    "fields = fields_original[:]\n",
    "print \"Shape of all data before: {}\".format(all_data_original.shape)\n",
    "\n",
    "# Remove weather features\n",
    "fields = np.delete(fields, weather_field_ids, 0).tolist()\n",
    "features = fields[1:]\n",
    "n_features = len(features)\n",
    "print \"Number of features: {}\".format(len(features))\n",
    "print \"Features:\"\n",
    "print features\n",
    "all_data = np.delete(all_data, weather_field_ids, 1)\n",
    "\n",
    "# Replace rest of data with 0    \n",
    "all_data[np.isnan(all_data)] = 0\n",
    "\n",
    "print \"Shape of all data after: {}\".format(all_data.shape)\n",
    "\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1], sparse=False,\n",
    "                            n_values=[145, 7])),\n",
    "  ('estimate', LinearRegression())\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_w = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace missing weather features instead of removing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = np.copy(all_data_original)\n",
    "fields = fields_original[:]\n",
    "print \"Shape of all data before: {}\".format(all_data_original.shape)\n",
    "\n",
    "# Remove district 54\n",
    "# rows = bq.Query(distnums).to_dataframe()['rownum'].tolist()\n",
    "# all_data = np.delete(all_data, rows, 0)\n",
    "\n",
    "# Replace weather data with past values\n",
    "to_replace_list = [\n",
    "  ['weather_1_slots_ago', 0],\n",
    "  ['weather_2_slots_ago', 0],\n",
    "  ['weather_3_slots_ago', 0],\n",
    "  ['temperature_1_slots_ago', 0],\n",
    "  ['pm25_1_slots_ago', 0],\n",
    "  ['temperature_2_slots_ago', 0],\n",
    "  ['pm25_2_slots_ago', 0],\n",
    "  ['temperature_3_slots_ago', 0],\n",
    "  ['pm25_3_slots_ago', 0]\n",
    "]\n",
    "for datum in all_data:\n",
    "  for id, (title, value) in enumerate(to_replace_list):\n",
    "    if math.isnan(datum[fields.index(title)]):\n",
    "      datum[fields.index(title)] = to_replace_list[id][1]\n",
    "    to_replace_list[id][1] = datum[fields.index(title)]\n",
    "\n",
    "# Replace rest of data with 0    \n",
    "all_data[np.isnan(all_data)] = 0\n",
    "\n",
    "print \"Shape of all data after: {}\".format(all_data.shape)\n",
    "\n",
    "data = all_data[:,1:]\n",
    "targets = all_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = [\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3, 4], sparse=False,\n",
    "                            n_values=[145, 7, 10, 10, 10])),\n",
    "  ('estimate', LinearRegression())\n",
    "]\n",
    "\n",
    "\n",
    "folds = KFold(len(targets), n_folds=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "  print \"Now training fold {}\".format(fold+1)\n",
    "  est = Pipeline(steps)\n",
    "  est.fit(data[train_ids], targets[train_ids])\n",
    "  preds = est.predict(data[test_ids])\n",
    "  score = mape(targets[test_ids], preds)\n",
    "  scores.append(score)\n",
    "  print \"Score: {}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_w2 = sum(scores)/len(scores)\n",
    "print \"Mean score: {}\".format(score_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print score_\n",
    "print score_w\n",
    "print score_w2\n",
    "print score_d\n",
    "print score_wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Impute all missing data with 0 (baseline):\n",
    "  0.903058987437\n",
    "- Remove weather features:\n",
    "  0.897465409548\n",
    "- Replace missing weather features:\n",
    "  0.903488596483\n",
    "- Remove district 54 and replace missing traffic info:\n",
    "  0.901123545134\n",
    "- Remove weather features and district 54 and replace missing traffic info:\n",
    "  0.899022152778\n",
    "  \n",
    "Interestingly, removing weather features while making no change to traffic info produced the best model for this problem. This was uncalled for, since simply replacing traffic info and removed district 54 improved the performance from baseline, but combining it with weather features removal resulted in worse performance than only remove weather features.\n",
    "\n",
    "In next step, we will do automated feature selection, to further remove irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
