{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "EST_PICKLE_FILENAME = 'GradientBoostingRegressor_grid_best.pkl'\n",
    "\n",
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Put all categorical data first for easier implementation of One Hot Encoding.\n",
    "fields_str = \"\"\"\n",
    "gap\tday_in_week\tweather_1_slots_ago\tweather_2_slots_ago\tweather_3_slots_ago\tbusy_time\t\n",
    "tj_level1_1_slots_ago\ttj_level2_1_slots_ago\ttj_level3_1_slots_ago\ttj_level4_1_slots_ago\t\n",
    "tj_level1_2_slots_ago\ttj_level2_2_slots_ago\ttj_level3_2_slots_ago\ttj_level4_2_slots_ago\t\n",
    "tj_level1_3_slots_ago\ttj_level2_3_slots_ago\ttj_level3_3_slots_ago\ttj_level4_3_slots_ago\t\n",
    "temperature_1_slots_ago\tpm25_1_slots_ago\t\n",
    "temperature_2_slots_ago\tpm25_2_slots_ago\t\n",
    "temperature_3_slots_ago\tpm25_3_slots_ago\t\n",
    "gap_1_slots_ago\tsum_price_1_slots_ago\t\n",
    "gap_2_slots_ago\tsum_price_2_slots_ago\t\n",
    "gap_3_slots_ago\tsum_price_3_slots_ago\t\n",
    "f1\tf11\tf11_1\tf11_2\tf11_3\tf11_4\tf11_5\tf11_6\tf11_7\t\n",
    "f11_8\tf13_4\tf13_8\tf14\tf14_1\tf14_10\tf14_2\tf14_3\tf14_6\tf14_8\tf15\tf15_1\t\n",
    "f15_2\tf15_3\tf15_4\tf15_6\tf15_7\tf15_8\tf16\tf16_1\tf16_10\tf16_11\tf16_12\tf16_3\t\n",
    "f16_4\tf16_6\tf17\tf17_2\tf17_3\tf17_4\tf17_5\tf19\tf19_1\tf19_2\tf19_3\tf19_4\tf1_1\t\n",
    "f1_10\tf1_11\tf1_2\tf1_3\tf1_4\tf1_5\tf1_6\tf1_7\tf1_8\tf20\tf20_1\tf20_2\t\n",
    "f20_4\tf20_5\tf20_6\tf20_7\tf20_8\tf20_9\tf21_1\tf21_2\tf22\tf22_1\tf22_2\tf22_3\t\n",
    "f22_4\tf22_5\tf23\tf23_1\tf23_2\tf23_3\tf23_4\tf23_5\tf23_6\tf24\tf24_1\tf24_2\tf24_3\t\n",
    "f25\tf25_1\tf25_3\tf25_7\tf25_8\tf25_9\tf2_1\tf2_10\tf2_11\tf2_12\tf2_13\tf2_2\t\n",
    "f2_4\tf2_5\tf2_6\tf2_7\tf2_8\tf3_1\tf3_2\tf3_3\tf4\tf4_1\tf4_10\tf4_11\t\n",
    "f4_13\tf4_14\tf4_16\tf4_17\tf4_18\tf4_2\tf4_3\tf4_5\tf4_6\tf4_7\tf4_8\tf4_9\t\n",
    "f5\tf5_1\tf5_3\tf5_4\tf6\tf6_1\tf6_2\tf6_4\tf7\tf8\tf8_1\tf8_2\tf8_3\tf8_4\t\n",
    "f8_5\n",
    "\"\"\"\n",
    "fields = map(lambda x: x.strip(), fields_str.split('\\t'))\n",
    "features = fields[1:]\n",
    "\n",
    "# Use this instead of len(features) since this variable can change\n",
    "# e.g. when one hot encoding is used and/or new features are added.\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorer Creation (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(y, predictions):\n",
    "#   num_timeslots = 43\n",
    "#   num_districts = 66\n",
    "  if len(y.shape) == 1:\n",
    "    y = np.asmatrix(y)\n",
    "  if len(predictions.shape) == 1:\n",
    "    predictions = np.asmatrix(predictions)\n",
    "  y = y.astype(float)\n",
    "  predictions = predictions.astype(float)\n",
    "  return np.mean(np.absolute((y-predictions)/y))\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# def mape(y, predictions):\n",
    "#   return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.145833333333\n",
      "254.604166667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "predictions = np.array([1.0, 2.0, 3.0, 4.0]).astype('float32')\n",
    "y = np.array([1.0, 2.0, 3.0, 4.0]).astype('float32')\n",
    "\n",
    "# Should return 0.0\n",
    "print mape(y, predictions)\n",
    "\n",
    "# Should return higher score\n",
    "predictions = np.array([1.0, 2.0, 2.0, 3.0]).astype('float32')\n",
    "print(mape(y, predictions))\n",
    "\n",
    "# Should return highest score\n",
    "predictions = np.array([1000.0, 22.0, 11.0, 31.0]).astype('float32')\n",
    "print(mape(y, predictions))\n",
    "\n",
    "# est = LogisticRegression()\n",
    "# X = np.random.rand(10,4)\n",
    "# y = X.sum(axis=1)\n",
    "# est.fit(X, y)\n",
    "# predictions = est.predict(X)\n",
    "# print(mape(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module q_all\n",
    "\n",
    "SELECT *, HASH(CAST(district_id AS STRING) +timeslot) AS hash_value,\n",
    "  IF(ABS(HASH(CAST(district_id AS STRING) + timeslot)) % 2 == 1, 'True', 'False')\n",
    "    AS included_in_sample, IF(timeofday_slot >= 50 AND timeofday_slot <= 53, 1, 0) AS busy_time\n",
    "FROM [datalab-projects-1331:xjk_algo_comp.gaps]\n",
    "WHERE gap > 0\n",
    "LIMIT 80000\n",
    "\n",
    "# The above query randomizes its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 80000 rows\n",
      "processed 0 rows\n",
      "processed 5000 rows\n",
      "processed 10000 rows\n",
      "processed 15000 rows\n",
      "processed 20000 rows\n",
      "processed 25000 rows\n",
      "processed 30000 rows\n",
      "processed 35000 rows\n",
      "processed 40000 rows\n",
      "processed 45000 rows\n",
      "processed 50000 rows\n",
      "processed 55000 rows\n",
      "processed 60000 rows\n",
      "processed 65000 rows\n",
      "processed 70000 rows\n",
      "processed 75000 rows\n"
     ]
    }
   ],
   "source": [
    "query = bq.Query(q_all)\n",
    "tableresult = query.results()\n",
    "\n",
    "all_data = np.zeros((tableresult.length, len(fields)))\n",
    "print 'there are {} rows'.format(tableresult.length)\n",
    "for rcounter, row in enumerate(tableresult):\n",
    "  for fcounter, field in enumerate(fields):\n",
    "    all_data[rcounter, fcounter] = row[field]\n",
    "  if rcounter % 5000 == 0:\n",
    "    print 'processed {} rows'.format(rcounter)\n",
    "all_data_original = np.copy(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkinf for NaN and Inf\n",
      "np.nan= (array([    5,     5,    19, ..., 79999, 79999, 79999]), array([24, 25, 24, ..., 15, 16, 17])) total= 2\n",
      "is.inf= (array([], dtype=int64), array([], dtype=int64)) total= 2\n",
      "np.max= nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "where() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b70b111cf8e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"np.max=\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data_original\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mnulls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data_original\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnulls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Impute all NaN with numbers (not sure what to replace inf yet)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: where() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "# This chunk does further wrangling to dataset to produce training and test sets.\n",
    "\n",
    "# Useful code to check NaN and Inf values. This is needed since these values would\n",
    "# cause \"Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "# errors when left unchecked.\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data_original)), \"total=\", len(np.where(np.isnan(all_data_original)))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data_original)), \"total=\", len(np.where(np.isinf(all_data_original)))\n",
    "print \"np.max=\", np.max(abs(all_data_original))\n",
    "nulls = np.isnan(all_data_original)\n",
    "print np.where(nulls = False)\n",
    "\n",
    "# Impute all NaN with numbers (not sure what to replace inf yet)\n",
    "all_data[np.isnan(all_data_original)] = 0\n",
    "# all_data[np.isinf(all_data)] = 0\n",
    "\n",
    "# See that NaN and Inf values replaced\n",
    "print \"Checkinf for NaN and Inf\"\n",
    "print \"np.nan=\", np.where(np.isnan(all_data)), \"total=\", len(np.where(np.isnan(all_data)))\n",
    "print \"is.inf=\", np.where(np.isinf(all_data)), \"total=\", len(np.where(np.isinf(all_data)))\n",
    "print \"np.max=\", np.max(abs(all_data))\n",
    "\n",
    "# Split the data into train and test sets.\n",
    "data_size = all_data.shape[0]\n",
    "training_size = data_size * 80/100\n",
    "indices = np.random.permutation(data_size)\n",
    "training_idx, test_idx = indices[:training_size], indices[training_size:]\n",
    "all_data_train, all_data_test = all_data[training_idx,:], all_data[test_idx,:]\n",
    "\n",
    "data_train = all_data_train[:,1:]\n",
    "targets_train = all_data_train[:,0]\n",
    "data_test = all_data_test[:,1:]\n",
    "targets_test = all_data_test[:,0]\n",
    "data_train_original = np.copy(data_train)\n",
    "data_test_original = np.copy(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# one_hot = OneHotEncoder(categorical_features=[0, 1, 14, 17, 20], n_values='auto')\n",
    "one_hot = OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False)\n",
    "one_hot.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-15-bd7506dea7c2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-bd7506dea7c2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    np.where(nulls[0] = False)\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "np.where(np.any(nulls==False, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 10, 10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.n_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7, 17, 27, 37])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.feature_indices_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Testing Algorithm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.451136351423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "class CustomRegressor(BaseEstimator, RegressorMixin):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, X, y):\n",
    "#     self.classes_, indices = np.unique([\"foo\", \"bar\", \"foo\"],\n",
    "#                                     return_inverse=True)\n",
    "#     self.majority_ = np.argmax(np.bincount(indices))\n",
    "    return self\n",
    "\n",
    "  def predict(self, X):\n",
    "    # 56: gap_1_slots_ago\n",
    "    # 58: gap_2_slots_ago\n",
    "    # 60: gap_3_slots_ago\n",
    "#     X = X.tocsr()\n",
    "#     v1 = coo_matrix(np.asmatrix(np.ones(X.shape[0])).T)\n",
    "    v1 = np.asmatrix(np.ones(X.shape[0]))\n",
    "    v2 = np.asmatrix((X[:, 23]*0.65+X[:, 25]*0.25+X[:, 27]*0.15)/2)\n",
    "    predictions = np.asarray(np.concatenate((v1, v2), axis=0).max(axis=0))\n",
    "    \n",
    "    return predictions\n",
    "  \n",
    "custom_est = CustomRegressor()\n",
    "custom_est.fit(data_train_original, data_test_original)\n",
    "custom_predictions = custom_est.predict(data_test_original)\n",
    "print(mape(targets_test, custom_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=110 .........\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=110 .........\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=110 .........\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=110 .........\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=110 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=110, score=-0.874537 -21.9min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=110, score=-0.886750 -21.9min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=110, score=-0.867296 -22.0min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=110, score=-0.878946 -22.5min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=110, score=-0.866456 -22.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done   1 jobs       | elapsed: 22.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=130, score=-0.863527 -29.7min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=130, score=-0.855026 -30.0min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=130, score=-0.862286 -30.4min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=130, score=-0.868889 -30.3min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=130, score=-0.846937 -30.4min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=150, score=-0.852371 -37.3min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=150, score=-0.849532 -37.5min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=150, score=-0.850113 -37.4min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=150, score=-0.862197 -37.8min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=150, score=-0.832220 -38.3min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=170, score=-0.837837 -40.5min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=170, score=-0.839305 -41.3min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=170, score=-0.847533 -41.9min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=170, score=-0.853956 -41.4min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=170, score=-0.833629 -41.6min\n",
      "[CV] estimate__learning_rate=0.1, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=190, score=-0.831853 -46.6min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=110 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=190, score=-0.832230 -45.9min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=110 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=190, score=-0.835731 -46.3min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=110 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=190, score=-0.845529 -45.9min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=110 .........\n",
      "[CV]  estimate__learning_rate=0.1, estimate__n_estimators=190, score=-0.825329 -46.1min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=110 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=110, score=-0.811726 -26.9min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=110, score=-0.813344 -26.9min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=110, score=-0.806914 -27.3min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=110, score=-0.836271 -26.9min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=110, score=-0.811101 -26.9min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=130 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=130, score=-0.807818 -31.6min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=130, score=-0.810413 -31.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  32 jobs       | elapsed: 235.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=130, score=-0.810738 -31.3min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=130, score=-0.827862 -31.8min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=130, score=-0.804540 -31.2min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=150 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=150, score=-0.817718 -25.4min\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=150, score=-0.813261 -24.8min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=170 .........\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=150, score=-0.810743 -24.6min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=150, score=-0.831832 -24.3min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=150, score=-0.795132 -24.2min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=170 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=170, score=-0.814398 -26.9min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=170, score=-0.815009 -27.0min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=170, score=-0.817252 -27.7min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=170, score=-0.825804 -27.2min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=170, score=-0.805612 -27.5min\n",
      "[CV] estimate__learning_rate=0.5, estimate__n_estimators=190 .........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=190, score=-0.809902 -33.3min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=110 ...........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=190, score=-0.810688 -33.0min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=110 ...........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=190, score=-0.823018 -33.2min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=110 ...........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=190, score=-0.805750 -33.8min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=110 ...........\n",
      "[CV]  estimate__learning_rate=0.5, estimate__n_estimators=190, score=-0.800142 -33.5min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=110 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=110, score=-0.829033 -13.9min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=130 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=110, score=-0.859800 -14.2min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=130 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=110, score=-0.834761 -13.7min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=130 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=110, score=-0.870001 -13.6min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=130 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=110, score=-0.842385 -13.3min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=130 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=130, score=-0.844505 -14.2min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=150 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=130, score=-0.861480 -14.5min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=150 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=130, score=-0.839272 -14.3min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=150 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=130, score=-0.860530 -14.5min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=150 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=130, score=-0.836925 -14.4min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=150 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=150, score=-0.851785 -18.2min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=170 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=150, score=-0.846755 -17.9min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=170 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=150, score=-0.841111 -18.0min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=170 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=150, score=-0.830301 -17.7min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=170 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=150, score=-0.868845 -18.0min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=170 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=170, score=-0.844000 -17.9min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=190 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=170, score=-0.836046 -18.0min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=190 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=170, score=-0.860398 -18.1min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=190 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=170, score=-0.868008 -18.2min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=190 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=170, score=-0.836868 -18.7min\n",
      "[CV] estimate__learning_rate=1, estimate__n_estimators=190 ...........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=190, score=-0.848399 -20.6min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=110 ..........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=190, score=-0.867988 -21.0min\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=190, score=-0.850009 -20.8min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=110 ..........\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=110 ..........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=190, score=-0.879111 -21.4min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=110 ..........\n",
      "[CV]  estimate__learning_rate=1, estimate__n_estimators=190, score=-0.836483 -21.8min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=110 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=110, score=-4453554487192734386755844528315629674770697693938493857659660163110444622769751948552039969909574071222272.000000 -22.0min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=130 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=110, score=-4428399507682609966124701404199323335292628270480872742575239544590273979492857402108450796344580990042112.000000 -21.8min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=130 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=110, score=-4328306177892186112800568623633958108657994161138428020831783135785860648311720713968016056407480866963456.000000 -22.4min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=130 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=110, score=-4393200415624140613749205094637631251515659483615373251525602213552012369259881313836362609518107489730560.000000 -21.7min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=130 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=110, score=-4369608788794409608360400085681547615553846234093942543796575601329176907664370183725620422423698934333440.000000 -21.5min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=130 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=130, score=-54144825558727195673196414392468249558311052739960618584521930089662951311504286819696567710146894189505225185965758142742528.000000 -25.8min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=150 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=130, score=-53838999733470481345107056111942711094583164102596256449967790817591927466155480571867125842509711848343365675273287404355584.000000 -25.8min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=150 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=130, score=-52622098515081760849563183055974492267901023786445045818405125230359050624231252448951454412051879189186125221545340848570368.000000 -26.2min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=150 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=130, score=-53411060947610558347144302355231895382981260704204058370560293929972938252953235723809699531784306458885684237361130445471744.000000 -26.5min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=150 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=130, score=-53124241841104212759394932010858113652503653562519173092823704426898466861667068897621761539712137840912646186451668817674240.000000 -26.2min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=150 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=150, score=-658274675484621847890526623640808986441055873792887549123472091087217928802458812573815084237235620697603370420665069075247947133585100954927104.000000 -30.1min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=170 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=150, score=-654556547410668599858776021656282289821959474703969003512713242711206010818094471335975493929747742925302055493937363878514622184250407215169536.000000 -30.7min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=170 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=150, score=-639761869500362063707391757317459128746180125108793367234171740819106947742196229216795826961323010136812809289249679919800690917929638531432448.000000 -30.5min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=170 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=150, score=-649353810816750803753010932308771295369422678552345844905956159525703286139158201915417370537596482687041144352628851263549559315060531931381760.000000 -29.9min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=170 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=150, score=-645866760069251200889112370069592722521321848447373534508436132255727655539174398099910127711466756385367400984393834203808456391451498885677056.000000 -31.0min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=170 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=170, score=-1747481562548065035626429071651729466293824845526678651415625131872571564137470547675813265156526851711634734210568979673695327485667006266057721324308004864.000000 -35.0min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=190 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=170, score=-1639741945695404171995442840020740863309635556598758456215439580724061725054370240886376954493984037904463231132576346532997488123291048565402674454142648320.000000 -34.3min\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=170, score=-4913218354729829734851911174674216538131281615912163153747459206264397979251464703410516584284214790223132658067307730545613058141951324425890970487041818624.000000 -35.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  92 out of 100 | elapsed: 520.0min remaining: 45.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] estimate__learning_rate=10, estimate__n_estimators=190 ..........\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=190 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=170, score=-148565613245029288216645135609479532989964496063309593371556673816536088435956618389322424105893963200787499516744270981523338150572416796958001387110662144.000000 -35.8min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=190 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=170, score=-2589181004803655742058033281118839680083212551442695399541342634523257079625067036722094737375600464481339087450504320592192160489379048473758137359823536128.000000 -35.1min\n",
      "[CV] estimate__learning_rate=10, estimate__n_estimators=190 ..........\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=190, score=-298888602674526976669409775235691259965430163850232525469712643525683788057293573837191684461552214169847359238298044688107084292115205888595991931474429914748770127446016.000000 -38.0min\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=190, score=-602236254208515998791683939135058074838900428988644191851326204428137173737186151448765556407521985237928199354881874441420659573923965941435422517002928073330056000176128.000000 -38.7min\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=190, score=-50814315919911956794873357579467674019569879862903605966869242536127109531797374928555121243376610133318521845715096067656445585277913538127985117953457809101965620150272.000000 -38.9min\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=190, score=-329666549289669631988652134954586025826594429300158603041391092175947767502678792515114954153416209171043438469555023228531643381868661050361399567142340916969396351533056.000000 -37.0min\n",
      "[CV]  estimate__learning_rate=10, estimate__n_estimators=190, score=-117403127572525715730341160799859080523307484535935856678189303552428580224369204796093947296934247199398345850353906440995573406372568058766229472082924998251978315268096.000000 -36.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed: 559.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: -0.87480, std: 0.00756, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 110}, mean: -0.85933, std: 0.00761, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 130}, mean: -0.84929, std: 0.00968, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 150}, mean: -0.84245, std: 0.00731, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 170}, mean: -0.83413, std: 0.00661, params: {'estimate__learning_rate': 0.1, 'estimate__n_estimators': 190}, mean: -0.81587, std: 0.01042, params: {'estimate__learning_rate': 0.5, 'estimate__n_estimators': 110}, mean: -0.81227, std: 0.00811, params: {'estimate__learning_rate': 0.5, 'estimate__n_estimators': 130}, mean: -0.81374, std: 0.01182, params: {'estimate__learning_rate': 0.5, 'estimate__n_estimators': 150}, mean: -0.81562, std: 0.00646, params: {'estimate__learning_rate': 0.5, 'estimate__n_estimators': 170}, mean: -0.80990, std: 0.00755, params: {'estimate__learning_rate': 0.5, 'estimate__n_estimators': 190}, mean: -0.84720, std: 0.01541, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 110}, mean: -0.84854, std: 0.01047, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 130}, mean: -0.84776, std: 0.01273, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 150}, mean: -0.84906, std: 0.01289, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 170}, mean: -0.85640, std: 0.01518, params: {'estimate__learning_rate': 1, 'estimate__n_estimators': 190}, mean: -4394613875437216646817138030915139564269587270997962346144870548157616364534828651033422911754865016307712.00000, std: 43907693268301752165527793539505186652118115816676538985332236866923677590478143231156687664819279495168.00000, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 110}, mean: -53428245319198849310217442461561421637593940705023518065439925405520653166633373795458202173988004989203404132579286833954816.00000, std: 533815045804227943488043147630489640270282305383513333688900721744554568574611014042996043791853621366269215715636501544960.00000, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 130}, mean: -649562732656330955207320305763861501675253952803893299290145036576353150254708559152532302030954354565945470560792048994269460714772407296983040.00000, std: 6489944744688918161170429288954510459443472874331250308549181318649747417485175420954083195117499811933980339928643120007201558251758692597760.00000, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 150}, mean: -2207637696204396556379179123430535320959158444367395233857083575419782894262157361904942864085012505977987944189495166714541194536870983644995239338034855936.00000, std: inf, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 170}, mean: -279801769933030081469827140431866912225407973898682598812292756369212301485431762576495909852299509105868339468877301584402811802235159517589783063829699240704366394474496.00000, std: inf, params: {'estimate__learning_rate': 10, 'estimate__n_estimators': 190}]\n",
      "{'estimate__learning_rate': 0.5, 'estimate__n_estimators': 190}\n",
      "-0.809900071722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
       "       handle_unknown='error', n_values=[7, 10, 10, 10], sparse=False)), ('scale', StandardScaler(copy=True, with_mean=T...0.0, n_estimators=190,\n",
       "             random_state=None, subsample=1.0, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "steps = [\n",
    "#   ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False)),\n",
    "#   ('impute', Imputer(0)),\n",
    "#   ('feature_selection', SelectKBest(f_classif)),\n",
    "#   ('pca', PCA(n_components=120)),\n",
    "  \n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, is removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('estimate', GradientBoostingRegressor())\n",
    "\n",
    "]\n",
    "\n",
    "est = Pipeline(steps)\n",
    "\n",
    "params = {\n",
    "#   'one_hot__n_values': [7, 10, 20],\n",
    "#   \"feature_selection__k\": [i for i in range(1, n_features - 1)]\n",
    "#   'estimate__max_features': [i for i in range(110, n_features, 10)],\n",
    "  'estimate__learning_rate': [0.1, 0.5, 1, 10],\n",
    "  'estimate__n_estimators': [i for i in range(110, n_features, 20)],\n",
    "#   'estimate__loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "# cross_validation_iter = StratifiedShuffleSplit(y=targets_train, test_size=0.3,\n",
    "#                                                random_state=RANDOM_STATE, n_iter=10)\n",
    "# search_params = RandomizedSearchCV(\n",
    "#   estimator=est,\n",
    "#   param_distributions=params,\n",
    "#   cv=5,\n",
    "#   scoring=mape_scorer,\n",
    "#   n_jobs=2,\n",
    "#   verbose=1\n",
    "# )\n",
    "\n",
    "search_params = GridSearchCV(\n",
    "  estimator=est,\n",
    "  param_grid=params,\n",
    "  cv=5,\n",
    "  scoring=mape_scorer,\n",
    "  n_jobs=5,\n",
    "  verbose=3\n",
    ")\n",
    "\n",
    "search_params.fit(data_train_original, targets_train)\n",
    "print(search_params.grid_scores_)\n",
    "print(search_params.best_params_)\n",
    "print(search_params.best_score_)\n",
    "search_params.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('impute',\n",
       "  Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)),\n",
       " ('one_hot',\n",
       "  OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
       "         handle_unknown='error', n_values=[7, 10, 10, 10], sparse=False)),\n",
       " ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " ('estimate',\n",
       "  GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.5, loss='ls',\n",
       "               max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=190,\n",
       "               random_state=None, subsample=1.0, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_params.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data's prediction MAPE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797528122453\n"
     ]
    }
   ],
   "source": [
    "final_est = search_params.best_estimator_\n",
    "test_predictions = final_est.predict(data_test_original)\n",
    "print(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = search_params.predict(data_test_original)\n",
    "print(mape(targets_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(final_est, open(EST_PICKLE_FILENAME, \"w\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "loss: 10.10\n",
      "sample predictions:\n",
      "[[  6.47371387e+00]\n",
      " [  9.99732614e-01]\n",
      " [  8.96018438e+04]\n",
      " [  9.99730945e-01]\n",
      " [  4.54779688e+04]\n",
      " [  2.72998505e+01]\n",
      " [  9.99730945e-01]\n",
      " [  3.97196693e+01]\n",
      " [  3.78494692e+00]\n",
      " [  9.99730945e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compare baseline with final algorithm\n",
    "\n",
    "import pdb\n",
    "import numpy as np\n",
    "import gcp.bigquery as bq\n",
    "import gcp.storage as storage\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import time\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dropout\n",
    "\n",
    "HDF_FILENAME = 'final_model3.hdf5'\n",
    "JSON_MODEL_FILENAME = 'final_model.json'\n",
    "\n",
    "\n",
    "steps = [\n",
    "  ('impute', Imputer()),\n",
    "  # Remember that gap, the first variable, is removed, thus categorical_features\n",
    "  # should start at index 0.\n",
    "  ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], sparse=False,\n",
    "                           n_values=[7, 10, 10, 10])),\n",
    "  ('scale', StandardScaler()),\n",
    "#   ('pca', PCA(n_components=120)),\n",
    "#   ('estimate', final_model)\n",
    "]\n",
    "transformer = Pipeline(steps)\n",
    "data_test2 = transformer.fit_transform(data_test_original)\n",
    "\n",
    "\n",
    "\n",
    "def mape(y, predictions):\n",
    "  return K.mean(K.abs(y-predictions/K.clip(K.abs(y), K.epsilon(), np.inf)), axis=-1)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "def get_optimizer(epochs=50):\n",
    "  learning_rate = 0.2\n",
    "  decay_rate = learning_rate / epochs\n",
    "  momentum = 0.8\n",
    "  return SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "from keras.models import model_from_json\n",
    "json_file = open(JSON_MODEL_FILENAME, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(HDF_FILENAME)\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss=mape, optimizer=get_optimizer(epochs))\n",
    "\n",
    "score = loaded_model.evaluate(data_test2, targets_test, verbose=0)\n",
    "print \"%s: %.2f\" % (loaded_model.metrics_names[0], score)\n",
    "test_predictions2 = loaded_model.predict(data_test2)\n",
    "print 'sample predictions:'\n",
    "print(test_predictions2[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"Process Final Test Data With Final Algorithm\" to use pickled final algorithm against final test data to produce csv required by this competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=112,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "           \n",
    "1000 training data, Score: 0.924\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=114,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "\n",
    "5000 training data, Score: 0.992\n",
    "\n",
    "\n",
    "Pipeline(steps=[('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=False)), ('pca', PCA(copy=True, n_components=120, whiten=False)), ('estimate', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=111,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "           \n",
    "20000 training data, Score: 1.001\n",
    "\n",
    "Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('one_hot', OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "       handle_unknown='error', n_values=[7, 10, 10, 10], sparse=False)), ('scale', StandardScaler(copy=True, with_mean=T...s_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'))])\n",
    "           \n",
    "80000 training data, Score: 0.977 (submitted date 17)\n",
    "\n",
    "[('impute',\n",
    "  Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)),\n",
    " ('one_hot',\n",
    "  OneHotEncoder(categorical_features=[0, 1, 2, 3], dtype=<type 'float'>,\n",
    "         handle_unknown='error', n_values=[7, 10, 10, 10], sparse=False)),\n",
    " ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    " ('estimate',\n",
    "  GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.5, loss='ls',\n",
    "               max_depth=3, max_features=None, max_leaf_nodes=None,\n",
    "               min_samples_leaf=1, min_samples_split=2,\n",
    "               min_weight_fraction_leaf=0.0, n_estimators=190,\n",
    "               random_state=None, subsample=1.0, verbose=0, warm_start=False))]\n",
    "               \n",
    "80000 training data, Score: 0.798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
